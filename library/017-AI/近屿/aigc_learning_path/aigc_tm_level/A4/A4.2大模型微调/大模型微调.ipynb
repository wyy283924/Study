{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bfc4c2-f222-4bd7-b8f5-3a6cd75f465d",
   "metadata": {},
   "source": [
    "# 大模型微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564c641-28ea-418d-8c98-ba67efe0a7d5",
   "metadata": {},
   "source": [
    "一、大模型微调简介*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2c215-ae7f-4590-a123-017b4518be22",
   "metadata": {},
   "source": [
    "二、微调示例**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81cf5a-d9a9-4c93-bdb7-b1d3adba1d35",
   "metadata": {},
   "source": [
    "三、微调模型的数据准备**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205f732-ca3c-4a49-aa92-cc81908c798d",
   "metadata": {},
   "source": [
    "四、微调在大模型训练中的位置***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f4d05-42aa-4ec0-b6e0-f4adb68a6c18",
   "metadata": {},
   "source": [
    "五、PEFT方法对比 **\n",
    "\n",
    "六、可能有害的全量微调 **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2461a2-6464-434b-b7bb-d9f8572698bb",
   "metadata": {},
   "source": [
    "## 一、大模型微调简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abe44f-aa24-4825-bf32-3232099814ff",
   "metadata": {},
   "source": [
    "本章节内容：\n",
    "- 微调解决的问题\n",
    "- 微调到底是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a6791-130e-4c33-aa3f-c87acc8946d0",
   "metadata": {},
   "source": [
    "### 微调解决的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f8e8d-fc45-442d-9d62-9028492eb0e6",
   "metadata": {},
   "source": [
    "#### 基础模型输出模式较少"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a60624-0b38-48a0-bac1-25a1241c0173",
   "metadata": {},
   "source": [
    "**基座模型输出：**\n",
    "\n",
    "<img src=\"image/预训练输出例子.png\"  width=\"900\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "**针对对话能力微调后：**\n",
    "\n",
    "<img src=\"image/SFT输出例子.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7efa4-a693-4228-8a07-762acb09b9ae",
   "metadata": {},
   "source": [
    "#### 通用大模型缺乏领域知识/回答存在幻觉问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73010304-6709-4a9f-9064-51dca2a5cf08",
   "metadata": {},
   "source": [
    "问题：\n",
    "\n",
    "<img src=\"image/周鲁1.png\"  width=\"600\" alt=\"区别\"/><br>\n",
    "\n",
    "回答：\n",
    "\n",
    "<img src=\"image/周鲁2.png\"  width=\"600\" alt=\"区别\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f6934-4882-4795-b49d-53cc5a9f6a43",
   "metadata": {},
   "source": [
    "### 微调到底是什么\n",
    "\n",
    "微调（Fine-tuning）是一种机器学习技术，特别是在深度学习领域中，它指的是在一个预训练的模型基础上继续训练，以使模型适应特定的任务。预训练模型通常在大型数据集上进行训练，以学习丰富的特征表示。然后，这些模型可以用较小的、特定任务相关的数据集进行微调，以细化它们对特定问题的处理能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec99ce-6b91-4e9f-9b43-d43353042c8f",
   "metadata": {},
   "source": [
    "### Prompt Tuning\n",
    "\n",
    "- Prompt Tuning 起源于GPT3 (论文：[Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf))，其认为超大规模的LLM只要配合好合适的模板就可以极大化地发挥其推理和理解能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17091070-d694-46a1-948f-4c3857912bab",
   "metadata": {},
   "source": [
    "<img src=\"image/prompt t.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615aa1d-17cd-4791-aee4-83c01138f855",
   "metadata": {},
   "source": [
    "#### 什么是Prompt-Tuning？\n",
    "\n",
    "我们可以把Prompt-Tuning想象成一种训练智能机器人的简化方法。通常，这个智能机器人（我们称之为预训练语言模型，比如GPT-3或BERT）已经通过阅读大量文本学到了很多知识。但是，当我们想要让它完成一个特定的任务时，比如情感分析或者文本分类，传统的方法是Fine-Tuning。这就像让机器人回学校进修，学习大量特定任务的示例。但这个过程既耗时又耗资源。\n",
    "\n",
    "Prompt-Tuning，顾名思义，是通过设计一种“提示”（Prompt），来引导机器人利用它已经掌握的知识去完成新任务。这种方法更加高效，因为它不需要机器人学习大量新的信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284de45-fb19-4986-8572-ffd526a30b17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 二、微调示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe9cc6-e430-475c-bdf8-ce98528e4aa4",
   "metadata": {},
   "source": [
    "本章节内容：\n",
    "- 微调步骤\n",
    "- 微调全过程展示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c8554-3357-4fca-ae44-e0a811f0edbc",
   "metadata": {},
   "source": [
    "### 微调的步骤通常包括以下几个方面：\n",
    "\n",
    "- 准备特定任务的数据集：虽然预训练模型拥有广泛的知识，但它们需要通过特定任务的数据来了解特定领域的细节。\n",
    "- 选择预训练模型：这通常是一个在广泛数据上已经训练过的大型模型，如在文本处理任务上的BERT或GPT，或者在图像识别任务上的ResNet。\n",
    "- 模型调整(可选)：可能需要对模型架构进行一些调整，如添加或修改输出层，以适应新任务的输出需求。\n",
    "- 开始微调：使用特定任务的数据集继续训练模型。这个过程可能会涉及调整**学习率**，选择合适的**损失函数**和**优化器**，以及决定哪些层的权重需要被冻结或更新。\n",
    "- 评估和调优：在微调后，模型将在验证集上进行评估，以确保其性能符合预期。可能需要进行多次迭代，以找到最佳的参数和设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4643691",
   "metadata": {},
   "source": [
    "### 什么是损失函数？\n",
    "我们可以把深度学习模型想象成一个试图解决问题的机器。假设，我们要教这台机器如何识别照片中的猫和狗。为了让机器能学会这个技能，我们需要给它看很多已经标记好是猫还是狗的照片，这个过程就叫做“训练”。\n",
    "\n",
    "在训练过程中，深度学习模型会不断地去调整权重去尝试正确预测每张照片是猫还是狗。损失函数，就是用来衡量模型的预测结果和真实结果之间的差距的一种方法。你可以把它想象成一种评分规则，这个规则会告诉我们模型的预测有多准确，或者说错误有多大。\n",
    "\n",
    "例如，如果模型预测一张猫的照片是狗，损失函数就会给出一个高分（表示错误较大）。相反，如果预测正确，损失函数的分数就会很低（表示准确度高）。我们的目标是通过调整模型参数，使得这个损失函数的分数尽可能地低，这样模型的预测准确度就会越高。\n",
    "\n",
    "常见的损失函数有均方误差（Mean Squared Error，MSE）、交叉熵（Cross Entropy）等。不同的问题和模型可能需要使用不同的损失函数。\n",
    "\n",
    "例如，对于二分类问题，我们通常会使用二元交叉熵损失函数。这个损失函数的计算公式如下：\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "假设我们将图像分类为猫（类别 0）或狗（类别 1），那么 $y_i$ 是真实标签，$\\hat{y}_i$ 是模型的预测概率。例如在某一次训练中，模型输出了一个值表示去预测一张猫的照片是狗的概率为 0.8，那么损失函数的计算公式如下：\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - （0 \\times \\log(0.8) + 1 \\times \\log(0.2)） = -\\log(0.2) \\approx 0.69\n",
    "$$\n",
    "\n",
    "而如果模型预测一张狗的照片是狗的概率为 0.8，那么损失函数的计算公式如下：\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - （1 \\times \\log(0.8) + 0 \\times \\log(0.2)） = -\\log(0.8) \\approx 0.22\n",
    "$$\n",
    "\n",
    "可以看到，当模型的预测结果和真实结果越接近，损失函数的值就越小。模型进行训练的过程就是通过不断地调整参数，使得损失函数的值总体上尽可能地小。\n",
    "\n",
    "总之，损失函数是深度学习中一个非常关键的概念，它帮助我们了解模型在学习过程中的表现如何，并指导我们如何改进模型，使其预测结果更加准确。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320405c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 什么是学习率？\n",
    "现在让我们来聊聊深度学习中的一个非常重要的概念——学习率。\n",
    "\n",
    "首先，想象一下你正在教一个机器人如何走路。你不可能一次就教会它所有的步骤，而是需要一步一步地调整它的动作，直到它能够流畅地行走。在这个过程中，你每次调整的幅度就好比是学习率。\n",
    "\n",
    "刚刚你学习了损失函数，它可以告诉我们机器学习模型的预测结果和实际结果之间有多大的差距。我们的目标是减小这个差距，使模型的预测更加准确。\n",
    "\n",
    "学习率，就是在这个优化过程中，我们调整模型参数的幅度。如果学习率太高，模型参数的调整幅度会过大，可能会导致模型的参数在参数空间中来回跳动，无法收敛到最优解。这就好比你教机器人走路时，每次调整它的步伐都太大，结果它不断摔倒。\n",
    "\n",
    "相反，如果学习率太低，每次参数的调整幅度又太小，模型需要很长时间才能达到最优状态，甚至可能陷入局部最优解，无法取得更好的效果。这类似于你教机器人走路时，每次调整它的步伐都太小，以至于它学习的速度非常缓慢。\n",
    "\n",
    "<img src=\"image/learning.png\"  width=\"900\" alt=\"学习率\"/><br>\n",
    "\n",
    "因此，选择一个合适的学习率非常关键，它决定了学习的速度和效率。在实际应用中，我们可能会开始时使用较大的学习率，以快速接近最佳解，然后逐渐减小学习率，以确保模型能够稳定地达到最优解。\n",
    "\n",
    "我们总结一下，学习率就是我们在训练深度学习模型时，调整模型权重的幅度。通过合理地设置学习率，我们可以加速模型的收敛速度，提高模型的训练效率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b616355",
   "metadata": {},
   "source": [
    "### 什么是优化器？\n",
    "刚刚我们学习了损失函数和学习率，它们分别用来衡量模型的预测结果和实际结果之间的差距，以及调整模型参数的幅度。那么，优化器是用来做什么的呢？\n",
    "\n",
    "在深度学习中，我们通过不断地调整模型的参数，使得损失函数的值尽可能地小，以提高模型的预测准确度。而优化器帮助我们自动地实现这个目标，它通过计算损失函数的梯度，通过某些优化算法来调整模型的参数，使得损失函数的值不断减小。\n",
    "\n",
    "例如，我们可以使用梯度下降算法来优化模型。这个算法的基本思想是，通过计算损失函数对模型参数的梯度，然后沿着梯度的反方向调整参数，理论上这使得损失函数的值不断减小，从而使得模型的预测结果就会越来越准确。\n",
    "\n",
    "又如，我们可以使用 Adam 优化器来优化模型。Adam 优化器是一种自适应学习率的优化算法，它可以根据梯度的大小自动调整学习率，简单来说，Adam 结合了动量（momentum）和 RMSprop 优化器的优点，通过计算梯度的一阶动量和二阶动量来动态调整每个参数的学习率，使得优化过程更高效、更稳健。\n",
    "\n",
    "Adam 是基于梯度下降算法的，但它通过两个关键的技术来改进传统的梯度下降方法：\n",
    "\n",
    "- 动量方法：Adam 通过计算梯度的一阶动量（即梯度的指数加权移动平均）来加速收敛。动量方法有助于在梯度方向一致时积累更快的更新，同时在梯度方向不一致时减少振荡。\n",
    "\n",
    "- 自适应学习率：Adam 还计算梯度的二阶动量（即梯度平方的指数加权移动平均），这有助于自动调整每个参数的学习率。梯度较大的参数会有较小的更新步伐，而梯度较小的参数则会有较大的更新步伐。这种方法类似于 RMSprop 优化器，但 Adam 结合了一阶动量和二阶动量，使得优化过程更为灵活。\n",
    "\n",
    "总而言之，优化器通过帮助我们自动计算损失函数的梯度，调整模型的参数。选择合适的优化器就可以以更高的效率和更快的速度训练模型，提高模型的性能。\n",
    "\n",
    "<img src=\"image/optimizer.png\"  width=\"600\" alt=\"优化器\"/><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c7b72-11bc-49c5-bd1d-1b2c6d425233",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 准备特定任务的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda5d5c-61dd-4283-94d9-d8a0e823afba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 数据集的重要指标\n",
    "1. **提供高质量数据**\n",
    "   \n",
    "   数据质量在微调中是非常重要的。给出垃圾数据，大模型更容易生成垃圾内容；给出高质量数据，大模型就有更大概率生成高质量内容\n",
    "   \n",
    "   **注意：** 数据质量不是最终模型性能的唯一决定因素\n",
    "2. **保证数据多样性**\n",
    "   \n",
    "   如果给出的数据集中有很多类似的输入输出，那么模型会倾向于记忆这种模式。当用户输入不一样的内容时，模型可能只能输出重复的内容。\n",
    "\n",
    "3. **数据真实性（对比生成的数据）**\n",
    "   虽然现在有很多工具都可以生成训练用数据，但是真实的数据在某些任务上是至关重要的，比如写作，生成的数据会有很多固定的生成模型，过多的生成数据无法保证模型可以学习到新的模式。\n",
    "\n",
    "4. **更多的数据**\n",
    "   这点没有其他三点重要（尤其是数据质量），但更多的数据在某种程度上仍然表现得比较少的数据数量更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dede3a-1a94-45d7-a75e-2040b80e5b63",
   "metadata": {},
   "source": [
    "#### 数据收集与重构\n",
    "开源网站：https://huggingface.co/\n",
    "\n",
    "Hugging Face提供了很多的开源数据集，但当我们想要基于自己的需求进行模型微调时仍需要自己构建数据集，这里我们不对数据来源进行限制，直接来看一下数据集应该以什么样的数据呈现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0004eec-e18c-4139-86b2-f17c746d0c47",
   "metadata": {},
   "source": [
    "下列数据来自[Standford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)，用于为大模型提供基本的对话能力\n",
    "\n",
    "<img src=\"image/alpaca.png\"  width=\"600\"  alt=\"alpaca\"/>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a04540cb-8863-4075-878e-55396c46a02e",
   "metadata": {},
   "source": [
    "{\n",
    "    \"instruction\": \"给出三个保持健康的提示。\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"1.饮食要均衡，确保包括足够的水果和蔬菜。\\n2.定期运动以保持身体活跃和强壮。\\n3.保持充足的睡眠并保持一致的睡眠时间表。\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"什么是三原色？\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"三原色是红色、蓝色和黄色。\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"描述一个原子的结构。\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"一个原子由核和绕核运动的电子组成。核内含有质子和中子，质子和中子带有正电荷，而电子带有负电荷，因此整个原子是中性的。每个粒子的数量决定了原子的原子序数和种类。\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"我们如何减少空气污染？\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"有许多方法可以减少空气污染，例如转向可再生能源，鼓励使用公共交通工具，禁止燃烧化石燃料，实施减少工业排放的政策，以及实施车辆排放标准。此外，个人可以通过减少汽车使用，避免燃烧木材等物质，更换高效节能的家电来减少空气污染。\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"描述做出艰难决定的时刻。\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"我曾经在一家建筑公司担任项目经理的时候，需要在特定日期之前完成一个项目以达到客户的期望。然而，由于意外延误，我们无法按时完成项目，所以我必须做出艰难的决定。我决定将截止日期延长，但这样会使团队资源更加紧张，预算也增加了。虽然这是一项冒险的决定，最终决定继续推进，确保项目能够按时完成并满足客户的期望。\"\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cf031-c0c3-453d-b3c8-37b47944512a",
   "metadata": {},
   "source": [
    "<img src=\"image/parse_analysis.png\"  width=\"600\"  alt=\"alpaca-data\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335dcdbc-f5de-4e9b-956f-d3f1f424d968",
   "metadata": {},
   "source": [
    "Llama2\n",
    "\n",
    "<img src=\"image/llamadata.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42adcc97-a1be-4d35-bcd5-10621a3abe83",
   "metadata": {},
   "source": [
    "GPT3\n",
    "\n",
    "<img src=\"image/gpt.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb756f8-6bcd-4df4-b18e-c64c24d32a9a",
   "metadata": {},
   "source": [
    "总体而言，他们都分为以下几个部分：\n",
    "\n",
    "**instruct/prompt:**\n",
    "给出一个问题或指令，例如，“世界上最高的山是什么名字？”\n",
    "\n",
    "**input:**\n",
    "给出上下文，例如回答问题的必要信息或者指定的回答问题的格式，有时候这部分会被合并到instruct/prompt\n",
    "\n",
    "**output/completion:**\n",
    "这是答案部分，针对上面的问题，答案可能是，“世界上最高的山是珠穆朗玛峰。”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b057a8c-5226-40a6-adec-85a9f80628e2",
   "metadata": {},
   "source": [
    "#### 下面我们来看看具体的数据处理过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084d7884-ac4d-4e28-8742-a032e7dc7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def convert_csv_to_json(csv_file_path):\n",
    "    # 根据文件结构调整CSV读取参数\n",
    "    try:\n",
    "        # 使用pandas读取CSV文件\n",
    "        # 看起来分隔符是逗号，不是制表符\n",
    "        df = pd.read_csv(csv_file_path, sep=',', encoding='utf-8')\n",
    "    \n",
    "        # 将DataFrame转换为字典列表\n",
    "        data = df.to_dict(orient='records')\n",
    "    \n",
    "        # 将数据格式化为所需的JSON结构\n",
    "        json_data = [{\"instruction\": item[\"问题\"], \"output\": item[\"回答\"]} for item in data]\n",
    "    except Exception as e:\n",
    "        json_data = f\"发生错误：{str(e)}\"\n",
    "    return json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a10db565-b219-4f34-a683-a988775fd11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>问题</th>\n",
       "      <th>回答</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>使用定性和定量分析相结合的方法，研究并编写一份关于地震的地质、地震学和社会影响的全面报告。你...</td>\n",
       "      <td>标题：全面研究报告：地震的地质、地震学影响与社会影响\\n\\n一、地震的定义和理解\\n\\n地震...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我该如何在C++中评估废除夏令时对经济的影响？你能否提供一个考虑能源消耗、生产力和交通成本等...</td>\n",
       "      <td>在现实中，评估废除夏令时对经济的影响这样的复杂问题并不适合用编程语言（包括C++）来解决。这...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>假设你正在调试一段Java代码，但突然意识到自己既是侦探又是犯罪电影中的凶手。为了解决这个谜...</td>\n",
       "      <td>在Java代码中，有许多可能的数据类型可能引起错误。以下是几个可能的\\\"罪犯\\\"：\\n\\n1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  问题  \\\n",
       "0  使用定性和定量分析相结合的方法，研究并编写一份关于地震的地质、地震学和社会影响的全面报告。你...   \n",
       "1  我该如何在C++中评估废除夏令时对经济的影响？你能否提供一个考虑能源消耗、生产力和交通成本等...   \n",
       "2  假设你正在调试一段Java代码，但突然意识到自己既是侦探又是犯罪电影中的凶手。为了解决这个谜...   \n",
       "\n",
       "                                                  回答  \n",
       "0  标题：全面研究报告：地震的地质、地震学影响与社会影响\\n\\n一、地震的定义和理解\\n\\n地震...  \n",
       "1  在现实中，评估废除夏令时对经济的影响这样的复杂问题并不适合用编程语言（包括C++）来解决。这...  \n",
       "2  在Java代码中，有许多可能的数据类型可能引起错误。以下是几个可能的\\\"罪犯\\\"：\\n\\n1...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./example.csv\", sep=',', encoding='utf-8')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4203af1",
   "metadata": {},
   "source": [
    "#### pd.read_csv的内部实现原理\n",
    "实际上，pandas的read_csv函数实现并不简单，它可以接受超过50个参数，这些参数控制数据的读取、解析、转换等过程。下面我们来大致看看read_csv函数的内部实现原理。\n",
    "\n",
    "**1.函数入口**\n",
    "\n",
    "read_csv函数在pandas/io/parsers.py中定义。调用read_csv时，会首先进行一些参数检查和预处理，然后调用内部的TextFileReader类来处理文件读取逻辑。\n",
    "\n",
    "**2.参数解析**\n",
    "\n",
    "read_csv函数的参数非常多，这里只列出一部分常用的参数：\n",
    "\n",
    "- filepath_or_buffer：文件路径或文件对象\n",
    "- sep：分隔符，默认为逗号\n",
    "- header：指定哪一行作为列名\n",
    "- names: 用于结果的列名列表。\n",
    "- index_col: 指定列为索引的列编号或列名。\n",
    "- usecols: 返回一个数据子集，该子集由列组成。\n",
    "- dtype: 指定列的数据类型。\n",
    "\n",
    "**3.创建TextFileReader对象**\n",
    "\n",
    "实际上，TextFileReader是一个迭代器，它会根据chunksize参数将文件分块读取，然后对每个块进行解析。\n",
    "\n",
    "```python\n",
    "parser = TextFileReader(filepath_or_buffer, **kwds)\n",
    "```\n",
    "\n",
    "**4. 建立解析引擎**\n",
    "\n",
    "TextFileReader内部根据文件类型和参数，选择合适的解析器。主要的解析器是PythonParser和CParserWrapper，分别对应Python实现和C实现。\n",
    "\n",
    "```python\n",
    "self._engine = self._make_engine(self.engine)\n",
    "\n",
    "def _make_engine(\n",
    "        self,\n",
    "        f: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str] | list | IO,\n",
    "        engine: CSVEngine = \"c\",\n",
    "    ) -> ParserBase:\n",
    "    mapping: dict[str, type[ParserBase]] = {\n",
    "            \"c\": CParserWrapper,\n",
    "            \"python\": PythonParser,\n",
    "            \"pyarrow\": ArrowParserWrapper,\n",
    "            \"python-fwf\": FixedWidthFieldParser,\n",
    "        }\n",
    "    ...\n",
    "\n",
    "    try:\n",
    "        return mapping[engine](f, **self.options)\n",
    "```\n",
    "\n",
    "**5. 解析数据**\n",
    "\n",
    "无论使用哪种解析器，都会调用相应引擎的read方法来读取文件内容并将其转换为DataFrame。以下是PythonParser的read方法的逻辑：\n",
    "\n",
    "1. 如果存在索引名称，并且第一行的数据全为空，那么这一行就被认为是索引名称行，将被单独处理，并从数据中移除。\n",
    "\n",
    "2. 将行数据转换为列数据。\n",
    "\n",
    "3. 处理隐式索引。隐式索引是指没有明确标记为索引的列，但是在解析数据时被当作索引处理。\n",
    "\n",
    "4. 将数据转换为适当的数据类型。\n",
    "\n",
    "5. 处理日期转换。如果某一列的数据可以被解析为日期，那么这一列的数据将被转换为日期类型。\n",
    "\n",
    "6. 创建索引。\n",
    "\n",
    "7. 最后，返回一个元组，包含索引、列名和数据。\n",
    "\n",
    "显然，这个元组可以最终被Pandas的DataFrame接受，从而创建一个DataFrame对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d771426c-d8a4-429e-86fa-4df508eb8a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': '使用定性和定量分析相结合的方法，研究并编写一份关于地震的地质、地震学和社会影响的全面报告。你的报告必须包括至少10个关于地震的独特和有趣的事实，这些事实来自各种来源，包括但不限于学术期刊、政府报告和可靠的新闻媒体。此外，你的报告应探讨地震的历史和原因，包括但不限于构造板块、断层线和地震波的作用。此外，你的报告还应分析地震对社会和经济的影响，包括但不限于对基础设施、公共卫生和应急响应系统的影响。最后，你的报告应以一系列建议为结论，为政策制定者和利益相关者提供如何减轻与地震相关的风险的建议，考虑到最新的科学研究和来自世界各地的最佳实践。你的报告应使用清晰简洁的语言编写，适当引用和参考文献。', 'output': '标题：全面研究报告：地震的地质、地震学影响与社会影响\\\\n\\\\n一、地震的定义和理解\\\\n\\\\n地震是地壳内部突然释放能量的过程，这种过程引发振动和散射地震波，从而引发的横跨数千公里的地表振动。地壳内的能量主要是由于地球板块的相互作用，尤其是构造板块以及到地震中心的断层线（定量事实1）。\\\\n\\\\n二、地震发生的原因\\\\n\\\\n1. 构造板块：地震常常在板块的边界发生，这是因为这些地区压力累积，最终导致地壳破裂。例如，太平洋板块和北美板块在西部美国的加利福尼亚州交界处，这就是著名的圣安地列斯断层线，经常发生地震（定量事实2）。\\\\n\\\\n2. 断层线：断层是地壳中的裂缝，两边的岩石发生相对位移。这种移动可导致地震，或由地震引起。\\\\n\\\\n3. 地震波：地震会产生三种地震波，即P波（初到波），S波（剪切波）和表面波。P波是最先到达地震仪的，速度最快。然后是S波，最后是表面波，它们在地震的过程中产生最大的破坏（定量事实3）。\\\\n\\\\n三、地震的影响\\\\n\\\\n1. 基础设施：地震会导致建筑物、桥梁、道路、电网、水管等基础设施破裂或倒塌，不仅导致财产损失，还可能导致人员伤亡。例如，1994年的北岭地震导致57人死亡，超过8700人受伤，直接的经济损失约为200亿美元（定量事实4）。\\\\n\\\\n2. 公共卫生：地震后的灾区常常出现供水、排水、卫生设施、医疗服务等基础设施的破坏，这可能会引发公共卫生危机，如传染病的爆发等（定量事实5）。\\\\n   \\\\n3. 应急响应系统：地震会对应急响应系统提出挑战。一方面，基础设施的损坏会影响救援行动的实施；另一方面，地震的突然性和难以预测，使得需要快速、适时的应急响应。\\\\n\\\\n四、建议与结论\\\\n\\\\n1. 提高设施抗震能力：为了减少地震对基础设施的影响，建议建立和执行严格的建筑抗震标准，以提高建筑物、桥梁等对地震的抗击能力。\\\\n\\\\n2. 加强预警系统的建设：通过加强地震监测和预警系统，可以提早预警地震，减轻地震灾害（定量事实6）。\\\\n\\\\n3. 建立健全应急响应机制：在地震频繁发生的地区，应建立健全应急响应机制，提高应对地震的能力与效率。\\\\n\\\\n引用文献：\\\\n无\\\\n\\\\n报告编写者：#\\\\n报告审查者：\\n'}\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"./example.csv\"\n",
    "json_output = convert_csv_to_json(csv_file_path)\n",
    "print(json_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a80a0d-68da-4d84-857d-ac54d670f1b4",
   "metadata": {},
   "source": [
    "### 选择预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d2c8d-c7aa-43d6-ade7-62cf1071abe9",
   "metadata": {},
   "source": [
    "<img src=\"image/hfos.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0d4d4-52e4-4b36-8204-1b83f22cbfda",
   "metadata": {},
   "source": [
    "<img src=\"image/describe.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cb096-9631-4b80-b33b-b7369bc28719",
   "metadata": {},
   "source": [
    "### 模型调整(可选)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81076c96-dea3-45e8-9556-6a04590cdc4a",
   "metadata": {},
   "source": [
    "微调方法可以被分为全量微调和PEFT\n",
    "#### 全量微调\n",
    "\n",
    "全量微调是一类较早诞生的微调方法，全参数微调需要消耗大量的算力，实际使用起来并不方便，因此不久之后又诞生了只围绕部分参数进行微调的高效微调方法\n",
    "\n",
    "- 全量微调的问题：\n",
    "\n",
    "1. 高计算成本\n",
    "\n",
    "    全面微调涉及更新大型模型的所有参数。大规模的预训练语言模型拥有数百亿甚至数千亿的参数，因此训练需要大量的计算资源。即使微调数据集相对较小，标记的数量也可能很大，计算成本会非常高昂。\n",
    "\n",
    "2. 时间和专业知识密集型\n",
    "\n",
    "    当模型非常大时，通常需要将计算分布到多个GPU和节点上。这需要适当的专业知识。根据模型的大小和数据集的大小，微调可能需要几小时、几天甚至几周的时间。\n",
    "\n",
    "3. 灾难性遗忘：\n",
    "   \n",
    "   当模型学习了一个新的任务后，可能会忘记之前学到的知识。这种现象可能会限制微调模型在实际应用中的通用性和可扩展性。\n",
    "\n",
    "#### PEFT（Parameter-Efficient Fine-Tuning）\n",
    "\n",
    "PEFT技术旨在通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。\n",
    "\n",
    "<img src=\"image/PEFT tree.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/> \n",
    "\n",
    "图片源自: https://arxiv.org/abs/2312.12148"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc247f69-351c-4650-be85-c1be1fe4f985",
   "metadata": {},
   "source": [
    "1. Additive Fine-tuning （Adapter, Prompt Tuning, Prefix-Tuning等）\n",
    "- 定义：Additive Fine-tuning 指的是在模型的原始架构上增加额外的层或模块，然后只对这些新增的部分进行训练。\n",
    "\n",
    "2. Partial Fine-tuning\n",
    "- 定义：Partial Fine-tuning 涉及选择模型中的一部分层（通常是顶层）进行训练，而保持其他层固定。\n",
    "  \n",
    "3. Reparameterized Fine-tuning (Lora, QLora等)\n",
    "- 定义：Reparameterized Fine-tuning 指的是改变模型中参数的表示方式，以优化训练过程。\n",
    "  \n",
    "4. Hybrid Fine-tuning\n",
    "- 定义：Hybrid Fine-tuning 结合了多种微调方法，旨在从不同方法中吸取优点。\n",
    "  \n",
    "5. Unified Fine-tuning\n",
    "- 定义：Unified Fine-tuning提供了一个用于微调的统一框架，它简化了将多种微调方法整合到一个协调的架构中的过程，确保了在模型的适应和优化过程中的一致性和效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb7253-b862-42b5-bbcd-4ae7dc2d2ad0",
   "metadata": {},
   "source": [
    "### 开始微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41630c5-6415-46e0-8509-58c25a063237",
   "metadata": {},
   "source": [
    "这里我们展示一个通过QLoRA去微调Llama2的例子，我们的目标是让Llama2学会写对联"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05958a31-18d0-47ff-8b04-844247a1e65a",
   "metadata": {},
   "source": [
    "导入依赖库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f434d5-414a-494c-aa02-6214fe4a5894",
   "metadata": {},
   "source": [
    "```\n",
    "import os                          # 操作系统功能\n",
    "import torch                       # PyTorch库，用于深度学习\n",
    "from datasets import load_dataset  # 加载用于训练的数据集\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,          # 用于语言建模任务的AutoModel\n",
    "    AutoTokenizer,                # 用于令牌化的AutoTokenizer\n",
    "    BitsAndBytesConfig,           # BitsAndBytes的配置\n",
    "    HfArgumentParser,             # Hugging Face模型的参数解析器\n",
    "    TrainingArguments,            # 模型训练的训练参数\n",
    "    pipeline,                     # 创建模型推理的管道\n",
    "    logging,                      # 在训练过程中记录信息\n",
    ")\n",
    "from peft import LoraConfig, PeftModel  # 用于参数高效微调（PEFT）的包\n",
    "from trl import SFTTrainer         # 用于监督式微调的SFTTrainer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36155e9a-ef17-4312-93d1-e8e784d4424f",
   "metadata": {},
   "source": [
    "指定基座模型，数据集（如果是本地微调部署则不需要）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3faa47c-ea30-465d-9352-a0feef58b2b6",
   "metadata": {},
   "source": [
    "```\n",
    "# Hugging Face训练的基座模型\n",
    "model_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "\n",
    "# 使用的指导数据集\n",
    "dataset_name = \"chenqile09/llama2-chinese-couplet\"\n",
    "\n",
    "# 微调后的模型名称\n",
    "new_model = \"llama-2-7b-miniguanaco\"\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf03fd1-2080-4511-8c3d-135f84b84520",
   "metadata": {},
   "source": [
    "QLora参数配置（刚刚说的模型调整）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675d7510-5ded-431f-bd37-b832294f8171",
   "metadata": {},
   "source": [
    "```\n",
    "################################################################################\n",
    "# QLoRA参数\n",
    "################################################################################\n",
    "\n",
    "# LoRA注意力维度\n",
    "lora_r = 64\n",
    "\n",
    "# LoRA缩放的Alpha参数\n",
    "lora_alpha = 16\n",
    "\n",
    "# LoRA层的Dropout概率\n",
    "lora_dropout = 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc79890-4ce9-401c-9d7f-53c3d9cc7b68",
   "metadata": {},
   "source": [
    "模型参数定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba43e475-3817-4e01-b766-317fc9b0addf",
   "metadata": {},
   "source": [
    "```\n",
    "################################################################################\n",
    "# bitsandbytes参数\n",
    "################################################################################\n",
    "\n",
    "# 激活4比特精度基础模型加载\n",
    "use_4bit = True\n",
    "\n",
    "# 计算4比特基础模型的数据类型\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# 量化类型（fp4或nf4）\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# 激活4比特基础模型的嵌套量化（双重量化）\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210bf4d9-9614-4fca-a487-7a0c57870d84",
   "metadata": {},
   "source": [
    "训练基本参数定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d9081-de4e-4caa-ae85-0baf03eef09c",
   "metadata": {},
   "source": [
    "```\n",
    "################################################################################\n",
    "# TrainingArguments参数\n",
    "################################################################################\n",
    "\n",
    "# 模型预测和检查点将被存储的输出目录\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# 训练周期数\n",
    "num_train_epochs = 1\n",
    "\n",
    "# 启用fp16/bf16训练（在A100上设置bf16为True）\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# 训练时每个GPU的批量大小\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# 评估时每个GPU的批量大小\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# 累积梯度更新步数\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# 启用梯度检查点\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# 最大梯度正则化（梯度裁剪）\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# 初始学习率（AdamW优化器）\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# 应用于除偏置/LayerNorm权重以外的所有层的权重衰减\n",
    "weight_decay = 0.001\n",
    "\n",
    "# 使用的优化器\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# 学习率调度（恒定比余弦稍好）\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# 训练步数（覆盖num_train_epochs）\n",
    "max_steps = -1\n",
    "\n",
    "# 线性预热的步骤比例（从0到学习率）\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# 将具有相同长度的序列分组到批次中\n",
    "# 节省内存并显著加快训练速度\n",
    "group_by_length = True\n",
    "\n",
    "# 每X更新步骤保存一次检查点\n",
    "save_steps = 25\n",
    "\n",
    "# 每X更新步骤记录一次\n",
    "logging_steps = 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202fc823",
   "metadata": {},
   "source": [
    "### 上述部分参数的解释\n",
    "#### **warmup_ratio**：\n",
    "\n",
    "`warmup_ratio` 参数定义了在训练开始时逐渐增加学习率到设定的最大学习率的过程中，预热阶段所占的训练总步数的比例。\n",
    "\n",
    "例如，如果 `warmup_ratio` 设置为 0.1，并且总训练步数为 1000 步，那么在前 100 步（1000 * 0.1）中，学习率将从 0 或一个较低的值逐渐增加到设定好的最大学习率。在预热阶段之后，学习率可以按照设定的其他策略（如线性衰减）进行调整。\n",
    "\n",
    "使用预热策略的目的是帮助模型在训练初期稳定下来，避免一开始使用过大的学习率导致模型训练不稳定或者不收敛。这种策略特别适用于大规模模型和复杂的优化场景，可以帮助模型更好地收敛，提高最终模型的性能和稳定性。\n",
    "\n",
    "#### **weight_decay**：\n",
    "`weight_decay` 参数是用来设置权重衰减的，这是一种正则化技术，旨在防止模型过拟合。\n",
    "\n",
    "权重衰减的工作原理是在模型的损失函数中添加一个额外的项，这个项是模型权重的L2范数（即所有权重值的平方和）乘以一个系数（即 `weight_decay` 参数的值）。\n",
    "\n",
    "其公式如下：\n",
    "\n",
    "$$\n",
    "L = L_0 + \\frac{\\lambda}{2} (w_1^2 + w_2^2 + ... + w_n^2)\n",
    "$$\n",
    "\n",
    "其中 $L_0$ 是模型的原始损失函数，$\\lambda$ 是 `weight_decay` 参数的值，$w_1, w_2, ..., w_n$ 是模型的权重值。\n",
    "\n",
    "更抽象地，上面的式子可以写成下面的公式：\n",
    "\n",
    "$$\n",
    "L'(w) = L(w) + \\frac{\\lambda}{2} ||w||^2\n",
    "$$\n",
    "\n",
    "于是，在梯度下降算法更新模型权重时，这会导致梯度更新规则变为：\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\alpha (\\nabla L(w) + \\lambda ||w||)\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$是学习率，与原来计算新的梯度计算方法相比，多了个$\\lambda ||w||$。\n",
    "\n",
    "从上面的一系列公式和解释，我们可以得到如下结论：\n",
    "\n",
    "- `weight_decay` 参数的值越大，正则化项对损失函数的影响就越大，从而对模型的权重值进行更强的惩罚。\n",
    "- 模型的权重值越大，正则化项对损失函数的影响就越大，从而对模型的权重值进行更强的惩罚。\n",
    "- 若 `weight_decay` 过大，那么原本的损失函数项的影响就会被削弱，从而可能导致模型的参数光顾着变小，而忽略了原本的损失函数项，这样可能会导致模型欠拟合。\n",
    "                        \n",
    "因此，我们这样做的效果是在训练过程中对模型的权重进行惩罚，倾向于使权重值更小。权重较小的模型通常具有更好的泛化能力，因为它们依赖于较少的、更强的信号来做出预测，从而减少了对训练数据中**噪声**的拟合。\n",
    "\n",
    "具体来说，如果 `weight_decay` 设置为一个正数，那么在每次参数更新时，每个权重会被缩小一个固定比例（这个比例由 `weight_decay` 的值决定），这有助于防止权重变得过大，从而有助于防止模型过拟合。如果 `weight_decay` 设置为0，则不会应用权重衰减。\n",
    "\n",
    "在实际应用中，选择合适的 `weight_decay` 值是一个需要通过实验调整的过程，以找到最佳的模型性能和泛化能力的平衡点。\n",
    "\n",
    "#### **lr_scheduler_type**：\n",
    "`lr_scheduler_type` 参数用于指定学习率调度器（Learning Rate Scheduler）的类型。学习率调度器是用来调整训练过程中学习率的策略，以帮助模型更好地收敛或避免过拟合。\n",
    "\n",
    "`lr_scheduler_type` 参数可以接受以下几种类型的学习率调度器：\n",
    "\n",
    "1. **Linear**: 学习率会线性地随着训练进度逐渐减小到0。\n",
    "2. **Cosine**: 学习率遵循一个余弦曲线，从初始学习率减小到0。\n",
    "3. **CosineWithRestarts**: 这是余弦调度器的一个变种，会在达到最低点后重新开始。\n",
    "4. **Polynomial**: 学习率按照一个多项式的幂次衰减。\n",
    "5. **Constant**: 学习率保持不变。\n",
    "6. **ConstantWithWarmup**: 学习率在开始时通过一个预热期逐渐增加到一个常数值。\n",
    "7. **LinearWithWarmup**: 学习率在预热期后线性减少。\n",
    "8. **CosineWithWarmup**: 学习率在预热期后遵循余弦衰减。\n",
    "9. **CosineWithHardRestarts**: 类似于CosineWithRestarts，但是重启更加突然。\n",
    "\n",
    "选择合适的学习率调度器可以根据具体的任务和模型架构来优化训练效果。例如，预热期可以帮助模型在训练初期稳定下来，避免因为过高的初始学习率导致的训练不稳定。而逐渐减小的学习率有助于模型在训练后期细化学习，进一步提高性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289594e1-abe4-440f-a456-8cd57c94c6c3",
   "metadata": {},
   "source": [
    "微调参数定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41018a4-e28f-4b8a-9c99-aa3283a71279",
   "metadata": {},
   "source": [
    "```\n",
    "################################################################################\n",
    "# SFT参数\n",
    "################################################################################\n",
    "\n",
    "# 使用的最大序列长度\n",
    "max_seq_length = None\n",
    "\n",
    "# 将多个短示例打包在同一输入序列中以提高效率\n",
    "packing = False\n",
    "\n",
    "# 将整个模型加载到GPU 0上\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1585d-eeca-47cd-ba26-080e9fef389f",
   "metadata": {},
   "source": [
    "加载数据集（在第二部分定义的）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb8ab5-65c3-4bbe-a293-4f2fb1a87880",
   "metadata": {},
   "source": [
    "```\n",
    "# 第1步：加载数据集（您可以在此处处理它）\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf37b45-d92d-4c29-b92e-cf254bc49e29",
   "metadata": {},
   "source": [
    "加载配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f155999d-85a4-4740-9a80-006fe1af89a9",
   "metadata": {},
   "source": [
    "```\n",
    "# 第2步：加载配置\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,                        # 在4比特中加载\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,      # 4比特量化类型\n",
    "    bnb_4bit_compute_dtype=compute_dtype,         # 4比特计算数据类型\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,   # 使用4比特的双重量化\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f69fe-480e-4b15-8596-14c4db364ed9",
   "metadata": {},
   "source": [
    "检查GPU是否满足训练条件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b2b44-d79a-401e-8820-fafdd3559e1e",
   "metadata": {},
   "source": [
    "```\n",
    "# 第3步：检查GPU对bfloat16的兼容性\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"您的GPU支持bfloat16：使用bf16=True加速训练\")\n",
    "        print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0e743e-14ef-4148-bb8d-ae7d02c09c76",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d77b6a-9234-4f45-9d1d-35a29c008243",
   "metadata": {},
   "source": [
    "```\n",
    "# 第4步：加载基础模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,    # 量化配置\n",
    "    device_map=device_map              # 设备映射\n",
    ")\n",
    "model.config.use_cache = False         # 关闭缓存使用\n",
    "model.config.pretraining_tp = 1        # 预训练的TP参数设置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ba454-5210-49f1-ae36-fa62bd7b09fe",
   "metadata": {},
   "source": [
    "加载分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5dfeb-5baf-42c1-a0b5-aaa1153a10b5",
   "metadata": {},
   "source": [
    "```\n",
    "# 第5步：加载LLaMA分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # 从预训练模型加载分词器，信任远程代码\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})                           # 添加特殊的填充令牌\n",
    "tokenizer.pad_token = tokenizer.eos_token                                      # 将填充令牌设置为结束令牌\n",
    "tokenizer.padding_side = \"right\"                                               # 设置填充方向为右侧\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e47ca4-8e57-4389-bae2-a9ddff771954",
   "metadata": {},
   "source": [
    "加载LoRA配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4ae74-33a6-4c46-9c83-d291a9a62254",
   "metadata": {},
   "source": [
    "```\n",
    "# 第6步：加载LoRA配置\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,          # LoRA缩放的Alpha参数\n",
    "    lora_dropout=lora_dropout,      # LoRA层的Dropout概率\n",
    "    r=lora_r,                       # LoRA注意力维度\n",
    "    bias=\"none\",                    # 偏置设置\n",
    "    task_type=\"CAUSAL_LM\",          # 任务类型：因果语言模型\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e9d83-0c16-41e4-8a54-dc477cd0c0ab",
   "metadata": {},
   "source": [
    "设置训练参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f90eef-942e-4ed8-9d59-eff414d55ad5",
   "metadata": {},
   "source": [
    "```\n",
    "# 第7步：设置训练参数\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                            # 模型预测和检查点将被存储的输出目录\n",
    "    num_train_epochs=num_train_epochs,                # 训练周期数\n",
    "    per_device_train_batch_size=per_device_train_batch_size,  # 训练时每个GPU的批量大小\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # 累积梯度更新步数\n",
    "    optim=optim,                                      # 使用的优化器\n",
    "    save_steps=save_steps,                            # 每X更新步骤保存一次检查点\n",
    "    logging_steps=logging_steps,                      # 每X更新步骤记录一次\n",
    "    learning_rate=learning_rate,                      # 初始学习率\n",
    "    weight_decay=weight_decay,                        # 权重衰减\n",
    "    fp16=fp16,                                        # 启用fp16训练\n",
    "    bf16=bf16,                                        # 启用bf16训练\n",
    "    max_grad_norm=max_grad_norm,                      # 最大梯度正则化（梯度裁剪）\n",
    "    max_steps=max_steps,                              # 训练步数（覆盖num_train_epochs）\n",
    "    warmup_ratio=warmup_ratio,                        # 线性预热的步骤比例\n",
    "    group_by_length=group_by_length,                  # 将具有相同长度的序列分组到批次中\n",
    "    lr_scheduler_type=lr_scheduler_type,              # 学习率调度\n",
    "    report_to=\"tensorboard\"                           # 报告方式（TensorBoard）\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc540be-281d-4baa-bded-8179813e08ee",
   "metadata": {},
   "source": [
    "设置微调参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f39fe-647d-4174-9234-d9789abf4b16",
   "metadata": {},
   "source": [
    "```\n",
    "# 第8步：设置监督式微调参数\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                          # 使用的模型\n",
    "    train_dataset=dataset,                # 训练数据集\n",
    "    peft_config=peft_config,              # PEFT（参数高效微调）配置\n",
    "    dataset_text_field=\"text\",            # 数据集中文本字段的名称\n",
    "    max_seq_length=max_seq_length,        # 使用的最大序列长度\n",
    "    tokenizer=tokenizer,                  # 使用的分词器\n",
    "    args=training_arguments,              # 训练参数\n",
    "    packing=packing,                      # 是否打包多个短示例以提高效率\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576821b-481a-4af4-abf6-745cd986566a",
   "metadata": {},
   "source": [
    "训练并保存微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ca5cb-be6b-46eb-ac46-78a747200ce7",
   "metadata": {},
   "source": [
    "```\n",
    "# 第9步：训练模型\n",
    "trainer.train()\n",
    "\n",
    "# 第10步：保存训练后的模型\n",
    "trainer.model.save_pretrained(new_model)  # 保存经过训练的模型到指定的名称\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795fa35-a1db-40e0-a8ed-52d77bc55215",
   "metadata": {},
   "source": [
    "<img src=\"image/training.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3102a2-11a6-4777-8aaa-5f9683ef9bc7",
   "metadata": {},
   "source": [
    "### 结果展示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa39d3-9097-42ac-9d08-f3344e261bd7",
   "metadata": {},
   "source": [
    "#### 新模型加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879fe6d-e004-4b6a-9ce7-5ed25bf7777f",
   "metadata": {},
   "source": [
    "由于PEFT模型一般训练时为了节省存储空间只储存被调整的参数部分，所以加载时需要基础模型与微调模型一起加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f72e3b2-fe6c-4449-9a7a-8b1b7e05e448",
   "metadata": {},
   "source": [
    "```\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb600147-479d-41f5-baad-cce041730237",
   "metadata": {},
   "source": [
    "<img src=\"image/result.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e6df54-a3a4-43f1-94cb-ae9538f5fb92",
   "metadata": {},
   "source": [
    "## 三、微调模型的数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca93d9a-03c2-49b7-b127-1349f8fb876e",
   "metadata": {},
   "source": [
    "本章节内容：\n",
    "- 数据收集\n",
    "- 数据预处理\n",
    "- 数据质量的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a26a69-5701-4963-bc58-24bf1739d4c6",
   "metadata": {},
   "source": [
    "### 数据收集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f438a7-2105-4f33-8211-b870087df819",
   "metadata": {},
   "source": [
    "#### 语料库/语料\n",
    "语料库是在自然交流环境中创建的大量有组织的机器可读文本集合。语料库的复数形式是语料。\n",
    "\n",
    "这些语料可以通过各种方式生成，如电子文本来源、口语转录、光学字符识别等。\n",
    "\n",
    "相较于较小的语言模型，大型语言模型（LLMs）需要更高质量的数据进行预训练，其模型容量主要取决于预训练语料及其预处理。\n",
    "\n",
    "#### 数据来源\n",
    "从各种来源收集大量自然语言语料库对于创建一个熟练的LLM至关重要。目前存在的LLMs主要使用各种公共文本数据集的组合作为它们的预训练语料。\n",
    "\n",
    "所使用的预训练数据可分为两大类：通用数据和专业数据。\n",
    "\n",
    "大多数LLMs使用通用数据（如网页、书籍和对话）作为它们的预训练语料，因为这些数据丰富、多样且易于获取。这有助于提高它们的语言建模和泛化能力。\n",
    "\n",
    "然而，一些研究探索了使用特殊数据集，如多语言数据、科学数据和代码，以赋予LLMs特定的解决问题能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff3af3-1ef5-4069-92ae-6042aa2f7c6c",
   "metadata": {},
   "source": [
    "**各模型训练数据来源比例**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed33442-94b7-47f3-a40b-0b2feaa2a5fc",
   "metadata": {},
   "source": [
    "<img src=\"image/数据来源.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd32c2d-fab2-4066-946a-7586c9c87e5d",
   "metadata": {},
   "source": [
    "图片展示了不同人工智能模型按照它们使用的数据集大小进行比较。数据集大小以GB计量，并且每个模型根据不同类型的数据源分了不同的颜色和柱子。这些数据源包括维基百科、书籍、学术期刊、论坛数据 、Common Crawl以及其他。\n",
    "“CC”代表“Common Crawl”，这是一个公共的网页数据集，包含了从互联网上抓取的大量网页文本数据。\n",
    "Common Crawl经常被用来训练大型语言模型，因为它是一个广泛的、多样化的文本来源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f5fb14-2617-40e2-8e83-d23d48bd2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三原色原理_百度百科 网页新闻贴吧知道网盘图片视频地图文库百科百度首页进入词条全站搜索帮助首页秒懂百科特色百科知识专题加入百科百科团队权威合作下载百科APP个人中心三原色原理播报讨论上传视频色度学最基本的原理收藏查看我的收藏0有用+10本词条由“科普中国”科学百科词条编写与应用工作项目 审核 。人眼对红、绿、蓝最为敏感，人的眼睛像一个三色接收器的体系，大多数的颜色可以通过红、绿、蓝三色按照不同的比例合成产生。同样，绝大多数单色光也可以分解成红、绿、蓝三种色光，这是色度学的最基本的原理，也称三原色原理。中文名三原色原理外文名three-primary colours颜    色红、绿、蓝产    生比例合成产生地    位色度学最基本的原理所属领域物理学目录1词语简介2基本内容3配色原理▪原色理论▪混色理论4三原色的应用5相关概念▪色度学▪光谱表色法词语简介播报编辑白光通过棱镜后被分解成多种颜色逐渐过渡的色谱，颜色依次为红、橙、黄、绿、蓝、靛、紫 [4]，这就是可见光谱。其中人眼对红、绿、蓝最为敏感，人的眼睛就像一个三色接收器的体系，大多数的颜色可以通过红、绿、蓝三色按照不同的比例合成产生。同样绝大多数单色光也可以恋享愚分解成红绿蓝三种色光。这是色度学的最基本原理，即三基色原理。三种基色是相互独立的，任何一种基色都不能有其它两种颜色合成。红绿蓝是三基色，这三种颜色合成的颜色范围最为广泛。红绿蓝三基色按照不同的比例相加合成混色称为相加混色。红色+绿雅旬店色=黄色绿色+她夜糠微酷踏蓝色=青色红色炼备背+蓝色=亮誉酷断紫色红色+绿色归她+蓝色=整探墓白色基本内容播报编辑彩色全息显示实际上是一种三维彩色图像传递技术。 根据色度学原理，利用红光、 绿光和蓝光经过适当比例混合后，可产生自然界大部分的颜色，因此把红光、 绿光和蓝光称作三原色。 因此彩色全息图的颜色记录和再现实质上是三原色的记录和再现。任何光都可以用红、绿、蓝这3种光按不同的比例混合而成，这就是三原色原理。三原色的原理可解释如下：（1）自然界的任何光色都可以由3种光色按不同的比例混合而成。（2）三原色之间是相互独立的，任何一种光色都不能由其余的两种光色来组成。（3）混合色的饱和度由3种光色的比例来决定。混合色的亮度为3种光色的亮度之和。配色原理播报编辑原色理论三原色，所谓三原色，就是指这三种色中的任意一色都不能由另外两种原色混合产生，而其它色可由这三色按照一定的比例混合出来，色彩学上将这三个独立的色称为三原色。混色理论色彩的混合分为加法混合和减法混合，色彩还可以在进入视觉之后才发生混合，称为中性混合。（一）加法混合 加法混合是指色光的混合，两种以上的光混合在一起，光亮度会提高，混合色的光的总亮度等于相混各色光亮度之和。色光混合中，三原色是红、绿、蓝。这三色光是不能用其它别的色光相混而产生的。而：红光+绿光=黄光绿光+蓝光=青光蓝光+红光=紫光黄光、青光、紫光为间色光。如果只通过两种色光混合就能产生白色光，那么这两种光就是互为补色。例如：红色光与青色光；绿色光与紫色光；蓝色光与黄色光。（二）减法混合减法混合主要是指的色料的混合。白色光线透过有色滤光片之后，一部分光线被反射而吸收其余的光线，减少掉一部分辐射功率，最后透过的光是两次减光的结果，这样的色彩混合称为减法混合。一般说来，透明性强的染料，混合后具有明显的减光作用。减法混合的三原色是加法混合的三原色的补色，即：翠绿的补色红（品红）、蓝紫的补色黄（淡黄）、朱红的补色蓝（天蓝）。用两种原色相混，产生的颜色为间色：红色+蓝色=紫色黄色+红色=橙色黄色+蓝色=绿色如果两种颜色能产生灰色或黑色，这两种色就是互补色。三原色按一定的比例相混，所得的色可以是黑色或黑灰色。在减法混合中，混合的色越多，明度越低，纯度也会有所下降。（三）中性混合 中性混合是基于人的视觉生理特征所产生的视觉色彩混合，而并不变化色光或发光材料本身，混色效果的亮度既不增加也不减低，所以称为中性混合。有两种视觉混合方式：A颜色旋转混合：把两种或多种色并置于一个圆盘上，通过动力令其快速旋转，而看到的新的色彩。颜色旋转混合效果在色相方面与加法混合的规律相似，但在明度上却是相混各色的平均值。B空间混合：将不同的颜色并置在一起，当它们在视网膜上的投影小到一定程度时，这些不同的颜色刺激就会同时作用到视网膜上非常邻近的部位的感光细胞，以致眼睛很难将它们独立地分辨出来，就会在视觉中产生色彩的混合，这种混合称空间混合。三原色的应用播报编辑在可见光光谱中，人眼对红、绿、蓝颜色最为敏感，人的眼睛就像一个三色接收器的体系，其他颜色可以通过红、绿、蓝三色按照不同的比例合成产生，这就是三基色原理。三种基色是相互独立的，任何一种基色都不能由其他两种基色合成。红、绿、蓝三基色按照不同的比例相加合成混色称为相加混色。红色+绿色=黄色，绿色+蓝色=青色，红色+蓝色=亮紫色，红色+绿色+蓝色=白色。另外，红色+青色=白色，绿色+紫色=白色，蓝色+黄色=白色。青色、紫色、黄色分别又是红色、绿色、蓝色的补色。除了相加混色法之外还有相减混色法。相减混色就是以吸收三基色比例不同而形成不同的颜色。由于白色一红色=青色，白色-绿色=品红，白色-蓝色=黄色，所以把青色、品红、黄色称为颜料三基色。用以上的相加混色三基色所表示的颜色模式称为RGB模式，而用相减混色三基色原理所表示的颜色模式称为CMYK模式，它们广泛运用于绘画和印刷领域 [1]。相关概念播报编辑色度学色度学是研究颜色度量和评价方法的一门学科，是颜色科学领域的一个重要部分。色度学要解决颜色的度量问题，首先必须找到外界光刺激与色知觉量之间的对应关系，以便能用光物理量的测量间接测得色知觉量。 [2]光谱表色法所谓光谱表色法就是以分光光度曲线来表示颜色特性的方法。非发光物体的颜色是由它对入射光选择性吸收后的反射光或透射光的光谱成分所决定的，而发光体的颜色即由它直接发射的光谱成分所决定。 [3]新手上路成长任务编辑入门编辑规则本人编辑我有疑问内容质疑在线客服官方贴吧意见反馈投诉建议举报不良信息未通过词条申诉投诉侵权信息封禁查询与解封©2024 Baidu 使用百度前必读 | 百科协议 | 隐私政策 | 百度百科合作平台 | 京ICP证030173号 京公网安备11000002000001号\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://baike.baidu.com/item/%E4%B8%89%E5%8E%9F%E8%89%B2%E5%8E%9F%E7%90%86/6969780?fr=ge_ala\"\n",
    "\n",
    "# 发送请求获取网页内容\n",
    "response = requests.get(url)\n",
    "\n",
    "# 检查请求是否成功\n",
    "if response.status_code == 200:\n",
    "    # 使用BeautifulSoup解析HTML内容\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # 提取并打印整个网页的文本内容\n",
    "    # 根据需要，可以修改这部分代码以提取特定元素\n",
    "    text = soup.get_text()\n",
    "    print(text)\n",
    "else:\n",
    "    print(\"请求失败，状态码：\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9e893-9d28-4814-b9cd-3e8965eeffe1",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58fcba-547c-47e9-b168-d9c893cfde91",
   "metadata": {},
   "source": [
    "为LLMs创建预训练语料库时，对收集到的文本数据进行预处理以去除噪音、冗余、不相关和潜在有害内容是非常重要的。\n",
    "\n",
    "这是因为数据质量会显著影响语言模型的能力和性能。本节讨论了各种数据预处理策略，以提高收集数据的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b779f0d-2f4e-4cea-a771-3f497e6cd407",
   "metadata": {},
   "source": [
    "#### 数据标注 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88efaf65-963d-412b-96ba-55357fed0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')  # 从上级目录的.env文件加载环境变量\n",
    "from openai import OpenAI\n",
    "from functools import lru_cache\n",
    "\n",
    "class BaseLLM:\n",
    "    @lru_cache(maxsize=1024)  # 使用LRU缓存来存储最近使用的请求结果\n",
    "    def chat(self, text):\n",
    "        return self._chat(text)\n",
    "    \n",
    "    def _chat(self, text):\n",
    "        raise NotImplementedError  # 如果直接调用_base类的_chat方法，则抛出未实现错误\n",
    "\n",
    "class OpenAILLM(BaseLLM):\n",
    "    def __init__(self, model_name):\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),  # 从环境变量获取API密钥\n",
    "            base_url=os.getenv(\"OPENAI_API_BASE\")  # 从环境变量获取API基础URL\n",
    "        )\n",
    "        self.model_name = model_name  # 设定模型名称\n",
    "        self.conversation_history = []  # 初始化对话历史列表\n",
    "\n",
    "    def convert_text_to_prompt(self, instr, target):\n",
    "        return instr.format(target)  # 格式化指令文本\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.chat(text)  # 允许对象像函数一样被调用，内部转发到chat方法\n",
    "        \n",
    "    def chat(self, text, temperature=0, messages=[], stops=None):\n",
    "        return self._chat(text, temperature, messages, stops)\n",
    "        \n",
    "    def _chat(self, text, temperature, messages=[], stops=None):\n",
    "        if not messages:\n",
    "            messages = [{\"role\": \"user\", \"content\": text}]  # 如果没有提供消息，创建一个用户角色的消息\n",
    "        print(\"开始请求模型%s\" % (self.model_name))  # 打印日志，表示开始请求模型\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            stream=False,\n",
    "            stop=stops,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content  # 返回模型的回答\n",
    "        \n",
    "    def history_chat(self, text, message=[], stops=None):\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": text})  # 添加用户消息到对话历史\n",
    "\n",
    "        # 创建请求\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=self.conversation_history,  # 传递完整的对话历史\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        # 将模型的回答添加到对话历史中\n",
    "        answer = response.choices[0].message.content\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        return answer\n",
    "\n",
    "gpt_3_5_turbo = OpenAILLM(\"gpt-3.5-turbo\")  # 创建一个使用gpt-3.5-turbo模型的对象\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c74867c1-0dc5-4b44-83ef-883b23ac2516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始请求模型gpt-3.5-turbo\n",
      "[\n",
      "  {\n",
      "    \"instruction\": \"三原色原理是什么？\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"三原色原理是指人眼对红、绿、蓝最为敏感，大多数的颜色可以通过红、绿、蓝三色按照不同的比例合成产生。\"\n",
      "  },\n",
      "  {\n",
      "    \"instruction\": \"三原色的中文名和外文名分别是什么？\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"三原色的中文名是三原色原理，外文名是three-primary colours。\"\n",
      "  },\n",
      "  {\n",
      "    \"instruction\": \"三原色的颜色是什么？\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"三原色的颜色是红、绿、蓝。\"\n",
      "  },\n",
      "  {\n",
      "    \"instruction\": \"三原色的应用有哪些？\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"三原色的应用包括彩色全息显示和绘画、印刷领域。\"\n",
      "  },\n",
      "  {\n",
      "    \"instruction\": \"什么是色度学？\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"色度学是研究颜色度量和评价方法的一门学科，是颜色科学领域的一个重要部分。\"\n",
      "  },\n",
      "  {\n",
      "    \"instruction\": \"什么是光谱表色法？\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"光谱表色法是一种以分光光度曲线来表示颜色特性的方法，用于描述非发光物体和发光体的颜色。\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(gpt_3_5_turbo.chat(\"以json格式梳理下面的文本，请将下面的文本分成{instruction: 问题,input:不填写 ,output: 答案}的格式，并且将多个问题答案对放入列表中\" + text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f769c-4acf-46e1-abc7-b1d88b1bf91f",
   "metadata": {},
   "source": [
    "#### 质量过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab677af-653b-428d-b41c-0dff91a22713",
   "metadata": {},
   "source": [
    "确保训练语料库只包含高质量数据的两种主要方法：**基于分类器的方法和基于启发式的方法**。\n",
    "\n",
    "基于分类器的方法训练一个二元分类器来识别和过滤低质量数据，以高质量文本（例如，维基百科页面）作为正例，候选数据作为负例。\n",
    "\n",
    "然而，基于分类器的方法可能无意中移除了方言、口语和社会方言等高质量文本，导致偏见和语料库多样性的减少。\n",
    "\n",
    "相比之下，有些**启发式的方法采用精心设计的规则**来排除低质量文本。\n",
    "\n",
    "这些规则包括基于语言的过滤、基于度量的过滤、基于统计的过滤和基于关键字的过滤。\n",
    "\n",
    "基于语言的过滤删除与LLM任务无关的语言文本，而基于度量的过滤使用评估度量，如**困惑度**来检测不自然的句子。基于统计的过滤根据语料库的统计特征来衡量文本质量，而基于关键字的过滤则根据特定的关键字集（如HTML标签、超链接、样板文本和冒犯性词汇）来移除噪声或无用元素。\n",
    "\n",
    "**困惑度**定义为语言模型对测试集的概率的倒数的几何平均。具体来说，对于一个长度为 \\(N\\) 的句子 \\(w_1, w_2, ..., w_N\\)，其困惑度的计算公式为：\n",
    "\n",
    "$$\n",
    "PP(W) = P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}}\n",
    "$$\n",
    "\n",
    "其中 $$P(w_1, w_2, ..., w_N) = P(w_1) \\times P(w_2|w_1) \\times ... \\times P(w_N|w_1, w_2, ..., w_{N-1})$$\n",
    "\n",
    "它是模型赋予该句子的概率。如果模型对句子中的词序列预测得越准确，这个概率就越高，从而困惑度就越低。\n",
    "\n",
    "困惑度可以用来检测不自然的句子，因为不自然的句子（例如语法错误的句子）通常会有更高的困惑度。这是因为这些句子的结构或用词可能与训练模型时使用的自然语言文本有显著差异。\n",
    "\n",
    "#### 示例：\n",
    "\n",
    "假设我们有一个训练好的语言模型（如已经完成预训练的GPT-2），我们可以利用它来评估以下两个句子的困惑度：\n",
    "\n",
    "1. 正常的句子：\"我今天\"\n",
    "2. 不自然的句子：\"我天今\"\n",
    "\n",
    "在中文中，句子2比句子1显得不自然。对于这个例子，GPT-2会针对每个词输出不同的概率分布，我们可以根据这个概率分布依照上面的公式计算句子的困惑度。\n",
    "\n",
    "例如，对于第一个句子\n",
    "\n",
    "$$ P(我，今，天) = P(我) \\times P(今|我) \\times P(天|我，今) $$\n",
    "\n",
    "对于第二个句子\n",
    "\n",
    "$$ P(我，天，今) = P(我) \\times P(天|我) \\times P(今|我，天) $$\n",
    "\n",
    "显然，第二个句子的困惑度可能会更高，因为‘我天’后面接‘今’的概率可能会比较低。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfb148-b7fa-437f-bc20-2d801e31e437",
   "metadata": {},
   "source": [
    "#### 去重复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989ea5e-44a4-4d78-aa3b-4685a7f9057c",
   "metadata": {},
   "source": [
    "以往的研究表明，在预训练语料库中存在**重复数据会减少语言模型的多样性**，可能导致训练过程中的不稳定，并对模型的性能产生负面影响。\n",
    "\n",
    "因此，从语料库中移除重复句子是必要的。这可以在不同的层面上进行，包括句子级、文档级和数据集级。\n",
    "\n",
    "在句子级别，包含重复短语或词汇的低质量句子应该被移除，以避免将重复模式引入模型中。\n",
    "\n",
    "在文档级别，先前的研究依赖于表面特征重叠（例如，词汇和n-gram）来识别和移除具有相似内容的重复文档。\n",
    "\n",
    "此外，为了防止训练集和评估集之间的污染，重要的是要从训练集中移除可能的重复文本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8c829-f477-4074-b1b4-730682e0b17a",
   "metadata": {},
   "source": [
    "#### 隐私编辑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6ac5f-c006-4d41-8bb1-cb398c171287",
   "metadata": {},
   "source": [
    "用于预训练大型语言模型（LLMs）的大部分文本数据来自网络，经常包含用户生成的内容，可能涉及敏感或个人信息。\n",
    "\n",
    "这带来了隐私泄露的风险，因此有必要从预训练语料库中移除**个人可识别信息（PII）**。\n",
    "\n",
    "基于规则的方法，如关键字检测，可以识别并移除姓名、地址和电话号码等PII。此外，预训练语料库中重复的PII数据可能使LLMs容易受到隐私攻击。\n",
    "\n",
    "因此，去重复也有助于降低隐私风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b80661-3239-466a-8763-1497e0782bae",
   "metadata": {},
   "source": [
    "### 数据质量的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7777aa-a05c-43f3-ad55-e1882d5cab88",
   "metadata": {},
   "source": [
    "反复训练LLMs是不可行的，因此，在训练LLM之前拥有一个高质量、准备充分的预训练语料库至关重要。在本节中，我们将探讨预训练语料库的质量和分布如何影响LLMs的性能。\n",
    "\n",
    "重复数据可能导致“双重下降”或压倒训练过程，并且会降低LLMs从上下文中复制的能力，可能影响其泛化能力。\n",
    "\n",
    "“双重下降”这个术语描述了一个机器学习现象，即模型的泛化误差随着模型复杂度的增加最初会降低，达到一个最小值，然后随着模型变得更复杂而再次上升。\n",
    "\n",
    "与普遍的看法相反，过拟合应该导致模型的泛化误差随着模型复杂度的不断增加而增加。\n",
    "\n",
    "第一次下降代表了传统的偏差与方差之间的权衡，即模型复杂度的增加会因为偏差的下降而导致对训练数据的更好拟合，但同时也会因为方差的增加而导致对测试数据的拟合变差。\n",
    "\n",
    "插值阈值是偏差和方差曲线汇合的地点。当模型复杂度超过这个点时，泛化误差又会降低。\n",
    "\n",
    "这种第二次下降可能是由于模型的正则化特性所引起的，这些特性可以防止**过拟合**并提高泛化性能。\n",
    "\n",
    "对低质量数据（如噪声、有害或重复数据）进行预训练会损害语言模型的性能。\n",
    "\n",
    "为了开发有效的LLM，考虑收集到的训练数据的数量和质量都很重要。包括T5、GLaM和Gopher在内的最新研究已经调查了数据质量对下游任务性能的影响。\n",
    "\n",
    "比较在过滤和未过滤数据上训练的模型发现，对清洁数据进行预训练可以提高性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6f726-5f7b-4604-83ff-9d06f8fea864",
   "metadata": {},
   "source": [
    "## 四、微调在模型训练中的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b742971-c74f-4392-9c85-fcb45dd6c797",
   "metadata": {},
   "source": [
    "<img src=\"image/pipeline.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9948dad-a9e5-4ba9-96c6-dc0b8d54c66f",
   "metadata": {},
   "source": [
    "### 大模型训练流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59441949-2546-4300-a4d9-a1454f99ed12",
   "metadata": {},
   "source": [
    "**预训练**\n",
    "<img src=\"image/预训练流程.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "- 从互联网上收集一个大型且多样化的数据集。这个数据集包含了来自广泛来源的文本，以确保模型学习到各种语言模式。\n",
    "- 清洗和预处理数据，去除噪声、格式问题以及不相关信息。\n",
    "- 将清洗后的文本数据进行分词处理，分解成更小的单元，如单词或子词单元（例如，字节对编码或WordPiece）。\n",
    "- 对于像GPT-3这样的大型语言模型（LLM），通常使用Transformer架构，因为它们在处理序列数据方面非常有效。\n",
    "- 大型语言模型（LLM）的预训练是通过训练模型预测文本序列中的下一个单词进行的，它使用了大量的数据集，使其能够理解和生成类似人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b53b8-9361-46a0-a3be-2bd3bdca829e",
   "metadata": {},
   "source": [
    "**预训练的输出**\n",
    "如果我们在仅仅完成预训练之后就使用模型，此时它基于输入的数据学会了如何“正确地”预测下一个单词，但我们可以注意到，在这时模型有一定概率会输出无意义的值，并且不会在正确的点停下来。\n",
    "<img src=\"image/预训练输出例子.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcd70e-45f1-4ad2-8bd9-5ae0a2e6334c",
   "metadata": {},
   "source": [
    "**SFT(Supervised fine-tuning) 或者 instruction tuning**\n",
    "- 在这个过程中，模型将用户的信息作为输入，以AI训练者的回应作为目标。模型通过最小化其预测和提供的回应之间的差异来学习生成回应。\n",
    "- 在这一阶段，模型能够理解指令的含义以及如何根据提供的指令从其记忆中检索知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1f45c-78fc-4ea6-bf3b-b4bff498e321",
   "metadata": {},
   "source": [
    "<img src=\"image/SFT流程.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e7672-9e69-418f-83ed-7e60aae37a50",
   "metadata": {},
   "source": [
    "**微调结果展示**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a49fe-cbef-4e9c-bb79-7c5e5876d650",
   "metadata": {},
   "source": [
    "<img src=\"image/SFT输出例子.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac6678-549d-48b2-a865-5f28f0d4b9e1",
   "metadata": {},
   "source": [
    "**Reward model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a4519-e5f5-4118-b185-e94b3a4a0550",
   "metadata": {},
   "source": [
    "<img src=\"image/ChatGPT应用大模型微调技术.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea8591-d354-4463-82fd-c767631e1412",
   "metadata": {},
   "source": [
    "**RLHF(Reinforcement Learning from Human Feedback)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab4c12-6725-4f82-b123-b1e09f0d7bd7",
   "metadata": {},
   "source": [
    "<img src=\"image/why rl.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd310d-90be-4866-9a68-66505c2566ed",
   "metadata": {},
   "source": [
    "我们将RHFL作为第二步微调，进一步根据我们讨论过的标准对模型进行校准：有益、诚实和无害。\n",
    "\n",
    "RHFL的目标是：\n",
    "最大化有益性，\n",
    "最小化伤害，\n",
    "避免危险话题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba7a96-5b06-46ef-9f3a-3eb0b56002e1",
   "metadata": {},
   "source": [
    "我们不会深入探讨强化学习的工作细节（可以讨论），总而言之，我们可以通过训练神经网络（NN）模型与环境互动，通过连续的决策来最大化累积的奖励信号。\n",
    "\n",
    "在RLHF中，我们会为同一个提示生成多个输出，并请测试人员对输出从最佳到最差进行排序。这些数据用于训练另一个称为奖励模型的神经网络模型。这个奖励模型现在能够理解人类偏好。可以将其视为由专家训练实习生，以识别有益、诚实和无害的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51e54b-cb5b-4ecd-ab75-26325b421c8b",
   "metadata": {},
   "source": [
    "<img src=\"image/reward.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7650646d-46d4-499e-bc00-27ef53d4ff5b",
   "metadata": {},
   "source": [
    "一旦奖励模型训练完成，它就可以代替人类来标记数据，基于这些反馈进一步在大规模上微调大型语言模型（LLM）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae605f5e-1c2f-4dec-b211-244d8275fef5",
   "metadata": {},
   "source": [
    "### 微调技术与提示工程（在未微调的模型上）的特性对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e11ccdc-f743-45e8-9f13-aa4aa56130b2",
   "metadata": {},
   "source": [
    "| 特性 | 提示工程 | 微调 |\n",
    "| ---- | -------- | ---- |\n",
    "| 可接受数据量 | 无法输入大量的数据，token有限制 | 理论上可以输入无限量的数据进行微调 |\n",
    "| 幻觉问题 | 模型幻觉难以纠正 | 纠正模型存在的错误信息 |\n",
    "| 面对大量数据的表现 | 如果输入大量数据，模型可能会遗忘 | 模型可以从大量数据中进行学习新的信息 |\n",
    "| 成本 | 使用成本低 | 微调需要大量算力成本 |\n",
    "| 使用门槛 | 使用门槛低 | 需要特定的技术能力 |\n",
    "| 使用场景 | 通用场景，快速启动项目和产品原型 | 行业应用，企业级应用 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195ffd7-8d76-4bd9-98d6-403b716beb77",
   "metadata": {},
   "source": [
    "## 五、PEFT方法对比 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ab235-52e7-4808-8a2b-99c2bb46be81",
   "metadata": {},
   "source": [
    "本章节内容：\n",
    "- Prefix Tuning 论文：[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)\n",
    "- LoRA 论文：[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- QLoRA 论文：[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)\n",
    "\n",
    "\n",
    "扩展阅读：\n",
    "- P-Tuning v1 论文：[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- P-Tuning v2 论文：[P-Tuning: Prompt Tuning Can Be \n",
    "Comparable to Fine-tuning Across Scales and Task](https://aclanthology.org/2022.acl-short.8.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445053a-391a-4fe2-9348-7a132280d1c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [Adapter](https://arxiv.org/pdf/1902.00751.pdf)\n",
    "\n",
    "<img src=\"image/adapter.png\"  width=\"400\" height=\"400\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "- Adapter位于Feed-Forward Layer之后、**残差连接**之前。Adapter本质上就是两层**MLP**，分别负责将Transformer的表征降维和升维（右图）。\n",
    "\n",
    "- 基于Adapter的方法， 只需要添加不到5%的可训练参数，就可以几乎达到全参数训练的效果 ，在训练过程中大大节省了训练时间，做到时间有效性。因此在真实场景应用时，不同的任务我们不需要重新对整个预训练模型进行微调，我们只需要保存Adapter即可 ，而预训练模型的其他参数都是原始预训练的，这样就做到了空间的有效性。\n",
    "\n",
    "#### 残差连接简介\n",
    "残差网络（ResNet）是一种深度神经网络架构，最初由微软研究院的Kaiming He等人在2015年提出。它通过引入“残差学习”的概念来解决深度网络中的退化问题，即随着网络层数的增加，网络性能反而下降的问题。\n",
    "\n",
    "残差网络的核心原理是引入了“残差链接”（Residual Connection），即将输入直接加到输出上，从而使网络可以学习输入与输出之间的残差。这种结构使得网络可以更容易地学习恒等映射，从而**缓解**了深度网络训练中的梯度消失问题。\n",
    "\n",
    "在传统的深度神经网络中，每一层都直接学习输入到输出的映射。而在残差网络中，每个单元会学习输入和输出之间的残差，即差异部分。具体来说，如果我们期望从层的输入x学到的目标映射为F(x)，残差网络不是直接学习这个映射，而是学习目标映射与输入之间的残差F(x) - x。这样，网络的输出就仍然等效于目标映射F(x)。\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./image/残差链接.jpg\" alt=\"替代文本\" width=\"764\" height=\"255\">\n",
    "</div>\n",
    "\n",
    "理论上，这样做能够简化与加速学习过程，因为学习残差往往比学习原始映射更容易。\n",
    "\n",
    "总的来说，残差网络通过引入跳跃连接，使得网络学习的输入与目标输出的差，从而有效解决了深度网络训练中的退化问题。你可以在论文[《Deep Residual Learning for Image Recognition》](https://arxiv.org/abs/1512.03385)中进一步了解残差网络的原理。\n",
    "\n",
    "\n",
    "#### 多层感知机（MLP）简介\n",
    "多层感知机（Multilayer Perceptron，MLP）是一种最基本的前馈神经网络，由**多个全连接层**组成。每个全连接层由多个神经元组成，每个神经元与上一层的所有神经元相连。MLP的核心是通过多个全连接层来学习输入数据的非线性映射。\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./image/MLP1.png\" alt=\"替代文本\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "从上图可以看出，每个全连接层就是一个线性变换（与权重W的矩阵乘法和偏置b矩阵加法）和一个非线性激活函数（如ReLU）的组合。这种结构使得MLP能够学习输入数据的复杂模式，从而适应各种不同的任务。\n",
    "\n",
    "总之，MLP通过非线性映射去学习输入与输出之间的复杂的变化模式，是一种最为基本的前馈神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb50ca5-d684-4c51-a82d-dabf52cbb833",
   "metadata": {},
   "source": [
    "### Prefix Tuning\n",
    "\n",
    "<img src=\"image/prefix1.jpg\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "<img src=\"image/prefix2.jpg\"  width=\"500\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "1. **提出原因**：为了减轻对于大型预训练语言模型进行全量微调的资源需求，Prefix-Tuning提供了一种轻量化的替代方案，减少了每个任务需要存储的模型副本。\n",
    "\n",
    "Prefix-tuning 是一种自然语言处理（NLP）领域的模型微调技术。这种技术的提出主要是为了解决以下问题：\n",
    "\n",
    "**参数效率低下**：传统的微调方法需要更新预训练模型的全部参数，这对于参数量巨大的模型来说既不经济也不高效。对于需要快速适应多个任务的应用场景，全参数微调需要大量的存储空间和计算资源。\n",
    "\n",
    "**迁移学习的灵活性有限**：预训练模型经过特定任务的全参数微调后，往往难以再次适应新的任务，因为原始预训练时学习到的知识可能被覆盖。\n",
    "\n",
    "**灾难性遗忘**：在微调过程中，模型可能会丢失在预训练阶段学到的一般性知识，这种现象被称为灾难性遗忘。这会导致模型无法保持在多个任务上的表现。\n",
    "\n",
    "**缓慢的部署和迭代**：全参数微调要求对每个新任务重新训练整个模型，这使得模型部署和迭代变得缓慢。\n",
    "\n",
    "如果不解决这些问题，可能会有以下严重的危害：\n",
    "\n",
    "**计算资源的浪费**：由于每个任务都需要全参数微调，使用大型预训练模型将变得不可持续，特别是在资源有限的环境中。\n",
    "\n",
    "**部署困难**：对于需要快速适应和更新的现实世界应用，模型的部署会因为微调的计算成本而变得十分困难。\n",
    "\n",
    "**模型性能下降**：灾难性遗忘会降低模型在多任务学习环境中的性能，影响模型在现实世界问题中的应用效果。\n",
    "\n",
    "**研究和应用的局限性**：对于研究者来说，没有一个高效的微调方法将限制模型的探索和创新。对于企业来说，缓慢的迭代周期会减缓产品的上市速度和市场竞争力。\n",
    "\n",
    "因此，Prefix-tuning 提出了一种优化少量参数的方法，通过在模型输入前加入一小段“前缀”序列来指导模型生成特定任务的响应，而不需要改变模型的大部分参数。\n",
    "\n",
    "\n",
    "2. **一般流程**：该方法优化一系列连续的、特定任务的向量序列（即前缀），这些前缀作为“虚拟tokens”加在输入之前，从而影响后续token的生成，而无需改变底层的模型参数。\n",
    "\n",
    "   Prefix-Tuning的流程可以更清晰地描述如下：\n",
    "\n",
    " ### Prefix-tuning的一般实现步骤：\n",
    "\n",
    "**选择基础模型**：选定一个已经预训练好的语言模型，如BERT、GPT-2等，作为微调的起点。\n",
    "\n",
    "**定义前缀**：为每个任务或每种输入类型创建一个可学习的前缀序列。这些前缀是一组连续的向量（或token），其长度远小于输入序列的长度。\n",
    "\n",
    "**集成前缀到模型中**：将前缀序列集成到模型的输入层或注意力层中。在输入层中，前缀可以直接添加到输入序列的开始；在注意力层中，前缀可以作为额外的键（K）和值（V）被注入到自注意力机制中。\n",
    "\n",
    "**微调训练**：在训练过程中，只有**前缀序列的参数和可能的少量模型参数**会被更新，模型的其他部分保持固定。训练数据将通过模型前向传播，同时携带前缀序列。\n",
    "\n",
    "**正则化和约束**：为了保持模型性能和避免过拟合，可以应用正则化方法，如dropout、权重衰减等，也可以对前缀参数的更新量设定约束。\n",
    "\n",
    "**评估和迭代**：使用验证集评估模型的性能，并根据需要对前缀参数和模型结构进行迭代调整。\n",
    "\n",
    "**部署**：完成微调后的模型可以部署到特定的任务中，用于推断新的输入数据。\n",
    "\n",
    "\n",
    "这个过程的关键在于前缀的设置和训练，它允许模型适应新任务，而无需重训练或改变大量的模型参数。\n",
    "\n",
    "\n",
    "图片展示了前缀长度与性能之间的关系。图表显示了随着前缀长度的增加，性能也在增加，这表明更长的前缀可以提供更多的表达能力，从而改善生成结果。同时，图表还指出了在训练和推断速度方面，更长的前缀对每个批次的影响可以忽略不计，因为GPU上的注意力计算是并行化的。\n",
    "\n",
    "<img src=\"image/pl.jpg\"  width=\"800\" height=\"600\" alt=\"prefix_length_Performance\"/>\n",
    "\n",
    "3. **方法样例**：在GPT-2上进行表格到文本的生成和在BART上进行摘要生成，表现与全量微调相当，但仅修改了0.1%的参数。\n",
    "\n",
    "\n",
    "4. **存在问题**：在某些任务上与全量微调相比有性能下降。平衡前缀长度和表达能力与过拟合的问题需要仔细调整。对于前缀调优如何改善外推能力以及在不同应用场景的潜在局限性，还需要进一步研究。\n",
    "\n",
    "Prefix-Tuning的核心思想，强调了它在优化语言模型性能的同时，解决了计算效率和存储限制的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f04500-ff4a-444a-978e-62fe87aec919",
   "metadata": {},
   "source": [
    "### 实现Prefix-tuning的具体编码示例（伪代码）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548059b-12ad-4059-82fc-fd720964242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "# 假设使用PyTorch和Huggingface的Transformers库\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "import torch\n",
    "\n",
    "# 加载预训练模型\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# 定义前缀\n",
    "prefix_length = 10  # 前缀的长度\n",
    "num_parameters = model.config.n_embd  # 每个前缀向量的维度\n",
    "prefix = torch.nn.Parameter(torch.zeros(1, prefix_length, num_parameters))\n",
    "\n",
    "# 微调过程\n",
    "optimizer = torch.optim.Adam([prefix] + [p for p in model.parameters() if p.requires_grad], lr=1e-5)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        inputs, labels = batch\n",
    "        # 将前缀添加到输入数据中\n",
    "        inputs_with_prefix = torch.cat((prefix.expand(inputs.size(0), -1, -1), inputs), dim=1)\n",
    "        outputs = model(inputs_with_prefix, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce0ef5-c2ad-4813-b541-7ef69718dd62",
   "metadata": {},
   "source": [
    "huggingface peft关于prefix-tuning的核心代码实现在[prefix_tuning](https://github.com/huggingface/peft/blob/main/src/peft/tuners/prefix_tuning/model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea75d1-47e2-4a66-b683-0d9e5fc6bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "class PrefixEncoder(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 是否使用前缀投影，一种在处理前缀时使用的特定技术\n",
    "        self.prefix_projection = config.prefix_projection\n",
    "        # 嵌入向量的维度\n",
    "        token_dim = config.token_dim\n",
    "        # 模型中的层数\n",
    "        num_layers = config.num_layers\n",
    "        # 编码器隐藏层的大小\n",
    "        encoder_hidden_size = config.encoder_hidden_size\n",
    "        # 虚拟（或者说“伪”）令牌的数量\n",
    "        num_virtual_tokens = config.num_virtual_tokens\n",
    "        if self.prefix_projection and not config.inference_mode:\n",
    "            # 如果使用前缀投影且不处于推理模式，则使用一个两层的多层感知机（MLP）来编码前缀\n",
    "            self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)\n",
    "            self.transform = torch.nn.Sequential(\n",
    "                torch.nn.Linear(token_dim, encoder_hidden_size),  # 从令牌维度到隐藏层大小的线性变换\n",
    "                torch.nn.Tanh(),  # 激活函数\n",
    "                torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim),  # 从隐藏层大小到最终输出维度的线性变换\n",
    "            )\n",
    "        else:\n",
    "            # 如果不使用前缀投影，或者处于推理模式，直接使用嵌入层映射虚拟令牌\n",
    "            self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * 2 * token_dim)\n",
    "\n",
    "    def forward(self, prefix: torch.Tensor):\n",
    "        # 前向传播函数，根据是否使用前缀投影来处理输入的前缀\n",
    "        if self.prefix_projection:\n",
    "            prefix_tokens = self.embedding(prefix)  # 获取前缀令牌的嵌入表示\n",
    "            past_key_values = self.transform(prefix_tokens)  # 通过MLP变换得到最终的键值对\n",
    "        else:\n",
    "            past_key_values = self.embedding(prefix)  # 直接\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bdfe5-20cc-47c1-ae83-34e4d5e15610",
   "metadata": {},
   "source": [
    "### LoRA\n",
    "\n",
    "<img src=\"image/LoRA.jpg\"  width=\"500\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "1. **提出原因**：为了在大型预训练语言模型（如GPT-3）上进行更高效的微调，同时减少存储和计算成本。\n",
    "\n",
    "2. **解决问题**：\n",
    "   - 减少大型模型微调所需的资源消耗，特别是减少训练时的GPU内存需求。\n",
    "   - 允许在保持原有预训练模型权重不变的情况下，通过微调少量参数来适应下游任务。\n",
    "   - 降低部署不同微调模型实例的成本，尤其是在模型规模非常大时。\n",
    "  \n",
    "   如果不解决高效微调的问题，那么在需要快速适应的实际应用中部署大型模型将变得困难。这将导致成本增加，限制了资源有限用户的使用，并可能减缓大型语言模型的研究和应用步伐。\n",
    "\n",
    "\n",
    "   \n",
    "与Prefix tuning的区别：\n",
    "更新的参数部分：LoRA主要更新的是模型内部的权重矩阵，而Prefix tuning更新的是模型输入的一部分。\n",
    "方法的侵入性：LoRA对模型的侵入性较小，因为它通过小规模的权重更新实现调整，而Prefix tuning则通过改变模型的输入实现调整。\n",
    "适用性：虽然两者都能实现任务的快速切换，但在实现细节和适应特定任务的方式上存在差异。\n",
    "\n",
    "3. **一般流程**：\n",
    "    - 模型选择和权重冻结：选择一个大型预训练语言模型（如GPT-3），并冻结它的所有原始权重。    \n",
    "    - 低秩分解矩阵的引入：在模型的每个层中引入两个低秩矩阵（记为A和B）。这些矩阵将与原始权重矩阵结合，形成更新后的权重。\n",
    "    - 优化低秩矩阵：在微调过程中，只对这些低秩矩阵的参数进行优化，而非整个模型的所有参数。\n",
    "    - 训练和应用：在特定下游任务的数据上训练这些参数。训练完成后，将训练好的低秩矩阵与原始权重结合，用于特定任务的预测或生成。\n",
    "    - 任务切换：在需要切换到不同的下游任务时，可以通过更换不同的低秩矩阵来实现，这个过程快速且内存开销小。\n",
    "\n",
    "\n",
    "4. **方法的样例**：\n",
    "   - 在GPT-3模型上应用LoRA进行自然语言理解（NLU）和自然语言生成（NLG）任务。\n",
    "   - 在保持原模型结构不变的情况下，只优化低秩矩阵参数来适应不同任务。\n",
    "\n",
    "5. **存在的问题**：\n",
    "   - LoRA的最优秩的确定以及如何适应特定任务的权重选择还需要进一步的研究和实验验证。\n",
    "\n",
    "\n",
    "LoRA的未来研究方向包括将其与其他高效的适应方法结合起来，以获得更大的改进，探索微调或LoRA成功背后的机制，并研究更原理化的权重选择方法进行适应。同时，也有兴趣了解LoRA更新中观察到的秩亏缺性是否暗示了原始预训练模型中类似的属性。\n",
    "\n",
    "\n",
    "<img src=\"image/lora.png\"  width=\"600\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e103d5-81aa-4c5b-b49f-72bb6065cedf",
   "metadata": {},
   "source": [
    "[1,2,3]\n",
    "[3,4,5]\n",
    "[5,6,7]\n",
    "\n",
    "\n",
    "3*1 1*3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb764d-b796-4a11-abca-b2e3f0fe09c4",
   "metadata": {},
   "source": [
    "LoRA层定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab128796-9b26-4e39-bb53-ecac08db4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        r: int,  # 'r' 是 LoRA 层的秩，用于控制 LoRA 层的参数数量\n",
    "        lora_alpha: int,  # 'lora_alpha' 是 LoRA 层的学习率缩放因子，用于调整学习速率\n",
    "        lora_dropout: float,  # 'lora_dropout' 是应用于 LoRA 层的dropout比例，用于正则化和减少过拟合\n",
    "        merge_weights: bool,  # 'merge_weights' 指示是否合并 LoRA 层的权重到原始模型权重\n",
    "    ):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        # 如果 lora_dropout 大于0，则使用指定的 dropout 比例，否则不应用 dropout\n",
    "        if lora_dropout > 0.:\n",
    "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            self.lora_dropout = lambda x: x  # 如果 dropout 比例为 0，则定义一个空操作\n",
    "        # 将权重标记为未合并\n",
    "        self.merged = False\n",
    "        self.merge_weights = merge_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46193052-12aa-42db-a921-686036e521ac",
   "metadata": {},
   "source": [
    "必要参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a3ebe-e086-4640-811d-f148955a8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Linear(nn.Linear, LoRALayer):\n",
    "    # 在一个Dense layer中实现LoRA\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int,  # 输入特征数\n",
    "        out_features: int,  # 输出特征数\n",
    "        r: int = 0,  # LoRA 参数 r，控制权重矩阵的秩\n",
    "        lora_alpha: int = 1,  # LoRA 参数 alpha，用于缩放权重\n",
    "        lora_dropout: float = 0.,  # LoRA 参数 dropout，用于权重的可选丢弃\n",
    "        fan_in_fan_out: bool = False,  # 如果要替换的层存储权重类似于 (fan_in, fan_out)，则将其设置为 True\n",
    "        merge_weights: bool = True,  # 是否合并权重\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        # 实际可训练参数\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))  # LoRA 参数 A\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))  # LoRA 参数 B\n",
    "            self.scaling = self.lora_alpha / self.r  # 缩放因子\n",
    "            # 冻结预训练的权重矩阵\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.transpose(0, 1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # 初始化 B 与 nn.Linear 默认方式相同，A 初始化为零\n",
    "            # 这与论文中描述的不同，但不应影响性能\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        '''切换训练模式'''\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "        nn.Linear.train(self, mode)\n",
    "        \n",
    "        # 权重合并时的逻辑\n",
    "        if mode:\n",
    "            if self.merge_weights and self.merged:\n",
    "                # 执行合并\n",
    "                if self.r > 0:\n",
    "                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling\n",
    "                # 确保训练时权重不再合并\n",
    "                self.merged = False\n",
    "        else:\n",
    "            if self.merge_weights and not self.merged:\n",
    "                # 执行权重合并并标记\n",
    "                if self.r > 0:\n",
    "                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n",
    "                self.merged = True       \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = F.linear(x, T(self.weight), bias=self.bias)            \n",
    "            result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return F.linear(x, T(self.weight), bias=self.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14468066-6296-4590-b27b-021d58cabdf2",
   "metadata": {},
   "source": [
    "### QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747b27c-89ef-42e3-a408-b8fd707787a9",
   "metadata": {},
   "source": [
    "#### 什么是模型量化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1371cf4-0e25-4dfb-af03-7a6bd34b968f",
   "metadata": {},
   "source": [
    "- 模型量化的定义：模型量化是指将神经网络中的权重和激活从浮点数（如32位浮点数）转换为定点数（如8位整数）。这个过程通常用于优化模型的大小和运算速度。\n",
    "\n",
    "- 常规精度模型：这通常指使用32位浮点数（FP32）来表示模型的权重和激活，这是许多神经网络默认的数值格式。\n",
    "\n",
    "- 低精度模型：在这里，低精度指的是使用比标准32位浮点数（FP32）更低的数值精度来表示模型的权重和激活。常见的格式包括16位浮点数（FP16，也称为半精度浮点）和8位定点整数（INT8）。目前，低精度通常指的是INT8，这是因为使用8位整数可以进一步减少模型的大小和计算需求。\n",
    "\n",
    "- 混合精度：混合精度训练是一种同时使用FP32和FP16的方法。在这种设置中，某些部分的模型使用FP16以减少内存使用和提高计算速度，而其他关键部分仍然使用FP32以保持计算的准确性。FP16减少了一半的内存需求，但是并不是所有的操作和参数都适合用FP16表示，因为这可能会导致精度损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebdc96-ed22-4ef8-8a73-7d31e09ef549",
   "metadata": {},
   "source": [
    "#### 什么是QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ffd89-41d3-4712-9662-5ad5c7e27949",
   "metadata": {},
   "source": [
    "<img src=\"image/QLoRA.jpg\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "**提出原因**：由于微调大型语言模型（如65亿参数模型）需要巨大的计算资源，对于许多研究者和实践者来说不可行。因此，QLoRA被提出，旨在大幅降低内存使用，使得在单个GPU上微调这类模型成为可能。\n",
    "\n",
    "QLoRA (Quantized Low-Rank Adaptation) 是一种高效的微调方法，旨在降低大型语言模型微调时的内存使用量，使得一个具有650亿参数的模型能够在单个48GB GPU上进行微调，同时保持完整的16位微调任务性能。QLoRA通过一个冻结的4比特量化预训练语言模型传播梯度到低秩适配器（LoRA）。此方法引入了多种创新以节省内存，包括一个新的数据类型4比特NormalFloat、双重量化以减少平均内存占用，以及页面优化器来管理内存峰值。\n",
    "\n",
    "QLoRA提出的原因是，尽管量化方法可以减少大型语言模型的内存占用，但这些技术仅适用于推理，并在训练期间失效。QLoRA解决了这一问题，首次实现了在不降低性能的情况下对量化模型进行微调。这一方法不仅解决了现有微调方法在内存需求上的局限性，而且还提高了模型训练的可达性和效率。\n",
    "\n",
    "如果不解决这一问题，将会导致在有限的硬件资源下无法有效微调大型语言模型，这将限制模型性能的提升以及其在特定任务上的适应性。在法律、医疗等对模型准确性要求极高的领域，这一限制可能会严重影响模型的实用性和可信度。因此，QLoRA的开发对推动大型语言模型的实用化和普及化具有重要意义。\n",
    "\n",
    "\n",
    "**解决问题**：QLoRA（Quantized Low-Rank Adaptation）和LoRA（Low-Rank Adaptation）都是为了提升大型预训练语言模型在特定任务上的性能和适应性，同时减少微调过程中的资源需求。LoRA通过在模型的权重矩阵中引入低秩结构来实现这一点，这样可以减少需要更新的参数数量，从而降低内存和计算成本。而QLoRA在LoRA的基础上进一步引入了量化技术，允许在更低的位宽（如4比特）下对模型进行微调，从而大幅减少了模型训练时的内存占用。\n",
    "\n",
    "不同之处在于，LoRA主要关注通过低秩结构提高参数效率，而QLoRA则在此基础上进一步通过量化技术，减少了存储和操作参数时所需的内存和计算资源。QLoRA的优势在于使得在资源受限的设备上微调大型模型成为可能，而不会牺牲模型的性能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae671f-afcb-4b18-81ec-305eabbb9db1",
   "metadata": {},
   "source": [
    "**存在问题**：在特定场景或任务中，QLoRA的量化和优化技术可能不够有效；在内存效率和模型性能之间的平衡可能需要针对特定应用进行微调；该方法可能在捕捉非常大数据集或极其复杂任务的细微之处上有限制。\n",
    "\n",
    "经过分析QLORA的论文，可以概括出它存在的一系列问题：\n",
    "\n",
    "- **数据质量 vs 数据量**：QLORA的实验结果表明，数据质量比数据量更重要。例如，较小但高质量的数据集在聊天机器人性能上优于大型数据集。\n",
    "\n",
    "- **性能与精确度的平衡**：QLORA使用4位精度进行微调，虽然在某些方面能够匹配16位全模型微调的性能，但在特定情况下仍存在性能损失。\n",
    "\n",
    "- **模型输出可靠性**：在某些情况下，QLORA生成的回答可能在表面上看起来自信，但实际上是不准确的。\n",
    "\n",
    "- **抗错误信息的能力**：QLORA在某些情况下显示出对错误信息的抵抗力，但这可能取决于特定的情境和问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3633d70-56df-42cb-a551-76d3f226bbd7",
   "metadata": {},
   "source": [
    "#### 方法调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a8411-7504-455f-8b89-b98654938f0c",
   "metadata": {},
   "source": [
    "SFT Trainer：https://huggingface.co/docs/trl/sft_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346db17-1cd8-4025-9b2c-8f46eb6ee742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA注意力维度\n",
    "lora_r = 64\n",
    "\n",
    "# LoRA缩放的Alpha参数\n",
    "lora_alpha = 16\n",
    "\n",
    "# LoRA层的丢弃概率\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# 加载LoRA配置\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,  # LoRA的缩放系数Alpha\n",
    "    lora_dropout=lora_dropout,  # LoRA层的丢弃概率\n",
    "    r=lora_r,  # LoRA的注意力维度\n",
    "    bias=\"none\",  # 偏置配置\n",
    "    task_type=\"CAUSAL_LM\",  # 任务类型为因果语言模型\n",
    ")\n",
    "\n",
    "# 从预训练加载模型，并配置设备映射\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map  # 设备映射配置\n",
    ")\n",
    "\n",
    "# 创建SFT（Structured Fine-Tuning）训练器\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # 指定模型\n",
    "    train_dataset=dataset,  # 训练数据集\n",
    "    peft_config=peft_config,  # PEFT（Parameter-Efficient Fine-Tuning）配置，此处为LoRA配置\n",
    "    dataset_text_field=\"text\",  # 数据集中文本字段的名称\n",
    "    max_seq_length=max_seq_length,  # 最大序列长度\n",
    "    tokenizer=tokenizer,  # 用于文本编码的分词器\n",
    "    args=training_arguments,  # 训练参数\n",
    "    packing=packing,  # 打包配置（用于更高效的训练）\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28278872-e67d-4484-83ef-d893bbe8c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA注意力维度\n",
    "lora_r = 64\n",
    "\n",
    "# LoRA缩放的Alpha参数\n",
    "lora_alpha = 16\n",
    "\n",
    "# LoRA层的丢弃概率\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# 激活4位精度的基础模型加载\n",
    "use_4bit = True\n",
    "\n",
    "# 计算4位基础模型的数据类型\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# 量化类型（fp4或nf4）\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# 为4位基础模型激活嵌套量化（双重量化）\n",
    "use_nested_quant = False\n",
    "\n",
    "# 加载LoRA配置\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,  # LoRA的缩放系数Alpha\n",
    "    lora_dropout=lora_dropout,  # LoRA层的丢弃概率\n",
    "    r=lora_r,  # LoRA的注意力维度\n",
    "    bias=\"none\",  # 偏置配置\n",
    "    task_type=\"CAUSAL_LM\",  # 任务类型为因果语言模型\n",
    ")\n",
    "\n",
    "# 使用QLoRA配置加载模型\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,  # 是否以4位精度加载模型\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,  # 4位量化的类型\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # 4位模型的计算数据类型\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,  # 是否使用嵌套量化\n",
    ")\n",
    "\n",
    "# 从预训练加载模型，并配置设备映射\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # 量化配置\n",
    "    device_map=device_map  # 设备映射配置\n",
    ")\n",
    "\n",
    "# 创建SFT（Structured Fine-Tuning）训练器\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # 指定模型\n",
    "    train_dataset=dataset,  # 训练数据集\n",
    "    peft_config=peft_config,  # PEFT（Parameter-Efficient Fine-Tuning）配置，此处为LoRA配置\n",
    "    dataset_text_field=\"text\",  # 数据集中文本字段的名称\n",
    "    max_seq_length=max_seq_length,  # 最大序列长度\n",
    "    tokenizer=tokenizer,  # 用于文本编码的分词器\n",
    "    args=training_arguments,  # 训练参数\n",
    "    packing=packing,  # 打包配置（用于更高效的训练）\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ec5fe-71c2-4396-84b6-68c70192b8b5",
   "metadata": {},
   "source": [
    "### 三种方法的对比\n",
    "\n",
    "1. **实现区别**:\n",
    "   - **Prefix-Tuning**：通过在模型的输入端添加一组可训练的向量（前缀）来调整模型的输出，不直接改变模型的权重。\n",
    "   - **LoRA**：通过向模型的线性层添加低秩结构（而不是替换原有权重）来微调模型，保持大部分预训练权重不变。\n",
    "   - **QLoRA**：对预训练模型进行4位高精度量化，添加少量可学习的低秩适配器（LoRA）权重，通过反向传播梯度调整这些权重。\n",
    "\n",
    "     \n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Adapter、Prefix-Tuning、LoRA、QLoRA等参数有效性学习方法（PEFT）本质上都是插入少量的新的参数，这些新的参数可以对预训练模型起到提示作用，只不过并不是以人类可读的离散的模板形式体现而已 。<b>\n",
    "</div>\n",
    "\n",
    "2. **优缺点**:\n",
    "   - **Prefix-Tuning**：\n",
    "     - 优点：灵活性高，适用于各种大小的模型，不改变原始模型。\n",
    "     - 缺点：可能在复杂任务上不如直接微调模型有效。\n",
    "   - **LoRA**：\n",
    "     - 优点：有效地微调大型模型而不需要大量资源，保持大部分预训练权重不变。\n",
    "     - 缺点：可能无法在所有任务上达到完全微调的效果。\n",
    "   - **QLoRA**：\n",
    "     - 优点：显著降低微调大型模型的内存需求，保持良好性能。\n",
    "     - 缺点：可能在特定场景下不如完全微调模型有效。\n",
    "\n",
    "3. **使用情况**:\n",
    "   - **QLoRA**：在资源受限但需要微调大型模型以保持或接近其原始性能的情况下使用。\n",
    "   - **LoRA**：在希望微调大型模型但限于资源，同时希望保持大部分预训练权重不变的情况下使用。\n",
    "   - **Prefix-Tuning**：在需要快速适应新任务，同时希望保持原始模型不变的情况下使用，适用于各种规模的模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5002e",
   "metadata": {},
   "source": [
    "# 六、可能有害的全量微调\n",
    "\n",
    "最后，我们介绍一篇相当有意思的文章，该文章的内容对我们使用指令微调将预训练模型改造为Chat模型和下游专业模型有一定指导意义。该文章的研究表明，对于经过**充分预训练**的模型，使用通用指令微调数据集进行**全量微调**可能对模型的效果有害。LoRA指令微调虽然在学习知识方面有限，但能有效保持模型的格式输出能力，且不会引入过多无关词汇，避免“幻觉”现象的发生。\n",
    "\n",
    "## 充分预训练后：Lora微调有局限，但很有用\n",
    "许许多多的大模型初学者都会遇到一个问题：如何在不损失模型性能的情况下，将模型应用到自己的任务中。这个问题的答案通常是微调。更为具体地说，通常是Lora微调，相比于全量微调，Lora微调可以更快地适应新任务，同时减少了计算资源的消耗。然而，研究表明，Lora微调不能使得模型学习到新的知识，而只是使得模型能够按照期望的格式输出。换句话说，经过Lora微调的模型仅仅是提升了更加充分地利用了自己预训练的知识，而并没有学到新的知识。\n",
    "\n",
    "### LoRA仅能够让模型学会输出的格式\n",
    "文章通过分析微调后模型的输出概率分布与预训练模型的输出概率分布的差异来衡量模型是否通过指令微调学习到了知识。具体来说，作者通过计算KL散度来衡量两个概率分布的差异。\n",
    "\n",
    "实验结果表明，模型的概率分布偏移的并不大。模型仅在前百分之五的概率分布中有比较大的KL散度发散，而在余下的概率分布中几乎保持不变，并且与全量训练相比，LoRA训练的KL散度偏移接近于0。这说明LoRA仅仅做到了学会输出的格式，而做不到学会具体的知识。体现在loss上我们可以发现，使用LoRA训练时模型收敛的非常快，然而在快速收敛之后loss保持平稳，无法进行进一步的下降。\n",
    "\n",
    "<img src=\"image/youhai_1.png\"  width=\"600\" height=\"400\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "### 增大数据集规模并不能解决问题\n",
    "为了验证LoRA训练的模型是否真的没有学到新的知识，现在许多研究将下游训练的指令微调数据集扩大到百万级的规模，这种做法并不能进一步提高模型的性能。即使将数据集的规模扩大52倍；扩大326倍，也没有作用。如下图所示，扩大数据集规模后LoRA训练的模型在五个维度上的表现都没有得到增强。\n",
    "\n",
    "<img src=\"image/youhai_2.png\"  width=\"800\" height=\"400\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "### 虽然LoRA微调有局限，但比全量微调强\n",
    "研究人员发现，全量微调会学习指令微调数据集中的用词导致严重的幻觉。团队研究了全量微调和LoRA微调后模型输出概率分布中的边缘偏移token和偏移token。发现LoRA训练后的偏移token常常为风格token，例如However和Typically。而全量微调中的偏移token包含了指令微调数据集中出现的所有token，也就是说全量微调可能会把指令微调数据集中的任何token利用到测试场景中，即使这些token与测试场景无关。\n",
    "\n",
    "<img src=\"image/youhai_3.png\"  width=\"600\" height=\"400\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "例如，上图的测试场景的提问为是什么导致了极光，而全量微调的模型大量使用了指令微调数据集中问题为“哪里能看到极光“的样本中的token，这导致了输出的内容偏离了实际的提问，而LoRA训练的模型则正确的回答了该问题。\n",
    "\n",
    "### 结论\n",
    "LoRA微调虽然不能使得模型学会新的知识，但是它可以使得模型更好地利用自己预训练的知识，同时避免了全量微调中可能出现的幻觉现象。在特定的指令数据集上，全量微调可能会学到一些无关的知识，并且这些无关的知识可能会对模型的性能产生严重的负面影响。因此，LoRA微调虽然有局限，但是通常是更好的选择。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ce2c0-b715-4db7-929a-9ec7ce6cce73",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1076689-0eac-4165-bda9-3443ab878066",
   "metadata": {},
   "source": [
    "一、大模型微调简介\n",
    "- 微调解决的问题\n",
    "- 微调到底是什么\n",
    "\n",
    "二、微调示例\n",
    "- 微调步骤\n",
    "- 微调过程展示\n",
    "\n",
    "三、微调模型数据准备\n",
    "- 数据收集\n",
    "- 数据预处理\n",
    "\n",
    "四、微调在模型训练中的位置\n",
    "- 大模型训练流程\n",
    "- 微调技术与提示工程对比\n",
    "\n",
    "五、PEFT方法对比\n",
    "- Prefix Tuning\n",
    "- LoRA\n",
    "- QLoRA\n",
    "\n",
    "六、可能有害的全量微调\n",
    "- Lora微调有局限，但很有用\n",
    "- 全量微调更加糟糕"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b1aae-1ecd-4199-97f1-e01adbcaf3bc",
   "metadata": {},
   "source": [
    "reference:\n",
    "- https://blog.csdn.net/weixin_39653948/article/details/132954262 概览\n",
    "- https://www.bilibili.com/video/BV1ry4y1F7QS?p=2&vd_source=ffb57dc7e05daab8f33d2ea7860289d0 概览\n",
    "- https://zhuanlan.zhihu.com/p/644122818 微调方法\n",
    "- https://medium.com/@masteringllm/llm-training-a-simple-3-step-guide-you-wont-find-anywhere-else-98ee218809e5 训练过程\n",
    "- https://cloud.tencent.com/developer/article/2315386 训练过程\n",
    "- https://arxiv.org/pdf/2106.09685.pdf Lora\n",
    "- https://arxiv.org/pdf/1902.00751.pdf Parameter-Efficient Transfer Learning for NLP \n",
    "- https://arxiv.org/pdf/2101.00190.pdf Prefix Tuning\n",
    "- https://arxiv.org/pdf/2103.10385.pdf P-Tuning\n",
    "- https://arxiv.org/pdf/2104.08691.pdf Prompt Tuning\n",
    "- https://arxiv.org/pdf/2303.15647.pdf PEFT survey\n",
    "- https://arxiv.org/pdf/2312.12148.pdf PEFT survey 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
