{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23baa409-ccbc-40d8-9230-9714c92c5921",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "一、数据集类型和重要性*\n",
    "\n",
    "二、数据处理常见流程***\n",
    "\n",
    "三、数据处理工具与框架**\n",
    "\n",
    "四、常见的数据处理案例分析*\n",
    "\n",
    "五、扩展学习*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0d165-46f2-4741-90ac-5fd550a28874",
   "metadata": {},
   "source": [
    "## 一、数据集类型和重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a30b07b-271e-415c-ba64-9fc9abaf82d6",
   "metadata": {},
   "source": [
    "### （一）数据集类型\n",
    "\n",
    "数据集作为大模型的基础，根据数据的组织形式和性质可以分为不同类型。\n",
    "\n",
    "数据集主要分为**结构化数据集**和**非结构化数据集**两类。\n",
    "结构化数据集是指数据以明确定义的格式存储，每条数据都按照相同的数据结构进行组织，常见的形式包括表格、数据库等；而非结构化数据集则是指数据没有固定的格式，包括文本、图像、音频等形式。\n",
    "\n",
    "![数据集类型2](images/数据集类型2.png)\n",
    "![数据集类型](images/数据集类型.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c806eb-6ab6-4e92-ad7e-811926ed423a",
   "metadata": {},
   "source": [
    "### （二）数据处理的重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47915f9-00da-419a-8cf8-d9d1dbcd62c8",
   "metadata": {},
   "source": [
    "在大模型的训练与应用中，数据处理是关键环节。数据的处理方式直接影响模型的性能、准确性与泛化能力。合理的数据处理能让模型更好地学习知识，减少错误与偏差。\n",
    "\n",
    "- 数据**数量**的重要性\n",
    "\n",
    "过拟合是指模型在训练数据上表现良好，但在未见过的数据上表现不佳。通常发生在模型的复杂度过高、训练数据有限或不平衡的情况下。通过使用大量的数据进行训练，可以减少过拟合的风险，使模型更好地适应各种陌生的输入，并在不同的情况下保持稳定的性能。\n",
    "\n",
    "- 数据**多样性**的重要性\n",
    "\n",
    "大模型的目标是能够适应各种不同的输入，并对未见过的数据进行准确的预测，多样化的数据可以使模型在各种任务和领域中表现出更好的泛化能力。如果是简单的同类型数据反馈，单条数据反馈和十条同类型数据反馈，虽然在数据的数量上增加了 10 倍，但模型的智能并没有得到拓展和增加。\n",
    "\n",
    "- 数据**质量**的重要性\n",
    "\n",
    "大模型“幻觉”指模型生成不正确、无意义或不真实的文本的现象，造成这一现象的主要原因是大模型缺乏高质量数据支撑。高质量的数据集可以帮助模型更好地理解和捕捉不同的概念、语义和语法结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1309f6-b5b3-4c0e-8f8b-f6fe58fea4a4",
   "metadata": {},
   "source": [
    "## 二、数据处理常见流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234a1a4-e494-4e12-85c1-4c821e862129",
   "metadata": {},
   "source": [
    "数据收集 → 数据清洗 → 数据标注  数据划分\n",
    "\n",
    "![数据处理流程](images/数据处理流程.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01befbb0-ddbe-48ec-95f8-40c99b64a433",
   "metadata": {},
   "source": [
    "### （一）数据收集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75209c3-83fa-43fb-8099-7f026466a270",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1、数据来源\n",
    "- 公开数据集\n",
    "  \n",
    "![公开数据集1](images/公开数据集1.png)\n",
    "![公开数据集2](images/公开数据集2.png)\n",
    "\n",
    "示例：Kaggle竞赛数据集\n",
    "https://www.kaggle.com/datasets\n",
    "![Kaggle竞赛数据集](images/Kaggle竞赛数据集.png)\n",
    "\n",
    "- 自有数据\n",
    "\n",
    "企业或特定组织在日常运营过程中会积累大量具有自身业务特色的数据，如电商平台所拥有的用户购买记录、浏览行为数据，社交媒体平台所记录的用户发布内容、社交互动信息等。\n",
    "\n",
    "**优势**：自有数据与业务紧密相关，能够精准反映特定场景下的用户行为和需求，用于训练模型时可使其更好地贴合实际应用场景。\n",
    "\n",
    "**注意事项**：在收集和使用自有数据时，需要严格遵守相关法律法规，特别是涉及用户隐私方面的规定，要对数据进行妥善的脱敏、加密等处理，确保数据安全合规。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa4a90-59d6-476b-8998-bf12d8b819d0",
   "metadata": {},
   "source": [
    "2、数据规模与多样性\n",
    "- 数据规模：大规模数据有助于模型学习到更丰富的语言模式和知识，但也要注意数据的质量和相关性，避免过多冗余或低质量数据影响模型训练效率和性能。\n",
    "- 多样性：增强模型的泛化能力，避免模型在训练过程中过度依赖于某一类特定的数据特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dbfbd1-4e88-46bd-8562-c2a0830b60c1",
   "metadata": {},
   "source": [
    "### （二）数据清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40553369-5b98-4bc5-aeb5-4dfe76f0620f",
   "metadata": {},
   "source": [
    "1、噪声数据处理\n",
    "- 乱码、特殊字符\n",
    "\n",
    "对于非ASCII字符集中的一些不明符号序列或者不符合文本编码规范的字符组合，通过正则表达式的匹配和替换操作，将其从数据中有效去除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "688be4c4-557e-44a8-ab10-fe2bf87c5a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original texts:\n",
      "This is a sample text with some noise: 乱码1 and special characters like @#$%^&*()\n",
      "Another line with noise: ",
      "乱码2 and more special characters!@#$%^&*()\n",
      "A third line of text 乱码3 with noise: @#$%^&*()\n",
      "\n",
      "Cleaned texts:\n",
      "This is a sample text with some noise 1 and special characters like \n",
      "Another line with noise 2 and more special characters\n",
      "A third line of text 3 with noise \n"
     ]
    }
   ],
   "source": [
    "# 多行文本噪声处理示例\n",
    "import re\n",
    "\n",
    "def clean_noise_data(text):\n",
    "    # 使用正则表达式去除非ASCII字符和一些特殊字符\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+|[^a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# 示例多行文本列表\n",
    "original_texts = [\n",
    "    \"This is a sample text with some noise: \\x80\\x81\\x82乱码1 and special characters like @#$%^&*()\",\n",
    "    \"Another line with noise: \\x83\\x84\\x85乱码2 and more special characters!@#$%^&*()\",\n",
    "    \"A third line of text 乱码3 with noise: @#$%^&*()\"\n",
    "]\n",
    "print(\"Original texts:\")\n",
    "for original_text in original_texts:\n",
    "    print(original_text)\n",
    "\n",
    "# 调用清理函数\n",
    "cleaned_texts = [clean_noise_data(text) for text in original_texts]\n",
    "print(\"\\nCleaned texts:\")\n",
    "for cleaned_text in cleaned_texts:\n",
    "    print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8dc5c5-8805-45ae-b409-5c32d952dd93",
   "metadata": {},
   "source": [
    "- 重复数据\n",
    "  \n",
    "可以运用代码快速准确地识别出重复的数据项，并将其从数据集中筛除。\n",
    "另外，一些数据库管理系统也提供了便捷的去重功能，可利用其对存储在数据库中的数据进行重复数据清理操作，确保数据的唯一性和简洁性，提高数据质量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30858420-81e2-45e1-b10e-817fa81bf3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "['apple', 'banana', 'apple', 'cherry', 'banana', 'banana', 'date']\n",
      "\n",
      "Filtered data:\n",
      "['apple', 'banana', 'cherry', 'date']\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(data):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in data:\n",
    "        hash_value = hash(item)\n",
    "        if hash_value not in seen:\n",
    "            seen.add(hash_value)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "# 示例数据，包含重复元素\n",
    "data = [\"apple\", \"banana\", \"apple\", \"cherry\", \"banana\", \"banana\", \"date\"]\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "\n",
    "# 调用函数去除重复数据\n",
    "filtered_data = remove_duplicates(data)\n",
    "print(\"\\nFiltered data:\")\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33badedc-609b-47fe-bd5b-96e96b39c5a2",
   "metadata": {},
   "source": [
    "2、低质量数据过滤  \n",
    "- 基于分类器的方法：训练二元分类器，以高质量文本为正例，候选数据为负例，识别和过滤低质量数据。但这种方法可能会误删一些有价值的方言、口语等文本，导致语料库多样性减少。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20423d04-dc05-4ee7-8ba3-2284f624342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 1.0\n",
      "\n",
      "Filtered data:\n",
      "This is a relatively long and informative sentence.\n",
      "Another long and meaningful statement.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def generate_dummy_data(high_quality_size, low_quality_size):\n",
    "    # 生成高质量数据，这里简单假设高质量数据是一些长句子\n",
    "    high_quality_data = [\n",
    "        \"This is a high-quality sentence with rich information. \" * np.random.randint(3, 10) for _ in range(high_quality_size)\n",
    "    ]\n",
    "    # 生成低质量数据，这里简单假设低质量数据是一些短句子\n",
    "    low_quality_data = [\n",
    "        \"Short sentence.\" * np.random.randint(1, 3) for _ in range(low_quality_size)\n",
    "    ]\n",
    "    # 组合成数据集，并创建标签\n",
    "    data = high_quality_data + low_quality_data\n",
    "    labels = [1] * len(high_quality_data) + [0] * len(low_quality_data)\n",
    "    return data, labels\n",
    "\n",
    "def train_binary_classifier(data, labels):\n",
    "    # 使用 TF-IDF 向量化文本数据\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    # 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, labels, test_size=0.2, random_state=42)\n",
    "    # 使用逻辑回归作为二元分类器\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # 评估分类器性能\n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    print(f\"Classifier accuracy: {accuracy}\")\n",
    "    return classifier, vectorizer\n",
    "\n",
    "def filter_low_quality_data(classifier, vectorizer, candidate_data):\n",
    "    # 向量化候选数据\n",
    "    X_candidate = vectorizer.transform(candidate_data)\n",
    "    # 预测候选数据的质量\n",
    "    predictions = classifier.predict(X_candidate)\n",
    "    # 过滤低质量数据\n",
    "    filtered_data = [\n",
    "        candidate_data[i] for i in range(len(candidate_data)) if predictions[i] == 1\n",
    "    ]\n",
    "    return filtered_data\n",
    "\n",
    "# 生成示例数据\n",
    "data, labels = generate_dummy_data(high_quality_size=100, low_quality_size=50)\n",
    "# 训练二元分类器\n",
    "classifier, vectorizer = train_binary_classifier(data, labels)\n",
    "\n",
    "# 假设的候选数据\n",
    "candidate_data = [\n",
    "    \"This is a relatively long and informative sentence.\",\n",
    "    \"Short and simple sentence.\",\n",
    "    \"Another long and meaningful statement.\",\n",
    "    \"Short one.\"\n",
    "]\n",
    "# 过滤候选数据\n",
    "filtered_data = filter_low_quality_data(classifier, vectorizer, candidate_data)\n",
    "print(\"\\nFiltered data:\")\n",
    "for data in filtered_data:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcb447-d487-415c-ac72-1a08008883db",
   "metadata": {},
   "source": [
    "- 启发式方法：采用基于语言的过滤（删除与任务无关的语言文本）、基于度量的过滤（如使用困惑度检测不自然句子）、基于统计的过滤（根据语料库统计特征衡量文本质量）和基于关键字的过滤（根据特定关键字集移除噪声或无用元素，如 HTML 标签、超链接、样板文本和冒犯性词汇）等规则排除低质量文本。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a2aac42-4635-48cf-bb19-276c02705f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered by language relevance:\n",
      "['This is a relevant text for the task. It contains useful information.', 'This is an irrelevant text with some noise like <html> and http://example.com', 'Another relevant text that is quite long and informative.']\n",
      "\n",
      "Filtered by perplexity:\n",
      "[]\n",
      "\n",
      "Filtered by word count:\n",
      "['This is a relevant text for the task. It contains useful information.', 'This is an irrelevant text with some noise like <html> and http://example.com', 'Another relevant text that is quite long and informative.']\n",
      "\n",
      "Filtered by keywords:\n",
      "['This is a relevant text for the task. It contains useful information.', 'This is an irrelevant text with some noise like  and ', 'A short sentence.', 'Another relevant text that is quite long and informative.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def remove_irrelevant_texts(texts, task_keywords):\n",
    "    \"\"\"\n",
    "    基于语言的过滤：删除与任务无关的语言文本\n",
    "    :param texts: 输入的文本列表\n",
    "    :param task_keywords: 与任务相关的关键字列表\n",
    "    :return: 过滤后的文本列表\n",
    "    \"\"\"\n",
    "    filtered_texts = []\n",
    "    for text in texts:\n",
    "        contains_keyword = any(keyword in text for keyword in task_keywords)\n",
    "        if contains_keyword:\n",
    "            filtered_texts.append(text)\n",
    "    return filtered_texts\n",
    "\n",
    "def perplexity(sentence, language_model):\n",
    "    \"\"\"\n",
    "    基于度量的过滤：使用困惑度检测不自然句子\n",
    "    :param sentence: 要计算困惑度的句子\n",
    "    :param language_model: 语言模型（这里假设是一个简单的概率分布）\n",
    "    :return: 句子的困惑度\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    N = len(tokens)\n",
    "    log_prob_sum = 0\n",
    "    for i in range(1, N):\n",
    "        bigram = (tokens[i - 1], tokens[i])\n",
    "        if bigram in language_model:\n",
    "            log_prob_sum += math.log(language_model[bigram])\n",
    "        else:\n",
    "            log_prob_sum += math.log(1e-10)  # 避免 log(0)\n",
    "    perplexity_value = math.exp((-1 / N) * log_prob_sum)\n",
    "    return perplexity_value\n",
    "\n",
    "def filter_by_perplexity(texts, language_model, threshold=100):\n",
    "    \"\"\"\n",
    "    根据困惑度过滤文本\n",
    "    :param texts: 输入的文本列表\n",
    "    :param language_model: 语言模型\n",
    "    :param threshold: 困惑度阈值\n",
    "    :return: 过滤后的文本列表\n",
    "    \"\"\"\n",
    "    filtered_texts = []\n",
    "    for text in texts:\n",
    "        perp = perplexity(text, language_model)\n",
    "        if perp < threshold:\n",
    "            filtered_texts.append(text)\n",
    "    return filtered_texts\n",
    "\n",
    "def statistical_filtering(texts, word_count_threshold=5):\n",
    "    \"\"\"\n",
    "    基于统计的过滤：根据语料库统计特征衡量文本质量\n",
    "    :param texts: 输入的文本列表\n",
    "    :param word_count_threshold: 单词数量阈值\n",
    "    :return: 过滤后的文本列表\n",
    "    \"\"\"\n",
    "    filtered_texts = []\n",
    "    for text in texts:\n",
    "        word_count = len(text.split())\n",
    "        if word_count >= word_count_threshold:\n",
    "            filtered_texts.append(text)\n",
    "    return filtered_texts\n",
    "\n",
    "def remove_keywords(texts, keywords):\n",
    "    \"\"\"\n",
    "    基于关键字的过滤：根据特定关键字集移除噪声或无用元素\n",
    "    :param texts: 输入的文本列表\n",
    "    :param keywords: 要移除的关键字列表\n",
    "    :return: 过滤后的文本列表\n",
    "    \"\"\"\n",
    "    filtered_texts = []\n",
    "    for text in texts:\n",
    "        for keyword in keywords:\n",
    "            text = re.sub(keyword, '', text)\n",
    "        filtered_texts.append(text)\n",
    "    return filtered_texts\n",
    "\n",
    "# 示例数据\n",
    "texts = [\n",
    "    \"This is a relevant text for the task. It contains useful information.\",\n",
    "    \"This is an irrelevant text with some noise like <html> and http://example.com\",\n",
    "    \"A short sentence.\",\n",
    "    \"Another relevant text that is quite long and informative.\"\n",
    "]\n",
    "\n",
    "# 与任务相关的关键字\n",
    "task_keywords = [\"relevant\", \"useful\", \"informative\"]\n",
    "\n",
    "# 简单的语言模型（这里仅为示例，实际应用中需要从大量数据中训练得到）\n",
    "language_model = {\n",
    "    (\"This\", \"is\"): 0.2,\n",
    "    (\"is\", \"a\"): 0.3,\n",
    "    (\"a\", \"relevant\"): 0.1\n",
    "}\n",
    "\n",
    "# 要移除的关键字\n",
    "keywords_to_remove = [\"<html>\", \"http://\\S+\"]\n",
    "\n",
    "# 基于语言的过滤\n",
    "filtered_texts_1 = remove_irrelevant_texts(texts, task_keywords)\n",
    "print(\"Filtered by language relevance:\")\n",
    "print(filtered_texts_1)\n",
    "\n",
    "# 基于度量的过滤\n",
    "filtered_texts_2 = filter_by_perplexity(texts, language_model)\n",
    "print(\"\\nFiltered by perplexity:\")\n",
    "print(filtered_texts_2)\n",
    "\n",
    "# 基于统计的过滤\n",
    "filtered_texts_3 = statistical_filtering(texts)\n",
    "print(\"\\nFiltered by word count:\")\n",
    "print(filtered_texts_3)\n",
    "\n",
    "# 基于关键字的过滤\n",
    "filtered_texts_4 = remove_keywords(texts, keywords_to_remove)\n",
    "print(\"\\nFiltered by keywords:\")\n",
    "print(filtered_texts_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71e300-16af-4983-9bc0-a8dc46207cf5",
   "metadata": {},
   "source": [
    "### （三）数据标注"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3902696-2f07-41e7-9036-ce5c3e6c610e",
   "metadata": {},
   "source": [
    "1. 标注任务类型\n",
    "- NLP领域\n",
    "\n",
    "**分类标注**：例如情感分类（积极、消极、中性）、主题分类（体育、科技、娱乐等）\n",
    "\n",
    "**实体标注**：标注文本中的人名、地名、组织机构名等实体\n",
    "\n",
    "**关系标注**：标注实体之间的关系，如父子关系、因果关系等\n",
    "\n",
    "- CV领域\n",
    "\n",
    "**目标检测标注**：在图像或视频中标记特定目标物体的边界框，如汽车、行人、动物等，明确目标位置与范围。\n",
    "![目标检测任务](images/目标检测.png)\n",
    "\n",
    "**语义分割标注**：将图像中每个像素点划分到特定类别，如道路、建筑物、植被等不同类别在图像上精确区分标注。\n",
    "![语义分割任务](images/语义分割.png)\n",
    "\n",
    "**关键点标注**：标注目标物体的关键特征点，例如在人体姿态识别中，标注人体关节点位置。\n",
    "![关键点检测任务](images/关键点检测.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dde031-6cec-4ddb-8f25-e37c8b985233",
   "metadata": {},
   "source": [
    "2. 标注方法\n",
    "\n",
    "- 人工标注\n",
    "  \n",
    "**NLP领域**：由专业人员标注，标注质量高，但成本大、效率低。\n",
    "\n",
    "**CV领域**：专业人员借助标注工具，细致标注图像或视频，能保证准确性，但耗费大量人力、时间成本，效率受限。例如人工手动绘制目标检测的边界框，或逐像素进行语义分割标注。\n",
    "\n",
    "- 半自动化标注\n",
    "  \n",
    "**NLP领域**：先利用预训练模型或规则初步标注，再由人工审核修正。\n",
    "\n",
    "**CV领域**：先通过基于深度学习的目标检测、分割等模型初步标注图像，如自动生成可能的边界框或分割区域，然后人工检查与修正标注结果，可提高标注效率并保证一定质量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d52db94-31ee-47f7-9fc2-c14a1425cadd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -riton (/home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: nltk in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -riton (/home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f9c5169-86ff-4c71-8174-edd1e56af3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Topic Annotations:\n",
      "Text: I love this amazing movie. It's very entertaining!, Label: 娱乐\n",
      "Text: The new iPhone has great features. It's a technological marvel., Label: 科技\n",
      "Text: I won the basketball game today. What a great sport!, Label: 体育\n",
      "Text: This book is so boring. I don't like it., Label: 其他\n",
      "Text: I love this amazing movie. It's very entertaining!\n",
      "Current Topic Label: 娱乐\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to correct the label? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Text: The new iPhone has great features. It's a technological marvel.\n",
      "Current Topic Label: 科技\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to correct the label? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Text: I won the basketball game today. What a great sport!\n",
      "Current Topic Label: 体育\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to correct the label? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Text: This book is so boring. I don't like it.\n",
      "Current Topic Label: 其他\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to correct the label? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Final Topic Annotations:\n",
      "Text: I love this amazing movie. It's very entertaining!, Label: 娱乐\n",
      "Text: The new iPhone has great features. It's a technological marvel., Label: 科技\n",
      "Text: I won the basketball game today. What a great sport!, Label: 体育\n",
      "Text: This book is so boring. I don't like it., Label: 其他\n",
      "\n",
      "Initial Sentiment Annotations:\n",
      "Text: I love this amazing movie. It's very entertaining!, Label: 积极\n",
      "Text: The new iPhone has great features. It's a technological marvel., Label: 积极\n",
      "Text: I won the basketball game today. What a great sport!, Label: 积极\n",
      "Text: This book is so boring. I don't like it., Label: 积极\n",
      "Text: I love this amazing movie. It's very entertaining!\n",
      "Current Sentiment Label: 积极\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to correct the label? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Text: The new iPhone has great features. It's a technological marvel.\n",
      "Current Sentiment Label: 积极\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to correct the label? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Text: I won the basketball game today. What a great sport!\n",
      "Current Sentiment Label: 积极\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to correct the label? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Text: This book is so boring. I don't like it.\n",
      "Current Sentiment Label: 积极\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to correct the label? (y/n):  y\n",
      "Enter the correct Sentiment label:  消极\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Final Sentiment Annotations:\n",
      "Text: I love this amazing movie. It's very entertaining!, Label: 积极\n",
      "Text: The new iPhone has great features. It's a technological marvel., Label: 积极\n",
      "Text: I won the basketball game today. What a great sport!, Label: 积极\n",
      "Text: This book is so boring. I don't like it., Label: 消极\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "def semi_automatic_topic_annotation(texts):\n",
    "    \"\"\"\n",
    "    主题分类的半自动化标注函数\n",
    "    使用简单的规则进行初步标注\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        if 'sports' in text.lower() or 'game' in text.lower():\n",
    "            results.append((text, '体育'))\n",
    "        elif 'technology' in text.lower() or 'tech' in text.lower():\n",
    "            results.append((text, '科技'))\n",
    "        elif 'entertainment' in text.lower() or 'movie' in text.lower() or 'music' in text.lower():\n",
    "            results.append((text, '娱乐'))\n",
    "        else:\n",
    "            results.append((text, '其他'))\n",
    "    return results\n",
    "\n",
    "def semi_automatic_sentiment_annotation(texts):\n",
    "    \"\"\"\n",
    "    情感分类的半自动化标注函数\n",
    "    使用简单的规则进行初步标注\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        if 'love' in text.lower() or 'like' in text.lower() or 'great' in text.lower() or 'amazing' in text.lower():\n",
    "            results.append((text, '积极'))\n",
    "        elif 'hate' in text.lower() or 'dislike' in text.lower() or 'boring' in text.lower() or 'terrible' in text.lower():\n",
    "            results.append((text, '消极'))\n",
    "        else:\n",
    "            results.append((text, '中性'))\n",
    "    return results\n",
    "\n",
    "def manual_review(results, annotation_type):\n",
    "    \"\"\"\n",
    "    人工审核和修正函数\n",
    "    对初步标注结果进行人工审核和修正\n",
    "    \"\"\"\n",
    "    reviewed_results = []\n",
    "    for i, (text, label) in enumerate(results):\n",
    "        print(f\"Text: {text}\\nCurrent {annotation_type} Label: {label}\\n\")\n",
    "        correction = input(\"Do you want to correct the label? (y/n): \")\n",
    "        if correction.lower() == 'y':\n",
    "            new_label = input(f\"Enter the correct {annotation_type} label: \")\n",
    "            reviewed_results.append((text, new_label))\n",
    "        else:\n",
    "            reviewed_results.append((text, label))\n",
    "        print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "    return reviewed_results\n",
    "\n",
    "# 示例数据\n",
    "texts = [\n",
    "    \"I love this amazing movie. It's very entertaining!\",\n",
    "    \"The new iPhone has great features. It's a technological marvel.\",\n",
    "    \"I won the basketball game today. What a great sport!\",\n",
    "    \"This book is so boring. I don't like it.\"\n",
    "]\n",
    "\n",
    "# 主题分类的半自动化标注\n",
    "topic_annotations = semi_automatic_topic_annotation(texts)\n",
    "print(\"Initial Topic Annotations:\")\n",
    "for text, label in topic_annotations:\n",
    "    print(f\"Text: {text}, Label: {label}\")\n",
    "final_topic_annotations = manual_review(topic_annotations, 'Topic')\n",
    "print(\"\\nFinal Topic Annotations:\")\n",
    "for text, label in final_topic_annotations:\n",
    "    print(f\"Text: {text}, Label: {label}\")\n",
    "\n",
    "# 情感分类的半自动化标注\n",
    "sentiment_annotations = semi_automatic_sentiment_annotation(texts)\n",
    "print(\"\\nInitial Sentiment Annotations:\")\n",
    "for text, label in sentiment_annotations:\n",
    "    print(f\"Text: {text}, Label: {label}\")\n",
    "final_sentiment_annotations = manual_review(sentiment_annotations, 'Sentiment')\n",
    "print(\"\\nFinal Sentiment Annotations:\")\n",
    "for text, label in final_sentiment_annotations:\n",
    "    print(f\"Text: {text}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc8896-9939-437f-9d3a-a9ceb9eb7d87",
   "metadata": {},
   "source": [
    "### （四）数据划分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec90b79-5a75-4e1b-91f3-57a40d144fa8",
   "metadata": {},
   "source": [
    "1、训练集、验证集、测试集划分\n",
    "- 比例：通常按照 80%、10%、10% 或其他合适比例划分。\n",
    "\n",
    "![数据划分](images/数据划分.png)\n",
    "![训练集](images/训练集.png)\n",
    "![验证集](images/验证集.png)\n",
    "![测试集](images/测试集.png)\n",
    "\n",
    "- 分层抽样：对于分类任务，保证各集合中类别分布与总体一致。\n",
    " \n",
    "2、数据泄露问题\n",
    "- 防止验证集或测试集中的数据特征被训练集包含，避免模型评估过于乐观。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa25aa8d-7a46-4407-ba72-1213a231e7b1",
   "metadata": {},
   "source": [
    "### （五）案例展示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b61c5-bf19-4262-b2d6-51322fb29152",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/shelterw/extra-dataset-process/notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e3337-67bd-420e-a088-4caa97413e51",
   "metadata": {},
   "source": [
    "## 三、数据处理工具与框架"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af748661-8813-4f67-b7cd-4bd166aa3b1a",
   "metadata": {},
   "source": [
    "### （一）常用工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a6e97a-95d9-473b-b8c4-f92187135709",
   "metadata": {},
   "source": [
    "- Python 数据处理库：如 Pandas、Numpy 等，可用于数据的读取、清洗、转换和分析。Pandas 可以方便地对数据进行表格化处理，如数据的筛选、合并、分组等操作；Numpy 则提供了高效的数值计算功能，在处理大规模数据时能够提高计算效率。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faf3d734-39f8-43f8-b34c-39783c83503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      Name  Age  Score\n",
      "0    Alice   25     85\n",
      "1      Bob   30     90\n",
      "2  Charlie   30     88\n",
      "3    David   40     92\n",
      "\n",
      "Filtered DataFrame (Age > 30):\n",
      "    Name  Age  Score\n",
      "3  David   40     92\n",
      "\n",
      "Grouped DataFrame (Average Score by Age):\n",
      "Age\n",
      "25    85.0\n",
      "30    89.0\n",
      "40    92.0\n",
      "Name: Score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 使用Pandas进行数据处理的示例\n",
    "def pandas_example():\n",
    "    # 创建一个简单的数据框\n",
    "    data = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "            'Age': [25, 30, 35, 40],\n",
    "            'Score': [85, 90, 88, 92]}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Original DataFrame:\")\n",
    "    print(df)\n",
    "    \n",
    "    # 筛选 Age 大于 30 的行\n",
    "    filtered_df = df[df['Age'] > 30]\n",
    "    print(\"\\nFiltered DataFrame (Age > 30):\")\n",
    "    print(filtered_df)\n",
    "    \n",
    "    # 按 Age 分组并计算平均 Score\n",
    "    grouped_df = df.groupby('Age')['Score'].mean()\n",
    "    print(\"\\nGrouped DataFrame (Average Score by Age):\")\n",
    "    print(grouped_df)\n",
    "\n",
    "# 调用函数\n",
    "pandas_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5db184-3d9c-4ae2-b6f8-3d0479517dfb",
   "metadata": {},
   "source": [
    "- 文本处理库：如 NLTK、SpaCy 等，用于文本的分词、词性标注、命名实体识别等任务。NLTK 提供了丰富的文本处理工具和语料库，便于进行文本的基础处理；SpaCy 则在处理效率和准确性上表现出色，能够快速准确地完成复杂的文本分析任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40cbcfa-e0c1-4755-8a22-d2ef9e0a1248",
   "metadata": {},
   "source": [
    "### （二）框架"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171be8e-fb67-4e20-9722-cf216874af5f",
   "metadata": {},
   "source": [
    "- Hugging Face 的 Datasets 库：方便数据加载、处理与共享。它提供了大量的公开数据集接口，并且可以方便地对数据集进行自定义处理和转换，同时支持数据的缓存和版本控制，便于团队协作和数据管理。\n",
    "\n",
    "![hugging face datasets](images/hunggingface_datasets.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc9c6a5-be19-4044-90d0-e7fc137b86b8",
   "metadata": {},
   "source": [
    "- TensorFlow Data Validation（TFDV）：用于数据的统计分析与验证，可帮助发现数据中的异常与偏差。它可以对数据的分布、特征相关性等进行深入分析，在数据预处理阶段及时发现数据质量问题，为模型训练提供可靠的数据保障。\n",
    "\n",
    "![TFDV](images/TFDV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7f444-9c0b-41e9-8e2f-120b71a37f40",
   "metadata": {},
   "source": [
    "## 四、常见的数据处理案例分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d4dfc-5161-49e0-af3c-c5cfbca20b70",
   "metadata": {},
   "source": [
    "- **电商智能客服模型**\n",
    "\n",
    "在电商领域，构建智能客服模型是提升用户购物体验、降低运营成本的关键举措。\n",
    "\n",
    "![智能客服模型](images/智能客服模型.png)\n",
    "\n",
    "- 初期，**数据收集**环节整合多元数据源，一方面从电商平台沉淀的海量售后对话记录、用户反馈表单等自有数据中精准提取信息；另一方面引入公开的客服对话数据集，扩充数据规模、丰富语义场景，确保涵盖商品咨询、物流追踪、售后投诉等全品类话题以及多样用户情绪表达。\n",
    "- 进入**数据清洗**阶段，利用正则表达式等技术剔除对话中的乱码、表情符号等无效噪声；借助分类器筛除敷衍、辱骂等低质对话，净化数据环境。\n",
    "**数据标注**过程严格分类用户需求，专业人员细致核查商品名、单号等关键实体标注，保障信息精准度。\n",
    "- 完成**数据划分**后，模型在训练集中 “沉浸式” 学习对话模式与业务知识；验证集实时监测模型表现，微调参数优化性能；部署上线后，持续收集新对话数据，经清洗、标注流程回流入模型，迭代优化，实现精准答疑，显著提升转化率与用户满意度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fb939-ccc9-43f3-a792-d434fd4be442",
   "metadata": {},
   "source": [
    "- **医疗影像诊断模型**\n",
    "\n",
    "医疗影像诊断模型关乎疾病早期筛查、精准诊疗，数据处理尤为重要。\n",
    "\n",
    "![医疗诊断模型](images/医疗诊断模型.png)\n",
    "\n",
    "- **数据收集**阶段，深度挖掘医院影像数据库资源，联合国际权威的公开医学影像集，广泛覆盖各类病症影像，从常见慢性病到罕见病一应俱全，保证样本多样性。\n",
    "- **数据清洗**流程需要筛除模糊不清、曝光过度等低质影像。凭借专业医师经验与标注工具，精准**标注**病灶类型、位置及严重程度等关键信息。\n",
    "- **划分数据**时，遵循分层抽样原则，确保各病症类别在训练、验证、测试集均衡分布。训练成型的模型在临床辅助诊断中，助力医生快速、精准识别病灶，缩短诊断时间，提升诊断准确率，为医疗资源合理配置、患者及时救治贡献关键力量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815fd81-31a7-4c8d-8d8c-d8fe6d70901d",
   "metadata": {},
   "source": [
    "## 五、拓展学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f090669-ee42-4b5e-8d4e-3f1d4e7d0fe2",
   "metadata": {},
   "source": [
    "### 数据增强\n",
    "1、应用场景与注意事项\n",
    "- 适用于数据量较小的情况，但要注意增强后数据的合理性与有效性，避免引入过多噪声。\n",
    "\n",
    "2、常用方法\n",
    "- 随机替换、插入、删除：在文本中随机替换单词、插入新单词或删除一些单词，生成新的文本数据。\n",
    "- 回译：将文本翻译成其他语言再翻译回原语言。\n",
    "\n",
    "![数据增强方法](images/数据增强方法.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a320fbb-8730-423a-8c1f-c18b6b95b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -riton (/home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: translate in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (3.6.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from translate) (8.1.7)\n",
      "Requirement already satisfied: lxml in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from translate) (5.2.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from translate) (2.31.0)\n",
      "Requirement already satisfied: libretranslatepy==2.1.1 in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from translate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from requests->translate) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from requests->translate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from requests->translate) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages (from requests->translate) (2023.5.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -riton (/home/ubuntu/anaconda3/envs/notebook/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce37b6-07f0-444c-a81d-c89a597eb436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def random_word_replacement(sentence, n=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    new_tokens = tokens.copy()\n",
    "    for _ in range(n):\n",
    "        random_index = random.randint(0, len(tokens) - 1)\n",
    "        word = tokens[random_index]\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = synonyms[random.randint(0, len(synonyms) - 1)]\n",
    "            new_tokens[random_index] = synonym\n",
    "    return ' '.join(new_tokens)\n",
    "\n",
    "def random_word_insertion(sentence, n=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for _ in range(n):\n",
    "        random_index = random.randint(0, len(tokens))\n",
    "        synonyms = []\n",
    "        random_word = tokens[random.randint(0, len(tokens) - 1)]\n",
    "        for syn in wordnet.synsets(random_word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = synonyms[random.randint(0, len(synonyms) - 1)]\n",
    "            tokens.insert(random_index, synonym)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def random_word_deletion(sentence, p=0.1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if random.uniform(0, 1) > p:\n",
    "            new_tokens.append(token)\n",
    "    return ' '.join(new_tokens)\n",
    "\n",
    "\n",
    "# 示例句子\n",
    "sentence = \"This is a sample sentence for data augmentation.\"\n",
    "\n",
    "print(\"Original sentence:\", sentence)\n",
    "print(\"\\nSentence after random word replacement:\", random_word_replacement(sentence))\n",
    "print(\"\\nSentence after random word insertion:\", random_word_insertion(sentence))\n",
    "print(\"\\nSentence after random word deletion:\", random_word_deletion(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f733b68-2f01-4c60-9a70-0cc395b72853",
   "metadata": {},
   "source": [
    "#### 分词\n",
    "\n",
    "**词表/词典** 传统的自然语言处理方法通常以单词为基本处理单元，模型都依赖预先确定的词表V，在编码输入词序列时，这些词表示模型只能处理词表中存在的词。<br>\n",
    "\n",
    "**未登录词/OOV** \n",
    "- 如果遇到不在词表中的未登录词，模型无法为其生成对应的表示，只能给予这些未登录词（Out-Of-Vocabulary，OOV）一个默认\n",
    "的表示UNK\n",
    "- UNK 的向量作为词表示矩阵的一部分一起参与训练，用来表示语料库中**所有**未登录词的向量表示。\n",
    "- 词表大小过小时，未登录词的比例较高，影响模型性能\n",
    "- 词表大小过大时，大量低频词出现\n",
    "在词表中，而这些词的词向量很难得到充分学习\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>词表示模型应能覆盖绝大部分的输入词，并避免词表过大所造成的数据稀疏问题。<b>\n",
    "</div。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b93dc-56aa-4090-b5f2-c60b725a61eb",
   "metadata": {},
   "source": [
    "分词是一种关键的数据预处理步骤，在这个过程中，原始文本被切分成单独的标记（tokens），以便输入到大型语言模型（LLMs）中。这个过程在自然语言处理（NLP）中扮演着非常重要的角色。它的主要任务是将文本数据（如句子或文档）转换为对计算机更易于处理的格式。\n",
    "\n",
    "虽然使用现有的分词器可能很方便，但使用针对预训练语料库量身定制的分词器可能更有优势，特别是对于包含多样化领域、语言和格式的语料库。\n",
    "\n",
    "为此，一部分LLMs使用SentencePiece开发了定制的分词器。这些分词器采用字节级的字节对编码（Byte Pair Encoding, BPE）算法，以确保在分词过程中不会丢失信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de9955c-4757-4121-9f3e-2debe9003c74",
   "metadata": {},
   "source": [
    "##### BPE分词法\n",
    "\n",
    "BPE（Byte Pair Encoding）分词法是一种常用于自然语言处理中的分词和子词处理技术。它的基本思想是从语料库中学习出现频率最高的字符序列，并将它们合并成一个更大的子词或标记。\n",
    "\n",
    "\n",
    "[BPE](https://aclanthology.org/P16-1162.pdf) 算法流程\n",
    "\n",
    "\n",
    "<img src=\"image/bpe.png\"  width=\"800\" height=\"600\" alt=\"rag-vs-finetuning-chn\"/>\n",
    "\n",
    "图片来源：[《大规模语言模型-从理论到实践》 复旦大学](https://intro-llm.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bb653-6bf9-4e56-a155-4d8bb61e228d",
   "metadata": {},
   "source": [
    "**训练过程：**\n",
    "- 首先，将语料库中的所有单词拆分成字符（或字符级别的子词）。\n",
    "- 计算字符对的频率，找到最频繁出现的字符对。\n",
    "- 合并最频繁的字符对，形成一个新的子词。\n",
    "- 重复上述步骤，直到达到预定的词汇大小或者不再有频繁的字符对可以合并。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940ee7c-e269-44f6-b85e-98c9f0763619",
   "metadata": {},
   "source": [
    "这里是一个分词器训练时的具体过程：\n",
    "\n",
    "**步骤1：准备语料库**\n",
    "\n",
    "假设我们有以下中文语料库：\n",
    "\n",
    "\"我喜欢吃巧克力\"\n",
    "\n",
    "\"蛋糕很好吃\"\n",
    "\n",
    "\"喜欢吃甜食的人很多\"\n",
    "\n",
    "\"我喜欢吃好吃的蛋糕\"\n",
    "\n",
    "**通过特定分隔符标注开头和结尾（此处通过#标注）**\n",
    "\n",
    "\"#我喜欢吃巧克力#\"\n",
    "\n",
    "\"#蛋糕很好吃#\"\n",
    "\n",
    "\"#喜欢吃甜食的人很多#\"\n",
    "\n",
    "\"#我喜欢吃好吃的蛋糕#\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c958a9-a58a-483a-b785-afdf7973f050",
   "metadata": {},
   "source": [
    "**步骤2：字符级别分词**\n",
    "\n",
    "将每个句子分割成字符级别的子词：\n",
    "\n",
    "句子1: \"我 喜 欢 吃 巧 克 力 \\</w>\"\n",
    "\n",
    "句子2: \"蛋 糕 很 好 吃 \\</w>\"\n",
    "\n",
    "句子3: \"喜 欢 吃 甜 食 的 人 很 多 \\</w>\"\n",
    "\n",
    "句子4: \"我 喜 欢 吃 好 吃 的 蛋 糕 \\</w>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a48480-8f56-46ab-84b9-c7c69ec625ad",
   "metadata": {},
   "source": [
    "**步骤3：初始化词汇表**\n",
    "\n",
    "初始词汇表包含语料库中的所有字符：\n",
    "\n",
    "词汇表：{'我', '喜', '欢', '吃', '巧', '克', '力', '蛋', '糕', '很', '好', '甜', '食', '的', '人', '多', '#'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a3f74-dbc2-4d18-9b68-192c1fbcfad5",
   "metadata": {},
   "source": [
    "**步骤4：训练BPE模型**\n",
    "\n",
    "重复以下步骤，直到停止条件满足：\n",
    "\n",
    "a. 统计字符对的频率，找到最频繁的字符对：\n",
    "\n",
    "统计频率：{'喜欢': 3, '好吃': 2, '#我': 2}\n",
    "\n",
    "最频繁的字符对：'喜欢'，频率为3\n",
    "\n",
    "b. 合并最频繁的字符对，生成新的子词：\n",
    "\n",
    "合并：'喜 欢' -> '喜欢'\n",
    "\n",
    "更新词汇表：{'我', '喜欢', '吃', '巧', '克', '力', '很', '好', '的', '人', '多', '蛋', '糕', '甜', '食', '#'}\n",
    "\n",
    "c. 更新语料库中的文本：\n",
    "\n",
    "更新后的句子1: \"我 喜欢 吃 巧 克 力 \\</w>\"\n",
    "\n",
    "更新后的句子2: \"蛋 糕 很 好 吃 \\</w>\"\n",
    "\n",
    "更新后的句子3: \"喜欢 吃 甜 食 的 人 很 多 \\</w>\"\n",
    "\n",
    "更新后的句子4: \"我 喜欢 吃 好 吃 的 蛋 糕 \\</w>\"\n",
    "\n",
    "d. 重复步骤a-c，直到满足停止条件（例如，合并次数达到预定次数或词汇表大小达到预定大小）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405509cf-b970-4f9e-a345-76558c58f635",
   "metadata": {},
   "source": [
    "**步骤5：生成子词词汇表**\n",
    "\n",
    "最终词汇表包含字符和更大的子词：\n",
    "\n",
    "词汇表：{'</w>', '人', '克', '力', '吃', '喜', '喜欢', '喜欢吃', '多', '好', '好吃', '巧', '很', '我', '我喜欢吃', '欢', '甜', '的', '糕', '蛋', '蛋糕', '食'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95b6da-e3de-43e4-88e3-db75f89f0a9a",
   "metadata": {},
   "source": [
    "**步骤6：应用BPE模型**\n",
    "\n",
    "使用训练好的BPE模型对新的文本进行分词，将文本拆分成词汇表中存在的子词。\n",
    "\n",
    "示例输入文本：\"我喜欢吃甜食很好吃\"\n",
    "\n",
    "分词结果：\"我喜欢吃 甜 食 很 好吃\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88955e5f-5ad8-4fe0-81cf-813e6a451d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</w>', '人', '克', '力', '吃', '喜', '多', '好', '巧', '很', '我', '欢', '甜', '的', '糕', '蛋', '食']\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "vocab = [\n",
    "    '我 喜 欢 吃 巧 克 力 </w>', \n",
    "    '蛋 糕 很 好 吃 </w>',\n",
    "    '喜 欢 吃 甜 食 的 人 很 多 </w>',\n",
    "    '我 喜 欢 吃 好 吃 的 蛋 糕 </w>'\n",
    "]\n",
    "vocab_set = set(word for v in vocab for word in v.split())\n",
    "print(sorted(list(vocab_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a9a71f-aa49-4208-b4fd-a139abe9d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Merged ('我', '喜') into 我喜\n",
      "Updated Vocabulary: ['吃', '喜', '好', '很', '我', '我喜', '欢', '甜', '食', '食,甜']\n",
      "Sentences: ['我喜 欢 吃 甜 食,甜 食 很 好 吃']\n",
      "\n",
      "Step 2: Merged ('我喜', '欢') into 我喜欢\n",
      "Updated Vocabulary: ['吃', '喜', '好', '很', '我', '我喜', '我喜欢', '欢', '甜', '食', '食,甜']\n",
      "Sentences: ['我喜欢 吃 甜 食,甜 食 很 好 吃']\n",
      "\n",
      "Step 3: Merged ('我喜欢', '吃') into 我喜欢吃\n",
      "Updated Vocabulary: ['吃', '喜', '好', '很', '我', '我喜', '我喜欢', '我喜欢吃', '欢', '甜', '食', '食,甜']\n",
      "Sentences: ['我喜欢吃 甜 食,甜 食 很 好 吃']\n",
      "\n",
      "Step 4: Merged ('我喜欢吃', '甜') into 我喜欢吃甜\n",
      "Updated Vocabulary: ['吃', '喜', '好', '很', '我', '我喜', '我喜欢', '我喜欢吃', '我喜欢吃甜', '欢', '甜', '食', '食,甜']\n",
      "Sentences: ['我喜欢吃甜 食,甜 食 很 好 吃']\n",
      "\n",
      "Step 5: Merged ('我喜欢吃甜', '食,甜') into 我喜欢吃甜食,甜\n",
      "Updated Vocabulary: ['吃', '喜', '好', '很', '我', '我喜', '我喜欢', '我喜欢吃', '我喜欢吃甜', '我喜欢吃甜食,甜', '欢', '甜', '食', '食,甜']\n",
      "Sentences: ['我喜欢吃甜食,甜 食 很 好 吃']\n",
      "\n",
      "['吃', '喜', '好', '很', '我', '我喜', '我喜欢', '我喜欢吃', '我喜欢吃甜', '我喜欢吃甜食,甜', '欢', '甜', '食', '食,甜']\n",
      "['我喜欢吃甜食,甜 食 很 好 吃']\n"
     ]
    }
   ],
   "source": [
    "def bpe_steps_update_vocab(sentences, num_merges):\n",
    "    # 初始化词汇表，包含所有句子中的单词\n",
    "    vocab = set(word for sentence in sentences for word in sentence.split())\n",
    "    for i in range(num_merges):\n",
    "        pair_stats = collections.defaultdict(int)  # 用于统计每对相邻单词出现的次数\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            for j in range(len(words)-1):\n",
    "                pair_stats[words[j], words[j+1]] += 1  # 对每对相邻单词进行计数\n",
    "\n",
    "        if not pair_stats:\n",
    "            break  # 如果没有可合并的单词对，则退出循环\n",
    "\n",
    "        best_pair = max(pair_stats, key=pair_stats.get)  # 找到出现次数最多的单词对\n",
    "        new_word = ''.join(best_pair)  # 合并这对单词\n",
    "        vocab.add(new_word)  # 将新词添加到词汇表中\n",
    "        # 在句子中替换所有出现的最佳单词对为新词\n",
    "        sentences = [re.sub(r'(?<!\\S)' + re.escape(' '.join(best_pair)) + r'(?!\\S)', new_word, sentence) for sentence in sentences]\n",
    "\n",
    "        # 打印每一步的合并信息、更新后的词汇表和句子\n",
    "        print(f\"Step {i+1}: Merged {best_pair} into {new_word}\")\n",
    "        print(\"Updated Vocabulary:\", sorted(list(vocab)))\n",
    "        print(\"Sentences:\", sentences)\n",
    "        print()\n",
    "\n",
    "    # 返回排序后的词汇表和更新后的句子\n",
    "    return sorted(list(vocab)), sentences\n",
    "sentences=[\"我 喜 欢 吃 甜 食,甜 食 很 好 吃\"]\n",
    "# 用初始句子列表开始BPE过程，并更新词汇表\n",
    "updated_vocab, updated_sentences = bpe_steps_update_vocab(sentences, 5)\n",
    "print(updated_vocab)\n",
    "print(updated_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9609558-b54a-4ca2-ad8f-090898e970c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我喜欢吃 苹 果 , 梨 子 也 很 好 吃'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_sentence_with_bpe(sentence, vocab):\n",
    "    # 将句子拆分为单个字符\n",
    "    symbols = list(sentence.replace(\" \", \"\"))\n",
    "    encoded_sentence = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(symbols):\n",
    "        matched = False\n",
    "        # 检查从当前位置开始的最长的可能匹配\n",
    "        for j in range(len(symbols), i, -1):\n",
    "            candidate = ''.join(symbols[i:j])\n",
    "            if candidate in vocab:\n",
    "                encoded_sentence.append(candidate)\n",
    "                i = j\n",
    "                matched = True\n",
    "                break\n",
    "        # 如果没有匹配到任何字词，就简单地使用单个字符\n",
    "        if not matched:\n",
    "            encoded_sentence.append(symbols[i])\n",
    "            i += 1\n",
    "\n",
    "    return encoded_sentence\n",
    "\n",
    "# 新句子\n",
    "# new_sentence = \"我喜欢吃蛋糕和巧克力\"\n",
    "new_sentence = \"我喜欢吃甜食,甜食很好吃\"\n",
    "new_sentence = \"我喜欢吃苹果,梨子也很好吃\"\n",
    "\n",
    "# 使用更新后的词汇表对新句子进行BPE编码\n",
    "encoded_new_sentence = encode_sentence_with_bpe(new_sentence, updated_vocab)\n",
    "# 将编码后的字词序列组合回句子\n",
    "reconstructed_sentence = ' '.join(encoded_new_sentence)\n",
    "reconstructed_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38a157-5be3-4919-a559-52414f618835",
   "metadata": {},
   "source": [
    "下面是我们通过chinese-alpaca-2-7b的专属分词器进行分词任务的示例：\n",
    "\n",
    "```\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"ziqingyang/chinese-alpaca-2-7b\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "text = \"三原色原理是指人眼对红、绿、蓝最为敏感，大多数的颜色可以通过红、绿、蓝三色按照不同的比例合成产生。\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c3ca4b-42fb-4bb1-94aa-35d1c41f5863",
   "metadata": {},
   "source": [
    "['▁', '三', '原', '色', '原理', '是指', '人', '眼', '对', '红', '、', '绿', '、', '蓝', '最为', '敏感', '，', '大多数', '的颜色', '可以通过', '红', '、', '绿', '、', '蓝', '三', '色', '按照', '不同的', '比例', '合成', '产生', '。']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7d01e-898e-4aeb-971c-02150c70430f",
   "metadata": {},
   "source": [
    "## 课程小结\n",
    "\n",
    "一、数据集类型和重要性\n",
    "\n",
    "二、数据处理常见流程\n",
    "- 数据收集（公开数据集、自有数据等）\n",
    "- 数据清洗（噪声处理、低质量过滤）\n",
    "- 数据标注（标注类型、标注方法）\n",
    "- 数据划分（比例、分层抽样、数据泄露）\n",
    "- 案例展示\n",
    "\n",
    "三、数据处理工具与框架\n",
    "- 常用工具（Pandas、Numpy等）\n",
    "- 框架（Hugging Face、TFDV等）\n",
    "\n",
    "四、常见的数据处理案例分析\n",
    "- 电商智能客服模型\n",
    "- 医疗影像诊断模型\n",
    "\n",
    "五、扩展学习\n",
    "- 数据增强（应用场景、常用方法）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
