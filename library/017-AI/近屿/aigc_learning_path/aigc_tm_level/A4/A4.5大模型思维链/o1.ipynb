{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. o1介绍\n",
    "\n",
    "#### 1.1 OpenAI o1\n",
    "\n",
    "OpenAI在2024年推出了o1模型，这款模型以其卓越的推理能力引起了广泛关注。它在多个平台上的表现，如AIME（人工智能数学竞赛）和CodeForces（编程竞赛），超越了其他领先的模型。o1模型通过强化学习技术来增强其推理能力，使其能够应对复杂、现实世界的挑战。特别地，o1模型采用了Chain-of-Thought (CoT) 精调方法，这种方法允许模型在生成答案前进行内部思考，从而提高回答的质量和准确性。此外，o1还利用了Monte Carlo Tree Search (MCTS) 和新颖的推理策略，以进一步优化其解决复杂问题的能力。这些特性使得o1不仅能够在标准答案明确的学科中表现出色，如数学、物理和编程，而且在没有明确标准或奖励难以量化的更广泛领域中也展现出了潜力。\n",
    "\n",
    "#### 1.2 Marco o1\n",
    "\n",
    "Marco-o1是由阿里巴巴国际数字商务团队开发的一个项目，灵感来源于OpenAI的o1模型，并在其基础上进行了扩展和改进。Marco-o1旨在探索大型推理模型（LRM）的技术路线图，特别是针对开放性问题解决方案。与传统的专注于有标准答案领域的模型不同，Marco-o1更加关注那些缺乏清晰标准和量化奖励机制的领域。\n",
    "\n",
    "![marco o1](./image_o1/logo.png \"marco o1\")\n",
    "\n",
    "Marco-o1的核心在于结合了几种先进的技术：CoT精调、MCTS以及自我反思机制。通过使用这些技术，Marco-o1可以有效地处理复杂的现实世界问题。例如，在MGSM数据集上，Marco-o1实现了显著的准确率提升，MGSM(English) 数据集上提升了6.17%，而在MGSM(Chinese) 数据集上则提升了5.60%。此外，Marco-o1还在翻译任务中展示了出色的表现，特别是在处理口语化表达方面。\n",
    "\n",
    "Marco-o1不仅仅是一个单一的努力成果，而是一个持续优化的过程。尽管目前的版本主要展现了o1类似的推理特征，但其性能尚未完全达到理想的o1水平。然而，随着不断的改进和优化，Marco-o1有望在未来取得更大的突破，尤其是在多语言应用领域展现出更多的可能性。项目主页提供了更多详细信息和技术文档，可供进一步阅读：[https://github.com/AIDC-AI/Marco-o1](https://github.com/AIDC-AI/Marco-o1)。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 思维链介绍\n",
    "\n",
    "#### 什么是思维链（Chain-of-Thought, CoT）\n",
    "\n",
    "**思维链**（Chain-of-Thought, CoT）是一种推理方法，旨在模仿人类在解决复杂问题时的思考过程。具体来说，CoT通过逐步构建一系列逻辑步骤来解释如何从输入信息到达最终答案。这种方法不仅提高了模型解决问题的能力，还增强了模型输出结果的可解释性。在自然语言处理（NLP）领域，CoT通常通过提供中间步骤或解释性的文本提示来引导大型语言模型（LLMs）进行更深层次的推理。\n",
    "\n",
    "在实践中，CoT可以通过以下方式实现：\n",
    "- **数据精调**：使用包含详细推理步骤的数据集对模型进行训练。这些步骤可以是人工标注的，也可以是通过合成生成的。\n",
    "- **提示工程**：在模型输入中加入特定的提示词或句子，鼓励模型按照CoT的方式进行思考和回答问题。\n",
    "\n",
    "#### 对大模型推理的好处\n",
    "\n",
    "1. **提高准确性**：\n",
    "   - **结构化推理路径**：CoT帮助模型形成清晰、结构化的推理路径，从而减少了错误的发生率。例如，在数学问题中，CoT可以使模型一步步地解题，而不是直接跳跃到答案。\n",
    "   - **增强理解力**：通过逐步解析问题，模型能够更好地理解复杂的语义关系，这对于涉及多步推理的任务尤为重要。\n",
    "\n",
    "2. **提升透明度与可解释性**：\n",
    "   - **解释性输出**：CoT使得模型的输出不仅仅是简单的答案，还包括了得出该答案的过程。这种解释性输出对于需要验证模型决策过程的应用场景非常有用，比如医疗诊断或金融风险评估。\n",
    "   - **用户信任**：当模型能够展示其推理过程时，用户更容易理解和信任模型的结果，这有助于提高系统的接受度。\n",
    "\n",
    "3. **促进多任务学习**：\n",
    "   - **跨域应用**：CoT不仅可以应用于数学和编程等具有明确标准答案的领域，还可以扩展到开放性问题解决方案，如翻译任务中的口语表达理解和生成。\n",
    "   - **适应性增强**：通过引入CoT机制，模型可以在面对新类型的问题时更快地调整并找到有效的解决策略。\n",
    "\n",
    "4. **优化推理效率**：\n",
    "   - **减少计算资源消耗**：虽然CoT增加了推理步骤的数量，但通过合理设计推理路径，可以避免不必要的计算，从而节省资源。\n",
    "   - **动态调整搜索空间**：结合MCTS等技术，CoT可以帮助模型在探索可能的解决方案时更加高效地分配计算资源，优先考虑高置信度的路径。\n",
    "\n",
    "5. **支持自我反思与修正**：\n",
    "   - **内置反馈机制**：CoT为模型提供了自我检查的机会，使其能够在发现潜在错误后重新审视并修正自己的推理路径。例如，“Wait! Maybe I made some mistakes!”这样的提示可以促使模型重新评估之前的结论。\n",
    "   - **持续改进能力**：基于CoT的自我反思机制允许模型在不断学习和应用中逐步完善自身的推理能力，从而在长期运行中表现出更高的稳定性和可靠性。\n",
    "\n",
    "综上所述，思维链（CoT）作为一种先进的推理方法，极大地提升了大型语言模型在处理复杂问题时的表现。它不仅提高了模型的准确性和透明度，还促进了多任务学习和自我反思能力的发展，为未来的人工智能研究和应用开辟了新的方向。特别是像Marco-o1这样的模型，通过集成CoT技术，已经在多个基准测试中展示了显著的优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 强化学习介绍\n",
    "\n",
    "强化学习（Reinforcement Learning, RL）是一种机器学习方法，通过代理（agent）与环境的交互来学习如何做出一系列决策以最大化某种长期奖励。强化学习在许多领域中都取得了显著的成功，其中最著名的例子是AlphaGo，它利用强化学习击败了围棋世界冠军。\n",
    "\n",
    "#### 3.1 MCTS 蒙特卡洛树搜索\n",
    "\n",
    "**一般的蒙特卡洛树搜索**\n",
    "\n",
    "蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）是一种用于决策制定的启发式搜索算法，特别适用于那些状态空间巨大且难以完全遍历的问题。MCTS通过模拟多个可能的行动路径来评估不同策略的好坏，并选择最佳路径进行下一步探索。其基本步骤包括：\n",
    "\n",
    "- **选择（Selection）**：从根节点开始，沿着树向下选择一个子节点，直到到达一个未完全展开的节点。\n",
    "- **扩展（Expansion）**：在该未完全展开的节点上添加一个新的子节点。\n",
    "- **模拟（Simulation）**：从新添加的子节点出发，随机执行动作直到游戏结束或达到某个终止条件，以估计该节点的价值。\n",
    "- **反向传播（Backpropagation）**：将模拟结果（如获胜概率或奖励值）反向传播回树中的父节点，更新这些节点的价值估计。\n",
    "\n",
    "这种迭代过程允许MCTS逐步聚焦于最有希望的策略，并随着更多的模拟而改进其决策质量。\n",
    "\n",
    "![蒙特卡洛树实例](./image_o1/tree_visualization.png \"蒙特卡洛树实例\")\n",
    "\n",
    "\n",
    "**实例：AlphaGo下的围棋**\n",
    "\n",
    "为了更好地理解MCTS的工作原理，我们可以看看它在AlphaGo中的应用。AlphaGo利用MCTS来评估棋盘上的每一个可能的走法及其后续影响。具体步骤如下：\n",
    "\n",
    "- **选择（Selection）**：AlphaGo从当前局面开始，根据已有的知识和统计信息，选择最有可能带来胜利的走法。这个过程会持续到遇到一个尚未充分探索的局面。\n",
    "  \n",
    "- **扩展（Expansion）**：一旦找到这样的局面，AlphaGo会在该位置上生成所有合法的下一步走法，并选择其中一个进行进一步分析。\n",
    "\n",
    "- **模拟（Simulation）**：接下来，AlphaGo会基于当前的选择进行快速的游戏模拟，通常采用一种简化的策略来进行快速评估，直到游戏结束或达到预设的深度限制。\n",
    "\n",
    "- **反向传播（Backpropagation）**：根据模拟的结果（赢、输或平局），AlphaGo会更新这条路径上所有相关节点的统计数据。这有助于指导未来的决策，使得更有效的策略得到优先考虑。\n",
    "\n",
    "通过这种方式，AlphaGo能够高效地探索庞大的搜索空间，并在有限时间内找到最优解。\n",
    "\n",
    "\n",
    "**Marco-o1中的MCTS应用**\n",
    "\n",
    "Marco-o1对传统的蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）进行了适应性修改，以便更好地与大型语言模型（LLMs）集成并支持复杂的推理任务。以下是几个关键点：\n",
    "\n",
    "- **节点作为推理状态**：在MCTS框架中，每个节点代表问题解决过程中的一个推理状态。对于Marco-o1来说，这意味着每个节点对应于推理链中的一个特定步骤或中间结论。例如，在处理数学问题时，每个节点可以表示一个具体的逻辑步骤或计算结果。这种结构化的方式帮助模型逐步构建复杂的推理路径。\n",
    "\n",
    "- **动作作为LLM输出**：从某个节点出发的可能动作即为大型语言模型生成的输出。这些输出表示推理链中的潜在步骤或子步骤。具体而言，LLM生成的每一个token都可以被视为一个可能的动作。例如，在处理数学问题时，LLM可能会生成一系列逻辑步骤，每一步都是一个可能的动作。通过这种方式，MCTS能够利用LLM的强大生成能力来探索多种可能的推理路径。\n",
    "\n",
    "- **展开和奖励计算**：在展开阶段，LLM会继续推理直到达到终止状态，并根据所采取的行动计算出相应的奖励得分R。为了评估每个token的置信度分数，我们使用softmax函数对其对数概率及其前五个替代token的对数概率进行处理：\n",
    "  \n",
    "$ c_i = \\frac{\\exp(p(t_i))}{\\sum_{k=1}^{5}\\exp(p(t_k))} $\n",
    "\n",
    "其中$c_i$ 是第i个token的置信度分数，$p(t_i)$ 是由LLM生成的第i个token的对数概率，而$p(t_k)$ (k=1到5) 则是第i步预测的前五个token的对数概率。最终，整个推演路径的整体奖励评分v是所有token置信度分数的平均值：\n",
    "\n",
    "$ v = \\frac{1}{n} \\sum_{i=1}^{n} c_i $\n",
    "\n",
    "这使得MCTS能够有效地扩展解决方案空间，允许模型基于计算出的置信度分数探索大量推理路径，并选择最有可能成功的路径。通过这种方式，MCTS与LLM紧密结合，充分利用LLM的生成能力来优化推理路径的选择。\n",
    "\n",
    "- **指导MCTS**：该奖励得分用于评估并选择有前景的路径，从而引导搜索朝向更自信、更可靠的推理链发展。例如，在MGSM数据集上的实验表明，采用不同粒度的动作策略（如“步骤作为行动”、“mini-step作为行动”）可以显著提高模型的性能。特别是，“mini-step（32 tokens）作为行动”的策略在中文数据集上取得了最高的准确率82.40%。此外，为了进一步增强模型的表现，Marco-o1引入了一个自我反思机制，鼓励模型重新审视自己的推理步骤。通过添加提示词“Wait! Maybe I made some mistakes! I need to rethink from scratch.”，模型能够在发现潜在错误后重新评估其推理路径，从而提高复杂问题解决的能力。\n",
    "\n",
    "- **多样化行动策略**：为了优化推理效率，Marco-o1还探索了不同的行动粒度策略。例如，初始阶段使用“步骤作为行动”，随后尝试将这些步骤细分为更小的单位（如32或64 tokens），称为“mini-step”。这种细粒度的探索方式使得模型能够更加细致地评估各种推理路径，提高了找到正确答案的概率。实验结果显示，这种策略在减少单独猜测次数时表现出色，尤其是在处理复杂问题时。\n",
    "\n",
    "通过以上方法，Marco-o1不仅能够在标准答案明确的学科中表现出色，而且在没有明确标准或奖励难以量化的领域也展现出了潜力。MCTS与LLM的结合，使得模型能够高效地探索解决方案空间，并在实际应用中取得显著的性能提升。\n",
    "\n",
    "\n",
    "#### 3.2 奖励函数\n",
    "\n",
    "奖励函数定义了当采取某一行动后，环境给予代理的反馈（正面或负面）。在MCTS中，奖励评分R被用来评估和选择有潜力的路径。奖励函数的设计直接影响模型的学习效果和性能。例如，在Marco-o1中，奖励是基于每个token的置信度分数计算得出的整体奖励评分。这种机制确保了模型倾向于选择那些具有较高置信度的推理路径，进而提高了找到正确答案的概率。\n",
    "\n",
    "#### 3.3 价值函数\n",
    "\n",
    "价值函数估计从当前状态开始遵循某一策略所能获得的预期回报。在MCTS中，这通常通过计算所有tokens的置信度分数的平均值得出整体奖励评分v。这个平均值充当了一种奖励信号，用于评价推理路径的质量。较高的v值意味着更自信且可能准确的推理路径。通过这种方式，价值函数帮助指导搜索过程，使其更加集中于高价值的推理路径上。\n",
    "\n",
    "此外，为了进一步提升模型的表现，Marco-o1还引入了不同的行动粒度策略（如“步骤作为行动”、“mini-step作为行动”），并通过自我反思机制鼓励模型重新审视自己的推理步骤，从而提高复杂问题解决能力。实验表明，这些策略在MGSM数据集上的表现优于基线模型，并展示了MCTS在减少单独猜测次数时的优势。体的阐述。希望这对您有所帮助。如果有任何进一步的需求，请随时告知。\n",
    "\n",
    "![数据构建](./image_o1/intro.jpg \"数据构建\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Marco-o1训练流程\n",
    "\n",
    "#### 4.1 训练数据准备\n",
    "\n",
    "为了增强Marco-o1模型的推理能力，marco o1采用了监督式精调（Supervised Fine-Tuning, SFT）策略，并使用了多种数据集。具体包括：\n",
    "\n",
    "- **Open-O1 CoT 数据集（过滤版）**：通过对原始Open-O1项目的CoT数据集进行启发式和质量过滤处理，确保数据集的质量。这种处理方式使得模型能够有效地采用结构化的推理模式。\n",
    "  \n",
    "- **Marco-o1 CoT 数据集（合成版）**：利用MCTS生成复杂推理路径，从而增强模型的推理能力。该数据集有助于模型学习如何构建复杂的推理链路。\n",
    "\n",
    "- **Marco Instruction 数据集**：考虑到指令跟随能力在执行复杂任务中的关键作用，marco o1整合了一套指令跟随数据。这保证了模型在广泛的任务中保持有效性和通用性，同时显著提升了其推理技巧。\n",
    "\n",
    "这些数据集总共包含60,266个样本，分别为：\n",
    "- Open-O1 CoT 数据集（过滤版）：45,125个样本\n",
    "- Marco-o1 CoT 数据集（合成版）：10,000个样本\n",
    "- Marco Instruction 数据集：5,141个样本\n",
    "\n",
    "#### 4.2 树状思考路径构建\n",
    "\n",
    "为了扩展解决方案空间并提升Marco-o1模型的推理能力，marco o1将大型语言模型（LLMs）与蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）相结合：\n",
    "\n",
    "- **节点作为推理状态**：在MCTS框架中，每个节点代表问题解决过程中的一个推理状态。\n",
    "  \n",
    "- **动作作为LLM输出**：从某个节点出发的可能动作即为LLM生成的输出。这些输出表示推理链中的潜在步骤或子步骤。\n",
    "\n",
    "- **展开和奖励计算**：在展开阶段，LLM会继续推理直到达到终止状态，并根据所采取的行动计算出相应的奖励得分R。这个奖励评分用于评估并选择有前景的路径，从而引导搜索朝向更自信、更可靠的推理链发展。\n",
    "\n",
    "此外，通过不同粒度的动作选择策略（如“步骤作为行动”、“mini-step作为行动”），进一步增强了模型探索复杂推理路径的能力。\n",
    "\n",
    "#### 4.3 反馈循环与权重更新\n",
    "\n",
    "为了促进模型自我反思并提高解决问题的能力，marco o1在推理过程中引入了一个反馈循环机制：\n",
    "\n",
    "- **自我反思机制**：通过添加提示词“Wait! Maybe I made some mistakes! I need to rethink from scratch.”来促使模型自我反思和重新评估其推理步骤。这种方法允许模型识别并纠正潜在错误，提高了对困难问题的解答准确性。\n",
    "  \n",
    "- **权重更新**：基于奖励信号和反馈信息，模型可以调整其内部参数以优化性能。例如，在实验中发现，通过增加自我反思环节，约有一半原本无法正确解答的问题得到了正确解决。\n",
    "\n",
    "#### 4.4 模型性能评估与改进\n",
    "\n",
    "为了评估Marco-o1模型的性能，marco o1在MGSM（Mathematical Generalization and Solution Modeling）数据集上进行了测试，包括英文和中文版本的数据集。主要结果如下：\n",
    "\n",
    "- **实验设置**：基于Qwen2-7B-Instruct模型，marco o1使用自定义的训练数据进行SFT，创建了Marco-o1-CoT。然后，在MCTS框架下应用了不同的动作粒度策略，包括“步骤作为行动”、“mini-step（64 tokens）作为行动”和“mini-step（32 tokens）作为行动”。\n",
    "\n",
    "- **实验结果**：Marco-o1-MCTS（step）在MGSM-en数据集上的准确率达到了90.40%，而在MGSM-zh数据集上达到了80.00%。相比之下，“mini-step（32 tokens）作为行动”的策略在MGSM-zh数据集上取得了最高的准确率82.40%。此外，marco o1还使用Test@N指标评估了模型在有限猜测次数下的表现，结果显示MCTS在较少猜测次数的情况下表现出色。\n",
    "\n",
    "未来的工作将集中在进一步优化奖励模型（Outcome Reward Modeling, ORM 和 Process Reward Modeling, PRM），减少随机性，并通过强化学习技术进一步提升决策制定过程，最终增强Marco-o1解决复杂现实世界任务的能力。 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 实例分析\n",
    "\n",
    "为了更好地理解Marco-o1模型在实际应用中的表现，marco o1可以通过几个具体的实例来展示其推理能力、解决复杂问题的能力以及在多语言环境下的应用效果。\n",
    "\n",
    "#### 实例一：经典问题的推理\n",
    "\n",
    "**问题**: \"How many ‘r’s are in ‘strawberry’?\"\n",
    "\n",
    "**Marco-o1的推理过程**:\n",
    "- Marco-o1首先通过Chain-of-Thought (CoT) 方法进行思考，逐步解析问题。\n",
    "- 在MCTS框架下，模型探索了多个可能的推理路径。例如，它会考虑单词“strawberry”中每个字母的位置和出现次数。\n",
    "- 最终，模型给出了正确的答案：“strawberry”中有两个‘r’。\n",
    "\n",
    "值得注意的是，在这个过程中，虽然模型正确地计算出了答案，但它并未明确提及最后一个字母‘y’。这表明模型有时会跳过显而易见的步骤，类似于人类在解决问题时的行为。尽管如此，这种略过并不会影响最终答案的准确性。\n",
    "\n",
    "\n",
    "\n",
    "![经典问题](./image_o1/strawberry.jpg \"草莓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例二：翻译任务中的表现\n",
    "\n",
    "为了展示Marco-o1在处理复杂翻译任务时的表现，特别是对于口语化表达的理解与生成，marco o1进行了一系列案例研究，并将其与Google Translate进行了对比。\n",
    "\n",
    "**示例**：\n",
    "- **中文输入**: “这鞋穿上有一种踩屎感。”\n",
    "- **Marco-o1输出**: “This shoe has a comfortable sole.”\n",
    "- **Google Translate输出**: “Wearing this shoe feels like stepping on shit.”\n",
    "\n",
    "在这个例子中，Marco-o1不仅正确理解了原句的意思，还成功地将俚语转换为更加自然、符合目标语言习惯的表达方式。相比之下，Google Translate则直接翻译了原文，未能捕捉到其中的细微差别。\n",
    "\n",
    "另一个例子展示了Marco-o1在处理长句和复杂表达时的能力：\n",
    "\n",
    "**中文输入**: “太漂亮了，简直让人着迷，上半部分有着明显的韩式风格，柔软蓬松的材质非常厚实，配上一层底，既独特又适合日常穿着。”\n",
    "\n",
    "**Marco-o1输出**: “It’s so beautiful that it’s captivating, the upper part has a distinctly Korean style, the soft and fluffy material is perfectly thick, and it’s complemented by a base layer, creating a unique and everyday-wear outfit.”\n",
    "\n",
    "通过这些例子可以看出，Marco-o1在处理复杂句子结构和非正式语言方面表现出色，能够提供比传统翻译工具更准确、更自然的翻译结果。\n",
    "\n",
    "![翻译](./image_o1/translation.jpg \"翻译\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### o1 结论\n",
    "\n",
    "通过上述实例分析，marco o1可以得出以下结论：\n",
    "\n",
    "1. **推理能力**：Marco-o1能够有效地利用CoT方法和MCTS技术进行复杂的推理，即使在某些情况下略过了显而易见的步骤，也不影响最终答案的准确性。\n",
    "   \n",
    "2. **性能提升**：相比于基线模型Qwen2-7B-Instruct，Marco-o1在多种配置下都展现了更高的准确率，尤其是在中文数据集上，采用细粒度的动作策略进一步提升了模型的表现。\n",
    "   \n",
    "3. **翻译能力**：在处理口语化表达和复杂句子结构时，Marco-o1展现出优于现有翻译工具的能力，能够生成更加自然和准确的翻译结果。\n",
    "\n",
    "综上所述，Marco-o1不仅在标准任务上表现出色，而且在开放性问题和多语言应用领域也展现出了巨大的潜力。未来的工作将继续优化奖励模型，减少随机性，并通过强化学习技术进一步提升决策制定过程，从而增强Marco-o1解决复杂现实世界任务的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek R1 介绍\r\n",
    "\r\n",
    "在深入探讨了OpenAI o1及其衍生版本Marco-o1的卓越推理能力和创新性技术之后，我们已经对当前最先进的语言模型在编程、数学以及逻辑推理等领域的应用有了全面的理解。OpenAI o1通过强化学习显著提升了其解决复杂问题的能力，尤其是在处理Codeforces竞赛题目和AIME数学考试中展现出了惊人的准确性和效率。与此同时，Marco-o1则进一步拓展了这一能力的应用范围，不仅限于具有明确标准答案的任务，还涵盖了开放式解决方案领域，这得益于它采用了过滤后的指令数据集和MCTS算法来优化其推理路径。\r\n",
    "\r\n",
    "然而，随着DeepSeek-R1的问世，我们迎来了另一个重要的里程碑。这款由DeepSeek公司开发的推理模型，同样基于大规模强化学习技术，并且在几乎没有监督微调的情况下，实现了与OpenAI o1相匹敌的表现。DeepSeek-R1不仅仅是在性能上对标了o1系列，它还引入了一系列技术创新，例如GRPO（Group Rewards Policy Optimization）算法，使得模型能够在推理过程中自动生成Chain of Thought (CoT)，从而大幅降低了计算成本并提高了推理效率。接下来，我们将详细剖析DeepSeek-R1的技术细节及其在多个应术性用场景表现技逻辑性。\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "DeepSeek-R1 是由 DeepSeek-AI 团队开发的第一代推理模型，它标志着在增强大型语言模型（LLMs）推理能力方面的重要进步。该模型包括两个主要版本：DeepSeek-R1-Zero 和 DeepSeek-R1。DeepSeek-R1-Zero 特别引人注目，因为它完全通过大规模强化学习（RL）训练而成，而没有经过监督微调（SFT）作为初步步骤。这种纯粹依赖于RL的训练方式让DeepSeek-R1-Zero展示了出色的推理能力，并自然地产生了多种强大且有趣的推理行为。\r\n",
    "\r\n",
    "DeepSeek-R1 则进一步改进了这一方法，通过引入多阶段训练和冷启动数据来解决诸如可读性差和语言混合的问题。DeepSeek-R1 的表现达到了与 OpenAI-o1-1217 相当的水平，在一系列推理任务中取得了卓越的成绩。这不仅证明了纯强化学习对于提升语言模型推理能力的有效性，也体现了 DeepSeek 在探索无需监督数据的情况下提高模型自我进化潜力方面的开创性贡献。\r\n",
    "\r\n",
    "####  DeepSeek 发布的背景及其目标\r\n",
    "\r\n",
    "近年来，随着人工智能技术的发展，特别是大型语言模型（LLMs）经历了快速迭代和进化，逐步缩小了向人工通用智能（AGI）迈进的差距。然而，尽管取得了显著进展，有效测试时间扩展仍然是研究界面临的一个开放问题。为了解决这个问题，DeepSeek 团队提出了利用纯强化学习来改善语言模型推理能力的方法。\r\n",
    "\r\n",
    "DeepSeek-R1 的发布旨在探索如何通过不依赖任何监督数据的方式提升LLMs的推理能力，强调模型通过纯RL过程实现自我进化的可能性。具体来说，DeepSeek-R1 基于 DeepSeek-V3-Base 模型，并采用了 GRPO（Group Relative Policy Optimization）框架进行改进。在整个训练过程中，DeepSeek-R1 自然地发展出了许多强大的推理行为，并在数千次RL步骤后表现出超凡的性能。\r\n",
    "\r\n",
    "此外，为了促进社区的进步，DeepSeek 还开源了 DeepSeek-R1-Zero、DeepSeek-R1 以及基于 Qwen 和 Llama 系列提炼出的六个密集模型（参数量从1.5B到70B不等）。这些举措不仅展示了 DeepSeek 对推动AI领域发展的承诺，也为研究人员提供了宝贵的资源，有助于加速相关技术的研究与发展。总之，DeepSeek 的目标是通过创新的技术解决方案推进人工智能领域的发展，特别是在提升模型推理能力方面做出了重要贡献。模型推理能力方面做出了重要贡献。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深入了解 DeepSeek R1 Zero 技术\r\n",
    "\r\n",
    "#### 基础模型概述\r\n",
    "\r\n",
    "DeepSeek V3（MOE）作为基础模型，其核心在于采用了混合专家系统（Mixture of Experts, MOE），这是一种允许模型根据输入动态选择最合适的子网络进行计算的方法。这种方法不仅提高了模型的效率和灵活性，还能有效扩展模型的能力以适应更复杂的任务。记忆系统在DeepSeek V3中扮演了重要角色，它使得模型能够存储并回忆先前遇到的信息或模式，这对于推理任务尤其重要。决策系统则负责在众多可能的操作路径中做出最优选择，确保了模型能够在处理复杂问题时保持高效与准确。\r\n",
    "\r\n",
    "#### DeepSeek R1-Zero 的技术实现\r\n",
    "\r\n",
    "DeepSeek-R1-Zero 是一个完全依赖强化学习（RL）训练而无需监督微调（SFT）的模型。通过直接将RL应用于基础模型上，DeepSeek-R1-Zero 能够自主探索解决复杂问题的链条思维（Chain-of-Thought, CoT）过程，从而发展出如自我验证、反思及生成长链条思维等能力。这一方法的重要性在于它证明了语言模型可以通过纯RL过程获得强大的推理能力，而不需要任何监督数据。这种突破为未来的研究开辟了新的道路，并展示了LLMs自我进化的潜力。\r\n",
    "\r\n",
    "**Template for DeepSeek-R1-Zero介绍**\r\n",
    "\r\n",
    "为了训练DeepSeek-R1-Zero，研究团队设计了一个简洁的模板来指导基础模型遵循特定指令。这个模板要求模型首先产生一个推理过程，然后给出最终答案。具体格式如下：\r\n",
    "\r\n",
    "```\r\n",
    "<think>推理过程在这里</think>\r\n",
    "<answer>答案在这里</answer>\r\n",
    "```\r\n",
    "\r\n",
    "在一个用户与助手之间的对话中，用户提出一个问题，助手先思考解决问题的推理过程，然后提供给用户答案。推理过程和答案分别用 `<think>` 和 `<answer>` 标签包围.例如，当用户提出一个问题时，Assistant会按照以下格式回应：\r\n",
    "\r\n",
    "```\r\n",
    "User: 如果a>1，则√(a-√(a+x))=x的所有实数解之和等于？\r\n",
    "Assistant:\r\n",
    "<think>\r\n",
    "为了求解方程 (√(a-√(a+x))=x)，我们首先对方程两边进行平方...\r\n",
    "</think>\r\n",
    "<answer>\r\n",
    "经过一系列推导，我们得到解为...\r\n",
    "</answer>\r\n",
    "```\r\n",
    "\r\n",
    "这种方法不仅确保了对模型自然进化过程的准确观察，还使得输出的内容更加结构化和易于理解。避免了任何形式的内容偏见，比如强制反思性思考或推广特定的问题解决策略。\r\n",
    "\r\n",
    "在ARC-AGI-1测试中，DeepSeek-R1-Zero 表现出了显著的进步，例如，在AIME 2024上的pass@1得分从最初的15.6%提升到了71.0%，且通过多数投票后得分进一步提高至86.7%，这与OpenAI-o1-0912的表现相当甚至有所超越。这些结果表明，即使不使用监督数据，DeepSeek-R1-Zero 也能够达到与其他顶尖模型相媲美的性能水平。通过这种纯粹依赖于RL的方法，DeepSeek-R1-Zero不仅展示了其在处理复杂推理任务方面的强大能力，还为未来的AI研究提供了宝贵的经验和启示。这种基于标签的输出方式不仅提高了模型输出的可读性和清晰度，也为后续的自动化评估和分析提供了便利。\r\n",
    "\r\n",
    "### DeepSeek R1 的多阶段训练流程\r\n",
    "\r\n",
    "DeepSeek-R1 的开发经历了一个精心设计的多阶段训练流程，旨在解决早期RL训练不稳定的问题，并进一步提升模型在推理任务中的表现。该流程包括冷启动监督微调（SFT）、强化学习（RL）以及最终针对所有场景的RL优化三个主要阶段。\r\n",
    "\r\n",
    "#### 冷启动SFT阶段\r\n",
    "\r\n",
    "为了克服RL训练初期的不稳定性，研究人员首先收集了数千条高质量的长链条思维（CoT）数据，这些数据经过精细处理和筛选后用于对DeepSeek-V3-Base进行初步微调。这一阶段的目标是确保模型能够生成结构清晰、逻辑连贯的回答，从而为后续的RL训练打下坚实的基础。例如，在创建冷启动数据时，团队特别注重输出格式的设计，采用了如`|special_token|<reasoning_process>|special_token|<summary>`的形式，不仅包含了详细的链条思维过程，还提供了易于理解的总结部分，极大地提高了输出内容的可读性和用户友好性。\r\n",
    "\r\n",
    "#### 强化学习（RL）阶段\r\n",
    "\r\n",
    "完成冷启动SFT后，DeepSeek-R1 进入了类似于DeepSeek-R1-Zero的大规模强化学习训练流程。在这个阶段，重点在于增强模型在编码、数学、科学等领域的推理能力。通过GRPO（Group Relative Policy Optimization）算法的应用，DeepSeek-R1 能够有效地优化其策略，同时减少了传统RL训练中所需的计算资源。GRPO算法的独特之处在于它放弃了通常与策略模型相同大小的评论模型，而是基于组得分来估计基线，这不仅节省了训练成本，还能更准确地指导模型优化其推理路径。\r\n",
    "\r\n",
    "此外，为了应对语言一致性问题，研究团队引入了一种语言一致性奖励机制。该机制根据目标语言词汇在链条思维过程中的比例来调整奖励值，尽管这种方法可能导致模型性能略有下降，但它显著提升了输出内容的语言一致性，使其更加符合人类用户的偏好，增强了模型的实际应用价值。\r\n",
    "\r\n",
    "#### 全场景RL阶段\r\n",
    "\r\n",
    "在接近RL训练收敛时，研究人员通过拒绝采样从当前检查点生成新的SFT数据，并结合来自DeepSeek-V3在写作、事实问答、自我认知等领域的监督数据，重新训练DeepSeek-V3-Base模型。之后，这个经过优化的检查点会再次经历RL训练，这次训练涵盖了所有可能的应用场景。通过这一系列步骤，DeepSeek-R1 不仅解决了R1-Zero在可读性和语言一致性方面的局限性，还在多个基准测试中实现了与顶级模型相匹敌的表现。例如，在AIME 2024上的pass@1得分达到了79.8%，而在MATH-500上更是高达97.3%，这些成绩不仅展示了DeepSeek-R1 在数学推理领域的卓越能力，也证明了其在广泛的任务类型中具有强大的泛化能力和适应性。\r\n",
    "\r\n",
    "总之，DeepSeek-R1 的多阶段训练流程通过结合冷启动数据、强化学习及全场景优化，成功解决了早期RL训练中的挑战，显著提升了模型的整体性能和实用性。这一创新方法为未来开发更高效、更智能的语言模型提供了宝贵的经验和技术支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek-R1-Zero 的“Aha Moment”\r\n",
    "\r\n",
    "在DeepSeek-R1-Zero的训练过程中，一个特别引人注目的现象是所谓的“Aha Moment”。这个时刻不仅标志着模型推理能力的一个重要突破，也为研究人员提供了一个关于强化学习（RL）潜力的深刻洞见。\r\n",
    "\r\n",
    "#### Aha Moment的定义与意义\r\n",
    "\r\n",
    "Aha Moment指的是模型在一个中间版本中学会了如何通过重新评估其初始方法来为一个问题分配更多的思考时间。例如，在解决一个复杂的数学问题时，DeepSeek-R1-Zero最初可能会尝试直接计算答案，但在某个训练阶段后，它开始学会先停下来反思之前的步骤是否正确，再探索不同的解题策略。这种行为不是预先编程好的，而是模型在与RL环境互动的过程中自发形成的。\r\n",
    "\r\n",
    "#### 具体案例分析\r\n",
    "\r\n",
    "一个具体的例子展示了这一过程。当面对方程“如果a>1，则√(a-√(a+x))=x的所有实数解之和等于？”时，DeepSeek-R1-Zero首先尝试了常规的代数操作，但在某一刻，它似乎“意识到”需要重新考虑它的方法：\r\n",
    "\r\n",
    "![aha](./image_o1/aha.png \"aha\")\r\n",
    "\r\n",
    "在这个“Aha Moment”，模型不仅仅是简单地继续沿用最初的解题路径，而是选择暂停并重新审视其逻辑步骤。这表明模型具备了一种自我反省的能力，能够在遇到复杂问题时采取更加灵活的方法。\r\n",
    "\r\n",
    "#### 对研究的意义\r\n",
    "\r\n",
    "对于研究人员来说，观察到这样的“Aha Moment”不仅是对模型不断发展的推理能力的一个证明，也展示了RL的强大之处：通过给予适当的激励而非直接教学，模型能够自主发展出高级的问题解决策略。这种自发性的改进不仅提高了模型处理更复杂任务的能力，也为未来开发更加自主、适应性更强的人工智能系统奠定了基础。总之，DeepSeek-R1-Zero的“Aha Moment”揭示了强化学习在解锁新层次智能方面的巨大潜力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek R1 的应用场景\n",
    "\n",
    "#### 3.1 数学推理与复杂决策支持\n",
    "\n",
    "DeepSeek-R1 在数学推理和复杂决策支持方面展现了卓越的能力。特别是在解决复杂的数学问题时，DeepSeek-R1 能够生成详细的链条思维（Chain-of-Thought, CoT），通过逐步推理来解决问题。例如，在AIME 2024的测试中，DeepSeek-R1 实现了79.8%的pass@1得分，这一成绩不仅超越了其他许多模型，而且在某些情况下甚至与OpenAI-o1-1217的表现相当或更优。\n",
    "\n",
    "一个具体的实例是当面对如下数学题目：“如果a>1，则√(a-√(a+x))=x的所有实数解之和等于？”DeepSeek-R1 能够首先思考解题过程，然后提供准确的答案。这种能力展示了其在处理需要深入分析和逻辑推理的数学问题上的强大功能。此外，在编程相关的任务上，DeepSeek-R1 表现出色，达到了Codeforces竞赛中的2,029 Elo评分，超过了96.3%的人类参与者，这表明它不仅擅长理论数学问题，也能有效地应用于实际编程挑战中。\n",
    "\n",
    "#### 3.2 跨领域泛化能力\n",
    "\n",
    "DeepSeek-R1 的另一个显著特点是其跨领域的泛化能力。为了实现这一点，DeepSeek 团队采用了混合深度推理SFT数据和通用SFT数据的方法。这种方法允许DeepSeek-R1 在不同的任务间进行有效的知识迁移，从而提高了其在多种应用场景下的表现。\n",
    "\n",
    "具体来说，通过结合针对特定领域（如数学、编码等）的深度推理SFT数据与涵盖广泛主题的通用SFT数据，DeepSeek-R1 能够更好地理解和处理各种类型的输入。例如，在处理MMLU、GPQA Diamond等基准测试时，DeepSeek-R1 分别取得了90.8%和71.5%的成绩，这些结果显著优于DeepSeek-V3，并且在教育相关任务中显示出强大的竞争力。同时，DeepSeek-R1 在事实性查询SimpleQA上也表现出色，进一步证明了其在处理基于事实的问题方面的有效性。\n",
    "\n",
    "此外，DeepSeek-R1 还展示了在创造性写作、一般问答、编辑和总结等多种任务中的出色表现。例如，在AlpacaEval 2.0长度控制获胜率上达到了87.6%，在ArenaHard上的获胜率为92.3%，这表明DeepSeek-R1 不仅限于考试导向的任务，还能智能地应对非考试性质的查询。特别是在需要理解长上下文的任务中，DeepSeek-R1 显著超越了DeepSeek-V3，显示了其在处理复杂文本信息方面的优势。\n",
    "\n",
    "总之，DeepSeek-R1 通过整合深度推理和通用SFT数据，成功实现了跨领域的泛化，使其成为了一个多功能的强大工具，适用于从数学推理到自然语言处理等多个领域。这种方法不仅增强了模型的适应性和灵活性，也为未来开发更加智能化的应用程序提供了新的可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 技术挑战与未来展望\r\n",
    "\r\n",
    "#### 4.1 当前的技术局限性\r\n",
    "\r\n",
    "尽管DeepSeek-R1-Zero展示了强大的推理能力，但它在可读性和语言一致性方面面临若干挑战。首先，在可读性方面，DeepSeek-R1-Zero生成的内容往往不适合直接阅读，可能存在多语言混用的问题，缺乏清晰的格式来突出答案，使得用户难以快速理解模型提供的信息。为了解决这个问题，DeepSeek团队开发了DeepSeek-R1，通过引入少量高质量的冷启动数据进行初步微调，从而提高了输出的可读性和连贯性。具体来说，DeepSeek-R1采用了特定的输出格式，如`|special_token|<reasoning_process>|special_token|<summary>`，这种结构不仅包含了详细的链条思维（CoT），还提供了易于理解的总结。\r\n",
    "\r\n",
    "其次，在语言一致性上，DeepSeek-R1-Zero在处理涉及多种语言的提示时可能会出现语言混合的情况。为了缓解这一问题，DeepSeek-R1引入了语言一致性奖励机制，该机制根据目标语言词汇在CoT中的比例来调整奖励值。虽然这种方法可能导致模型性能略有下降，但显著提升了输出内容的语言一致性，使其更符合人类用户的偏好。\r\n",
    "\r\n",
    "**不成功的尝试**\r\n",
    "\r\n",
    "在开发DeepSeek-R1的过程中，研究团队也遇到了一些不成功的尝试。以下是其中两个主要的例子：\r\n",
    "\r\n",
    "- **过程奖励模型（Process Reward Model, PRM）**：在早期实验中，团队尝试使用PRM来评估和优化模型的表现。PRM旨在通过分析模型生成的推理过程来提供反馈，而不是仅仅基于最终结果。然而，这种方法在大规模强化学习过程中容易遭受“奖励黑客”（reward hacking）的影响，即模型可能会找到一种方式最大化奖励函数，但实际上并没有提高其解决问题的能力。此外，重新训练和维护PRM需要额外的资源，并且复杂化了整个训练流程。因此，最终团队决定放弃这种做法，转而采用基于规则的奖励系统。\r\n",
    "\r\n",
    "- **蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）**：另一个尝试是将MCTS应用于RL训练中，以探索更多的潜在解决方案路径。MCTS通常用于游戏AI中，通过模拟多个可能的行动序列来选择最佳策略。然而，在DeepSeek-R1的应用场景中，MCTS并未能显著提升模型的推理能力。由于推理任务的多样性和复杂性，MCTS在某些情况下导致了过高的计算成本，同时未能带来预期的性能提升。因此，团队最终没有继续沿用这种方法。\r\n",
    "\r\n",
    "这些尝试虽然未能达到预期效果，但也为后续的研究提供了宝贵的经验教训，帮助团队更好地理解哪些方法更适合于增强大型语言模型的推理能力。\r\n",
    "\r\n",
    "#### 4.2 未来的研发方向\r\n",
    "\r\n",
    "为进一步提升推理模型的性能，特别是减少对人类标注数据依赖的趋势，未来的研究可以从以下几个方面入手：\r\n",
    "\r\n",
    "- **强化学习优化**：继续探索和改进用于训练LLMs的强化学习算法，如GRPO（Group Relative Policy Optimization）。这些方法不仅能节省训练成本，还能有效指导模型优化其推理能力。\r\n",
    "  \r\n",
    "- **自监督学习**：研究如何利用自监督学习技术来自动提取有用的信息，以减少对人工标注数据的需求。这可能包括开发新的预训练任务或利用现有的大规模无标签文本数据集。\r\n",
    "  \r\n",
    "- **跨领域知识迁移**：探索更加有效的策略来实现不同领域之间的知识迁移，使模型能够在不牺牲特定领域性能的情况下获得广泛的知识背景。例如，结合深度推理SFT数据和通用SFT数据可以增强模型的泛化能力。\r\n",
    "\r\n",
    "- **改进反馈机制**：设计更智能的反馈系统，能够动态地评估模型的表现，并据此调整训练策略。这样可以帮助模型更快地收敛到最优解，同时减少不必要的计算资源消耗。\r\n",
    "\r\n",
    "通过不断克服当前的技术局限并探索新的研究方向，DeepSeek-R1及其后续版本有望在推理能力和实用性方面取得更大的突破，推动人工智能技术的发展迈向新的高度。这些努力不仅增强了模型的基础推理能力，也为构建更加智能化、适应性更强的人工智能系统奠定了坚实的基础。\r\n",
    "\r\n",
    "### 结语\r\n",
    "\r\n",
    "DeepSeek R1 对AI领域产生了深远的影响，特别是在提高大型语言模型（LLMs）推理能力方面取得了重要突破。它证明了通过纯强化学习（RL）可以直接增强LLMs的推理能力，而无需依赖传统的监督微调（SFT）。此外，DeepSeek R1 的多阶段训练流程和冷启动数据的应用不仅解决了早期版本在可读性和语言一致性上的挑战，还进一步提升了模型的整体表现。\r\n",
    "\r\n",
    "DeepSeek R1 及其衍生版本在多个基准测试中表现出色，无论是数学推理、编程竞赛还是其他复杂的推理任务，都显示出了与顶尖模型相匹敌甚至超越的能力。更重要的是，DeepSeek 团队通过开源DeepSeek R1及其API，为整个研究社区提供了宝贵的资源，促进了更小但高效的密集型模型的发展。\r\n",
    "\r\n",
    "展望未来，随着技术的不断进步，我们有理由相信DeepSeek R1将推动更多创新应用的诞生，从教育到工业，再到日常生活中的各种场景，都将受益于其卓越的推理能力和适应性。通过持续的研究和开发，DeepSeek R1有望成为构建更加智能化、自主化的AI系统的基石。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
