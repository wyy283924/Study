{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796e356a-b463-4b87-9a4c-d93d7fa1095c",
   "metadata": {},
   "source": [
    "# MCP原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c56cf3-3dac-4402-a408-2cad5c59a0ec",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 第一模块：MCP协议概述\n",
    "\n",
    "## 1.1 大模型生态背景与问题引入\n",
    "\n",
    "近年来，以 ChatGPT、Claude 为代表的大型语言模型（LLM）取得了显著进展，其生成文本、理解语言和执行复杂任务的能力令人瞩目。一项最新的全国性调查显示，截至 2025 年 3 月，美国已有超过半数的成年人使用过这类人工智能大型语言模型，包括 ChatGPT、Gemini、Claude 和 Copilot 等。这些模型的快速普及和广泛应用，标志着人工智能技术正以前所未有的速度融入人们的日常生活和工作流程。\n",
    "\n",
    "调查还揭示，年轻、受过良好教育、相对富裕且有工作的成年人更倾向于使用 LLM。但值得注意的是，收入低于 5 万美元家庭的成年人中也有 53% 是 LLM 用户，而西班牙裔（66%）和非裔（57%）成年人的使用率也高于白人成年人（47%）。这种跨越不同人口统计学特征的广泛采用，进一步强调了 LLM 技术的普及性和影响力。\n",
    "\n",
    "尽管这些大型语言模型功能强大，但它们通常以封闭的方式运行，缺乏与外部世界的有效连接。大多数模型仅仅依赖于其训练数据和用户的直接输入，这导致了一个被称为“工具孤岛”的问题。这意味着，即使是最先进的 AI 模型，也常常在孤立的环境中运行，无法访问执行某些任务所需的关键实时数据或工具。例如：\n",
    "\n",
    "* 模型无法直接查询最新的天气信息；\n",
    "* 不能访问个人数据库或文件系统；\n",
    "* 无法调用外部 API 来完成跨平台协作任务。\n",
    "\n",
    "这种固有的局限性阻碍了 LLM 在更广泛的应用场景中发挥其全部潜力。\n",
    "\n",
    "面对 LLM 的封闭性和“工具孤岛”问题，我们迫切需要一种能够弥合 AI 模型与外部世界之间鸿沟的解决方案。这就引出了模型上下文协议（Model Context Protocol，简称 MCP）。MCP 的出现正是为了解决上述挑战。它提供了一个标准化的通信框架，使得 LLM 能够在运行时与外部系统（如数据库、API、各种工具）进行交互，从而扩展了模型的能力，使其能够访问实时信息、执行复杂操作，而无需重新训练模型。通过 MCP，我们可以构建更加智能、更加实用的 AI 应用，打破 LLM 的限制，使其真正融入并服务于我们的数字生活。\n",
    "\n",
    "<img src=\"images/21.png\" alt=\"MCP\" width=\"1000\"/>\n",
    "\n",
    "## 1.2 MCP 协议是什么\n",
    "\n",
    "模型上下文协议（MCP）是一个由 Anthropic 公司率先提出并开源发布的开放标准，其基本定义是一个通用的通信框架，旨在实现人工智能助手与外部数据源和系统之间的无缝连接。其核心目标包括：\n",
    "\n",
    "* **简化 AI 与外部工具的集成过程**：通过提供统一接口，无需为每一种数据源单独开发适配器；\n",
    "* **提升跨平台和多工具的互操作性**：促进不同系统之间更高效的协作；\n",
    "* **增强 AI 的上下文感知能力**：使得模型可以持续获取任务相关背景信息；\n",
    "* **支持动态工具发现与调用**：模型可在运行时发现、连接并调用工具；\n",
    "* **加强访问控制与安全保障**：防止敏感信息泄露或恶意调用。\n",
    "\n",
    "为了更好地理解 MCP 的作用，可以将其类比为“AI 界的 USB/HTTP 协议”。\n",
    "就像 USB 协议为计算机连接各种外设提供统一接口，使得不同厂商生产的设备能够即插即用；HTTP 协议为 Web 浏览器和服务器之间的通信提供标准规范，使用户能够访问全球网络信息。\n",
    "**类似地，MCP 为 AI 模型与外部系统之间的通信定义了一套统一标准，使得 AI 应用可以轻松调用各类工具与服务。**\n",
    "\n",
    "<img src=\"images/22.png\" alt=\"MCP\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "\n",
    "与传统插件或 API 不同，MCP 拥有以下优势：\n",
    "\n",
    "* **统一协议接口**：无需为每一个工具单独开发适配器；\n",
    "* **动态交互能力**：支持双向交互，可持续获取或发送数据；\n",
    "* **上下文跟踪机制**：保持任务执行中所需的信息连续性；\n",
    "* **可扩展性强**：适配范围广，便于在多个应用中通用部署；\n",
    "* **维护成本低**：减少重复集成，提高开发效率。\n",
    "\n",
    "总而言之，MCP 提供了一个灵活、标准化的方式，让 AI 模型不再“闭门造车”，而是能够真正参与复杂系统生态，推动 AI 更深入地走入生产实践。\n",
    "\n",
    "## 1.3 MCP 协议的发展与社区趋势\n",
    "\n",
    "MCP 协议的提出源于 Anthropic 在 2024 年底的技术观察：即便是最先进的 AI 系统，仍面临“信息孤岛”和“定制集成成本高”的现实问题。为此，Anthropic 推出了 MCP，并以开源形式发布，期望建立一个标准化的连接生态。\n",
    "\n",
    "发展趋势主要包括：\n",
    "\n",
    "* **早期应用案例推动协议落地**：Claude Desktop 首先集成 MCP，本地连接外部工具，提升使用体验；\n",
    "* **开发工具生态积极响应**：Cursor、Zed、Replit、Sourcegraph 等平台纷纷跟进，将 MCP 用于模型与开发工具的智能对接；\n",
    "* **社区合作构建连接器库**：包括 Google Drive、Slack、GitHub、Postgres 等常用系统的 MCP 连接器已被开发，降低开发门槛；\n",
    "* **生态系统逐步成熟**：Block、Apollo 等组织积极参与推动；MCP 正在成为 AI 助手系统构建的“事实标准”之一。\n",
    "\n",
    "可以预见，随着越来越多的开发者、组织与平台加入 MCP 生态，将涌现出更多高质量的服务器、客户端组件与集成工具，极大地拓展 LLM 的应用边界与商业价值。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8478cae-90c1-43ca-9e23-9a01275caf04",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 第二模块：MCP架构与通信机制\n",
    "\n",
    "## 2.1：MCP三层架构拆解\n",
    "\n",
    "模型上下文协议（Model Context Protocol，MCP）采用了**分层解耦的三层架构**，旨在实现 AI 应用与外部系统之间灵活、安全且模块化的交互能力。三层结构包括：**Host（宿主）**、**Client（客户端）**和**Server（服务器）**，各司其职，共同构建起完整的功能闭环。\n",
    "\n",
    "<img src=\"images/24.png\" alt=\"MCP\" width=\"1000\"/>\n",
    "\n",
    "---\n",
    "\n",
    "**1. Host（宿主）**\n",
    "Host 是用户交互的主要入口，通常是一个 LLM 驱动的应用程序或代理系统，例如 Claude 桌面客户端、集成开发环境（IDE）插件，或自定义的 AI 工具集。\n",
    "\n",
    "* **协调管理能力**：Host 负责整个交互流程的协调，创建并管理多个 Client 实例，并调度它们完成不同的任务。\n",
    "* **权限与隐私控制**：Host 执行统一的权限管理策略，确保在用户明确授权的前提下才可访问敏感数据。\n",
    "* **用户体验核心层**：Host 直接面向用户，承载所有人机交互逻辑。\n",
    "\n",
    "---\n",
    "\n",
    "**2. Client（客户端）**\n",
    "Client 运行于 Host 之中，起到桥梁作用，连接和管理一个或多个 Server。\n",
    "\n",
    "* **通信中介**：每个 Client 负责与单一 Server 保持连接，承担 JSON-RPC 协议的消息转发、响应解析等任务。\n",
    "* **功能发现与订阅管理**：Client 会主动探测 Server 提供的功能模块（如工具、资源、提示模板等），并管理其订阅通知。\n",
    "* **安全隔离机制**：多个 Client 实例之间互相隔离，防止访问权限越界，增强系统多租户支持能力。\n",
    "\n",
    "---\n",
    "\n",
    "**3. Server（服务器）**\n",
    "Server 是符合 MCP 协议规范的轻量服务进程，提供特定领域的工具、资源或服务。\n",
    "\n",
    "* **功能提供者**：每个 Server 专注于某一具体能力模块，例如数据库查询、API 调用、文件访问等。\n",
    "* **协议响应者**：Server 接收来自 Client 的调用请求，执行操作后将结果返回，完成一次完整的调用循环。\n",
    "* **权限执行者**：必须严格遵守由 Host 所施加的权限控制与用户访问策略。\n",
    "\n",
    "---\n",
    "\n",
    "这种三层解耦架构带来了以下核心优势：\n",
    "\n",
    "* **高度模块化**：组件之间职责明确、互不干扰，便于独立开发、调试与部署。\n",
    "* **灵活性增强**：Host 可灵活集成多个功能各异的 Server，实现功能扩展和快速组合。\n",
    "* **异构兼容性强**：各层之间通过统一的 MCP 协议通信，不依赖具体技术栈，可用不同语言或平台实现。\n",
    "\n",
    "常见的部署方式包括：\n",
    "\n",
    "* **本地集成（Local）**：Client 与 Server 部署在同一台机器上，使用 STDIO 进行通信。\n",
    "* **远程服务（Remote）**：Client 与 Server 部署在不同节点，通过 HTTP 或 SSE 协议实现远程调用。\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2：通信协议与数据格式\n",
    "\n",
    "MCP 的通信核心基于**JSON-RPC 2.0 协议标准**，确保 Client 与 Server 之间可以稳定、标准地交换结构化信息。\n",
    "\n",
    "### 核心特点：\n",
    "\n",
    "* **轻量化**：基于 JSON 格式，便于解析和调试。\n",
    "* **无状态调用**：每次请求与响应相互独立，不依赖上下文连接状态。\n",
    "* **标准一致性**：统一消息格式规范，保障不同实现之间的互操作性。\n",
    "\n",
    "---\n",
    "\n",
    "### 支持的通信方式：\n",
    "\n",
    "MCP 根据不同应用场景，支持三种主要通信方式：\n",
    "\n",
    "1. **STDIO（标准输入/输出）**\n",
    "\n",
    "   * 使用标准输入输出流进行本地进程间通信。\n",
    "   * 适用于 IDE 插件、CLI 工具等本地集成场景。\n",
    "   * 实现简单、性能优良。\n",
    "\n",
    "2. **HTTP**\n",
    "\n",
    "   * 使用 HTTP POST 请求进行远程过程调用。\n",
    "   * 适用于跨主机、多网络环境，便于服务发现与连接。\n",
    "   * 可结合 HTTPS 进行加密保障。\n",
    "\n",
    "3. **SSE（Server-Sent Events）**\n",
    "\n",
    "   * 基于 HTTP 长连接机制，实现 Server 主动推送消息。\n",
    "   * 客户端通过 `GET` 请求保持持久连接，服务端异步推送事件。\n",
    "   * 用于日志流、任务进度更新等事件驱动场景。\n",
    "\n",
    "---\n",
    "\n",
    "### 数据格式：\n",
    "\n",
    "* 所有消息体均采用 JSON 格式，字段结构严格遵循 JSON-RPC 2.0 规范，包括：\n",
    "\n",
    "  * `method`：调用方法名。\n",
    "  * `params`：调用参数。\n",
    "  * `id`：请求标识符。\n",
    "  * `result` / `error`：响应结果或错误信息。\n",
    "\n",
    "这种标准化数据格式使得 MCP 在不同语言、平台间实现变得简单高效。\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3：握手流程与会话初始化\n",
    "\n",
    "在正式通信之前，MCP 需要通过**握手流程**完成协议版本协商、能力互通和权限范围确认，从而初始化会话。\n",
    "\n",
    "<img src=\"images/25.png\" alt=\"MCP\" width=\"1000\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### 握手阶段：\n",
    "\n",
    "1. **Client 发起 `initialize` 请求**，内容包含：\n",
    "\n",
    "   * 支持的 MCP 协议版本；\n",
    "   * Client 支持的能力（如是否可访问文件系统、是否支持 LLM 功能）；\n",
    "   * 实现元信息（名称、版本号等）。\n",
    "\n",
    "2. **Server 返回响应**，包括：\n",
    "\n",
    "   * Server 所支持的协议版本；\n",
    "   * 提供的功能列表（如提示模板、外部资源、可执行工具）；\n",
    "   * Server 的实现信息（名称、版本）。\n",
    "\n",
    "---\n",
    "\n",
    "### 权限协商与 root 范围：\n",
    "\n",
    "* Client 可向 Server 发送 `workspace/roots` 请求，指定其可以访问的文件或目录范围。\n",
    "* 用户需明确授权目录访问权限，避免 Server 非法访问本地资源。\n",
    "* 发生变更时，Client 可通过事件通知 Server 重新评估其权限范围。\n",
    "\n",
    "---\n",
    "\n",
    "### 初始化完成：\n",
    "\n",
    "* Server 成功响应 `initialize` 后，Client 必须发送 `initialized` 通知，表明其准备完成；\n",
    "* 在此之前：\n",
    "\n",
    "  * Client **不得**发送除 `ping` 之外的其他请求；\n",
    "  * Server **不得**主动发送除 `ping` 和日志相关请求之外的消息。\n",
    "\n",
    "这一机制确保了 MCP 交互始终处于清晰、受控的生命周期状态中。\n",
    "\n",
    "---\n",
    "\n",
    "### 会话管理与关闭机制：\n",
    "\n",
    "* MCP 是**有状态连接**协议，会话在连接期间保持上下文一致性；\n",
    "* 支持如下关闭方式：\n",
    "\n",
    "  * **主动关闭**：Client 或 Server 明确发起终止；\n",
    "  * **被动断开**：传输层连接中断或请求超时；\n",
    "* 不同通信方式下关闭方法略有差异：\n",
    "\n",
    "  * STDIO：Client 关闭子进程输入流；\n",
    "  * HTTP：直接关闭连接；\n",
    "* 建议为所有请求设置合理的**超时时间**，防止资源悬挂和系统阻塞。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7714d-43fb-4c41-b25b-c6f70bf7042f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 第三模块：MCP能力模型解析\n",
    "\n",
    "## 3.1 三大能力模型：资源 / 工具 / 提示\n",
    "\n",
    "模型上下文协议（MCP）的核心在于其定义的三大能力模型：资源（Resources）、工具（Tools）和提示（Prompts）。这三种能力模型构成了MCP服务器向客户端（通常是AI应用）暴露其功能和数据的基本方式。理解它们的机制和应用场景对于开发和使用MCP至关重要。\n",
    "\n",
    "<img src=\"images/23.png\" alt=\"MCP\" width=\"1000\"/>\n",
    "\n",
    "**资源（Resources）**表示MCP服务器提供给客户端的各类数据和内容。资源可以是静态的，如配置文件、文档内容、数据库记录，也可以是结构化的，如API响应数据、实时系统状态等。资源的核心作用是为大语言模型（LLM）提供补充的上下文信息，以更好地理解用户输入或完成任务。资源通常为只读，客户端可以访问其内容，但无法进行修改。\n",
    "\n",
    "**工具（Tools）**是MCP服务器暴露的可执行操作，类似于传统API中的函数调用。LLM通过调用工具来执行具体任务，如查询数据库、调用外部API或进行某种计算等。工具通常需要输入参数，并在执行后返回结果。与资源不同，工具的执行可能具有副作用，如写入数据、发送通知等。\n",
    "\n",
    "**提示（Prompts）**是预定义的提示模板或交互工作流，由MCP服务器提供，并可在客户端中以结构化方式呈现。提示通常支持动态参数填充，并可以引用资源内容，旨在帮助用户完成特定任务，并规范化与LLM的交互方式，例如通过斜杠命令或可视化表单等形式呈现。\n",
    "\n",
    "选择哪种能力模型取决于任务的性质：若需为LLM提供背景知识或补充信息，使用资源；若需执行操作或调用外部系统，使用工具；若希望引导用户或规范交互流程，使用提示。\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 资源（Resources）的机制与应用\n",
    "\n",
    "在MCP中，资源作为能力模型之一，承担着向客户端提供数据上下文的角色。每个资源需通过统一资源标识符（URI）进行注册，从而使其可以被标准化访问。\n",
    "\n",
    "MCP服务器可注册多种类型的资源，包括静态文件和结构化数据。例如，通过`file://`协议注册本地文件，或通过`postgres://`注册数据库表，也可通过自定义协议集成第三方系统的API响应。资源可为文本（如JSON、日志、配置文件）或二进制数据（如图像、音频、视频等），后者需使用Base64编码传输。\n",
    "\n",
    "客户端可通过发送`resources/list`请求获取可用资源列表，每个条目包含URI、名称、描述及MIME类型。若需读取具体资源内容，客户端可发送`resources/read`请求，服务器将返回对应数据。同时，MCP支持资源的变更通知机制，如资源列表更新时推送`notifications/resources/list_changed`事件，或特定资源内容变更时发送`notifications/resources/updated`事件。\n",
    "\n",
    "资源广泛适用于多种场景。例如，在AI代码助手中，源代码、配置文件、文档等可作为资源供LLM参考；在客户服务系统中，产品手册、FAQ、知识库等可注册为资源，提高回答准确性；在实时系统中，资源也可承载动态状态或API数据，持续为LLM提供上下文支持。\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 工具（Tools）的机制与调用流程\n",
    "\n",
    "在MCP中，工具是连接LLM与服务器功能的重要桥梁，使得模型具备“可执行性”能力。其调用流程分为以下几个阶段：\n",
    "\n",
    "1. **工具发现**：客户端通过发送`tools/list`请求获取所有可用工具，响应中包括工具名称、描述及输入参数定义（以JSON Schema表示）。\n",
    "2. **调用请求**：当LLM决定使用某个工具时，客户端会构造并发送`tools/call`请求，携带工具名称和匹配的输入参数。\n",
    "3. **参数传入与执行**：服务器接收请求，解析参数，并调用相应功能代码，如外部接口请求、数据库操作、文件处理等。\n",
    "4. **结果返回**：执行完成后，服务器返回结果，包含输出数据、`isError`字段（指示是否成功）以及可选的资源引用等。\n",
    "\n",
    "由于工具调用可能产生副作用（如写入数据库、触发外部行为等），MCP建议在调用前需获得用户授权。客户端应向用户清晰展示工具名称、功能描述和参数，确保用户知情同意。此外，服务器需实施访问控制、参数校验、频率限制等安全机制，对于敏感操作，还可引入人工审批机制，保障系统的稳定性与安全性。\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 提示（Prompts）的机制与控制用法\n",
    "\n",
    "在MCP中，提示机制用于定义可复用、结构化的LLM交互模板，支持参数化填充和资源引用，从而实现标准化的人机交互流程。\n",
    "\n",
    "客户端可通过`prompts/list`请求获取所有可用提示，每个提示包含名称、描述及所需参数定义。若需使用某个提示，客户端可发送`prompts/get`请求，传入提示名称及相应参数，服务器则返回格式化的提示内容（包括对LLM的输入消息序列）。\n",
    "\n",
    "提示在客户端中可通过多种交互形式呈现，如：\n",
    "\n",
    "* 斜杠命令（Slash Command）\n",
    "* 快捷菜单项（Context Menu）\n",
    "* 引导式工作流（Wizard）\n",
    "* 表单界面（Interactive Form）\n",
    "\n",
    "提示机制适用于标准化任务流程，如文本生成模板、结构化写作指导、特定领域问答等。为了确保提示的有效性与安全性，服务器需校验提示参数，过滤用户输入，并可对提示使用频率进行限制。对于涉及敏感内容的提示，也应执行权限管理与访问控制，防止误用或滥用。\n",
    "\n",
    "通过系统化设计提示，开发者可大幅提升LLM交互的一致性、可预测性与任务完成质量。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ce60e-c770-4c40-8c15-4d1c61b5352e",
   "metadata": {},
   "source": [
    "# 第四模块：基础 MCP Server 实操教程\n",
    "\n",
    "本模块将带大家从零开始搭建并运行一个基础的 MCP Server 服务，分为三小节进行，涵盖开发环境准备、服务初始化、工具与资源注册，以及服务运行与验证等内容。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1：开发环境准备与 SDK 安装\n",
    "\n",
    "### 1. 创建 Python 3.10 环境\n",
    "\n",
    "建议使用 [Anaconda](https://www.anaconda.com/) 管理 Python 虚拟环境：\n",
    "\n",
    "```bash\n",
    "conda create -n mcp python=3.10\n",
    "conda activate mcp\n",
    "````\n",
    "\n",
    "### 2. 安装 `mcp` SDK 与 CLI 工具\n",
    "\n",
    "```bash\n",
    "pip install \"mcp[cli]\"\n",
    "```\n",
    "\n",
    "说明：\n",
    "\n",
    "* `mcp` 是 MCP Server 的核心库。\n",
    "* `[cli]` 安装了包含命令行工具的可选依赖。\n",
    "\n",
    "### 3. 安装 `uv` 工具（FastAPI 的现代开发工具）\n",
    "\n",
    "```bash\n",
    "pip install uv\n",
    "```\n",
    "\n",
    "安装成功后，可通过以下命令验证：\n",
    "\n",
    "```bash\n",
    "uv --version\n",
    "```\n",
    "\n",
    "<img src=\"images/1.png\" alt=\"MCP\" width=\"700\"/>\n",
    "\n",
    "### 4. 安装 Node.js（为了后续使用 `npx`）\n",
    "\n",
    "\n",
    "<img src=\"images/2.png\" alt=\"MCP\" width=\"700\"/>\n",
    "\n",
    "访问 [https://nodejs.org/](https://nodejs.org/) 安装 Node.js，安装完成后运行以下命令验证：\n",
    "\n",
    "\n",
    "```bash\n",
    "node -v\n",
    "npx -v\n",
    "```\n",
    "\n",
    "<img src=\"images/3.png\" alt=\"MCP\" width=\"700\"/>\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2：MCP Server 初始化与工具/资源注册\n",
    "\n",
    "在这一部分，我们将创建一个最小可用的 MCP Server，并注册工具与资源。\n",
    "\n",
    "### 1. 编写 MCP Server 主程序\n",
    "\n",
    "创建文件 `server.py`，代码如下：\n",
    "\n",
    "```python\n",
    "# server.py\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# 初始化一个名为 Demo 的 MCP Server\n",
    "mcp = FastMCP(\"Demo\")\n",
    "\n",
    "# 注册一个加法工具\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# 注册一个资源 greeting://{name}\n",
    "@mcp.resource(\"greeting://{name}\")\n",
    "def get_greeting(name: str) -> str:\n",
    "    \"\"\"Get a personalized greeting\"\"\"\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n",
    "```\n",
    "\n",
    "#### 说明：\n",
    "\n",
    "* `FastMCP` 是快速启动 MCP Server 的方式。\n",
    "* `@mcp.tool()` 装饰器注册一个工具，供客户端调用。\n",
    "* `@mcp.resource()` 装饰器注册一个资源，可根据路径参数动态返回内容。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3：运行 MCP Server 并验证\n",
    "\n",
    "### 1. 启动服务\n",
    "\n",
    "确保你处于 `server.py` 文件所在目录，运行以下命令：\n",
    "\n",
    "```bash\n",
    "mcp dev server.py\n",
    "```\n",
    "\n",
    "<img src=\"images/4.png\" alt=\"MCP\" width=\"700\"/>\n",
    "\n",
    "这会自动使用 `uvicorn` 启动本地开发服务器，并挂载 MCP 接口。\n",
    "\n",
    "### 2. 打开 Inspector 或控制台验证\n",
    "\n",
    "* 在浏览器中打开 MCP 控制台或开发工具地址（通常为 [http://localhost:6274）。](http://localhost:6274）。)\n",
    "* 点击connect，连接我们的MCP server。\n",
    "  \n",
    "<img src=\"images/5.png\" alt=\"MCP\" width=\"700\"/>\n",
    "\n",
    "* 连接后，右侧可以看到这个MCP server提供的服务。\n",
    "\n",
    "<img src=\"images/6.png\" alt=\"MCP\" width=\"700\"/>\n",
    "  \n",
    "* 你可以在工具列表中看到 `add`，尝试调用工具 `add`：传入 `a=3` 和 `b=4`，应返回 `7`。\n",
    "  \n",
    "<img src=\"images/7.png\" alt=\"MCP\" width=\"700\"/>\n",
    "  \n",
    "* 在资源浏览中的template看到 `greeting://{name}`。尝试访问访问 `greeting://Alice` 应返回：`Hello, Alice!`\n",
    "\n",
    "<img src=\"images/8.png\" alt=\"MCP\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef193c4-3d28-4f38-ad0c-17c4d24b8bc9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 第五模块：MCP Server 进阶实操\n",
    "\n",
    "## 5.1：扩展工具：封装\n",
    "\n",
    "在本小节中，我们将构建一个简单的 MCP 天气服务器。许多大型语言模型（LLM）目前不具备获取天气预报和恶劣天气警报的能力。我们将使用 MCP 来解决这个问题。\n",
    "\n",
    "我们将构建一个服务器，它暴露两个工具（tools）：`get_alerts` 和 `get_forecast`。\n",
    "\n",
    "### 1. 导入必要的模块\n",
    "\n",
    "```python\n",
    "from typing import Any\n",
    "import httpx\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "```\n",
    "\n",
    "**解释：**\n",
    "\n",
    "* `typing.Any`：用于类型注解，表示可以接受任意类型的数据。\n",
    "* `httpx`：一个支持异步请求的 HTTP 客户端库，用于发送网络请求。\n",
    "* `FastMCP`：MCP 框架中的快速服务器构建类，用于创建 MCP 服务器实例。\n",
    "\n",
    "### 2. 初始化 MCP 服务器\n",
    "\n",
    "```python\n",
    "mcp = FastMCP(\"weather\")\n",
    "```\n",
    "\n",
    "**解释：**\n",
    "\n",
    "创建一个名为 `\"weather\"` 的 MCP 服务器实例，用于注册和管理工具（tools）和资源（resources）。\n",
    "\n",
    "### 3. 定义常量\n",
    "\n",
    "```python\n",
    "NWS_API_BASE = \"https://api.weather.gov\"\n",
    "USER_AGENT = \"weather-app/1.0\"\n",
    "```\n",
    "\n",
    "**解释：**([YouTube][1])\n",
    "\n",
    "* `NWS_API_BASE`：美国国家气象局（NWS）API 的基础 URL。\n",
    "* `USER_AGENT`：HTTP 请求头中的用户代理字符串，用于标识请求来源。\n",
    "\n",
    "### 4. 定义异步请求函数\n",
    "\n",
    "```python\n",
    "async def make_nws_request(url: str) -> dict[str, Any] | None:\n",
    "    headers = {\n",
    "        \"User-Agent\": USER_AGENT,\n",
    "        \"Accept\": \"application/geo+json\"\n",
    "    }\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.get(url, headers=headers, timeout=30.0)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception:\n",
    "            return None\n",
    "```\n",
    "\n",
    "**解释：**([Stackademic][2])\n",
    "\n",
    "这是一个通用的异步函数，用于向指定的 NWS API URL 发送 GET 请求：\n",
    "\n",
    "* 设置请求头，包括用户代理和接受的内容类型。\n",
    "* 使用 `httpx.AsyncClient` 发送异步请求。\n",
    "* 处理可能出现的异常，确保函数在请求失败时返回 `None`，避免程序崩溃。\n",
    "\n",
    "### 5. 格式化警报信息\n",
    "\n",
    "```python\n",
    "def format_alert(feature: dict) -> str:\n",
    "    props = feature[\"properties\"]\n",
    "    return f\"\"\"\n",
    "事件: {props.get('event', 'Unknown')}\n",
    "区域: {props.get('areaDesc', 'Unknown')}\n",
    "严重性: {props.get('severity', 'Unknown')}\n",
    "描述: {props.get('description', 'No description available')}\n",
    "指示: {props.get('instruction', 'No specific instructions provided')}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**解释：**\n",
    "\n",
    "该函数用于将从 NWS API 获取的警报信息格式化为易读的字符串：\n",
    "\n",
    "* 提取警报的各个属性，如事件类型、影响区域、严重性、描述和指示。\n",
    "* 使用 `get` 方法提供默认值，确保在某些字段缺失时不会引发错误。\n",
    "\n",
    "### 6. 定义 `get_alerts` 工具\n",
    "\n",
    "```python\n",
    "@mcp.tool()\n",
    "async def get_alerts(state: str) -> str:\n",
    "    url = f\"{NWS_API_BASE}/alerts/active/area/{state}\"\n",
    "    data = await make_nws_request(url)\n",
    "\n",
    "    if not data or \"features\" not in data:\n",
    "        return \"无法获取警报或未找到警报。\"\n",
    "\n",
    "    if not data[\"features\"]:\n",
    "        return \"该州没有活跃的警报。\"\n",
    "\n",
    "    alerts = [format_alert(feature) for feature in data[\"features\"]]\n",
    "    return \"\\n---\\n\".join(alerts)\n",
    "```\n",
    "\n",
    "**解释：**\n",
    "\n",
    "这是一个 MCP 工具函数，用于获取指定州的天气警报：\n",
    "\n",
    "* 构建请求 URL，调用 `make_nws_request` 获取数据。\n",
    "* 检查返回的数据是否有效，并处理无数据或无警报的情况。\n",
    "* 使用 `format_alert` 函数格式化每个警报，并将它们合并为一个字符串返回。\n",
    "\n",
    "### 7. 定义 `get_forecast` 工具\n",
    "\n",
    "```python\n",
    "@mcp.tool()\n",
    "async def get_forecast(latitude: float, longitude: float) -> str:\n",
    "    points_url = f\"{NWS_API_BASE}/points/{latitude},{longitude}\"\n",
    "    points_data = await make_nws_request(points_url)\n",
    "\n",
    "    if not points_data:\n",
    "        return \"无法获取此位置的预报数据。\"\n",
    "\n",
    "    forecast_url = points_data[\"properties\"][\"forecast\"]\n",
    "    forecast_data = await make_nws_request(forecast_url)\n",
    "\n",
    "    if not forecast_data:\n",
    "        return \"无法获取详细预报。\"\n",
    "\n",
    "    periods = forecast_data[\"properties\"][\"periods\"]\n",
    "    forecasts = []\n",
    "    for period in periods[:5]:\n",
    "        forecast = f\"\"\"\n",
    "{period['name']}:\n",
    "温度: {period['temperature']}°{period['temperatureUnit']}\n",
    "风: {period['windSpeed']} {period['windDirection']}\n",
    "预报: {period['detailedForecast']}\n",
    "\"\"\"\n",
    "        forecasts.append(forecast)\n",
    "\n",
    "    return \"\\n---\\n\".join(forecasts)\n",
    "```\n",
    "\n",
    "**解释：**\n",
    "\n",
    "这是另一个 MCP 工具函数，用于获取指定经纬度位置的天气预报：\n",
    "\n",
    "* 首先调用 `points` API 获取该位置的预报 URL。\n",
    "* 然后使用该 URL 获取详细的预报数据。\n",
    "* 提取前 5 个预报时段的信息，包括名称、温度、风速和详细预报。\n",
    "* 将每个时段的预报信息格式化并合并为一个字符串返回。\n",
    "\n",
    "### 8. 启动 MCP 服务器\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport='stdio')\n",
    "```\n",
    "\n",
    "**解释：**\n",
    "\n",
    "这段代码用于启动 MCP 服务器：\n",
    "\n",
    "* `if __name__ == \"__main__\":` 确保只有在直接运行该脚本时才启动服务器。\n",
    "* `mcp.run(transport='stdio')` 启动服务器，使用标准输入输出作为通信方式。\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2：异常处理与网络请求安全\n",
    "\n",
    "在与外部 API 交互时，必须考虑网络请求可能失败的情况，并进行适当的异常处理和参数校验。\n",
    "\n",
    "### 请求失败处理\n",
    "\n",
    "在 `make_nws_request` 函数中，我们使用了 `try-except` 块来捕获可能的异常，例如网络超时、连接错误或无效的响应。\n",
    "\n",
    "```python\n",
    "try:\n",
    "    response = await client.get(url, headers=headers, timeout=30.0)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "except Exception:\n",
    "    return None\n",
    "```\n",
    "\n",
    "**解释：**([Medium][3])\n",
    "\n",
    "* `response.raise_for_status()`：如果响应状态码表示错误（如 4xx 或 5xx），将引发异常。\n",
    "* `except Exception:`：捕获所有异常，返回 `None`，避免程序崩溃。\n",
    "\n",
    "### 参数校验与返回内容格式设计\n",
    "\n",
    "在工具函数中，我们检查返回的数据是否包含预期的字段，并提供默认值或错误消息，确保返回内容的格式一致且易于理解。\n",
    "\n",
    "```python\n",
    "if not data or \"features\" not in data:\n",
    "    return \"无法获取警报或未找到警报。\"\n",
    "```\n",
    "\n",
    "**解释：**([python-httpx.org][4])\n",
    "\n",
    "* 检查 `data` 是否为 `None` 或不包含 `features` 字段，返回相应的错误消息。\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3：测试\n",
    "\n",
    "在完成服务器的开发后，我们需要进行测试，确保其功能正常。\n",
    "\n",
    "### 启动服务\n",
    "\n",
    "使用以下命令启动服务器：\n",
    "\n",
    "```bash\n",
    "mcp dev weather.py\n",
    "```\n",
    "\n",
    "**解释：**\n",
    "\n",
    "* `mcp dev`：启动 MCP 开发服务器，自动加载并运行指定的脚本。\n",
    "* `weather.py`：包含 MCP 服务器定义的脚本文件。\n",
    "\n",
    "### 使用 Inspector / 控制台验证调用\n",
    "\n",
    "启动服务器后，可以使用 MCP 提供的 Inspector 或控制台工具，调用 `get_alerts` 和 `get_forecast` 工具，验证其功能是否正常。\n",
    "\n",
    "<img src=\"images/9.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e60a2-de75-4347-aa69-17d4e2416e03",
   "metadata": {},
   "source": [
    "# 第六模块：集成与智能体应用场景\n",
    "\n",
    "## 6.1：对接 Cherry Studio\n",
    "\n",
    "### ✅ 环境准备与安装\n",
    "\n",
    "1. **下载 Cherry Studio：**\n",
    "   访问 [Cherry Studio 官网](https://www.cherry-ai.com/) 下载并安装最新版本客户端。\n",
    "\n",
    "   <img src=\"images/18.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "2. **配置大模型 API：**\n",
    "   在 Cherry Studio 设置中填写你的大模型 API Key，例如 DeepSeek 或其他兼容平台。\n",
    "\n",
    "   <img src=\"images/12.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "3. **配置 MCP 服务器：**\n",
    "   进入 **设置 > MCP服务器**，添加 MCP 服务器参数。以下以天气服务为例：\n",
    "\n",
    "   ```shell\n",
    "   --directory D:\\mcp\n",
    "   run weather.py\n",
    "   ```\n",
    "\n",
    "   <img src=\"images/10.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "4. **启用 MCP 功能：**\n",
    "\n",
    "   * 在对话界面勾选 MCP Server，启用工具调用能力。\n",
    "   * 开始对话测试，确认大模型能够自动选择工具并生成响应。\n",
    "\n",
    "   <img src=\"images/11.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 工作原理简析\n",
    "\n",
    "一个完整的请求流程如下所示：\n",
    "\n",
    "```\n",
    "[用户提问] → [大模型判断是否调用工具] → [MCP Server 执行工具] → [结果返回大模型] → [生成最终答复]\n",
    "```\n",
    "\n",
    "具体流程：\n",
    "\n",
    "1. Cherry Studio 将用户问题发送给大模型\n",
    "2. 大模型判断是否需要调用工具\n",
    "3. 若需要，则指令 MCP Server 执行相应工具\n",
    "4. 工具返回结果\n",
    "5. 大模型将结果组织为复\n",
    "6. 最终响应显示在对话界面\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 6.2：开发自定义 MCP 客户端\n",
    "\n",
    "本节将展示如何使用 Python 编写一个更灵活的 MCP 客户端，通过大模型与 MCP Server 工具进行交互调用，实现智能问答与工具组合使用。\n",
    "\n",
    "### 📁 项目结构概览\n",
    "\n",
    "```\n",
    "client.py              # MCP 客户端主程序\n",
    ".env                   # 存储 OpenAI API Key 和模型等环境变量\n",
    "```\n",
    "\n",
    "### 🧠 核心代码解析\n",
    "\n",
    "#### 1. 初始化 MCP 客户端与 OpenAI 接口\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from typing import Optional\n",
    "from contextlib import AsyncExitStack\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # 加载 .env 文件中的环境变量\n",
    "```\n",
    "\n",
    "🔍 **说明**：导入必要模块并使用 `load_dotenv()` 从 `.env` 文件加载 OpenAI API 相关的配置，包括 API Key、模型名等。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class MCPClient:\n",
    "    def __init__(self):\n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            base_url=os.getenv(\"OPENAI_BASE_URL\")\n",
    "        )\n",
    "        self.model = os.getenv(\"OPENAI_MODEL\")\n",
    "        self.messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a versatile assistant capable of answering questions, completing tasks, and intelligently invoking specialized tools to deliver optimal results.\"\n",
    "            }\n",
    "        ]\n",
    "        self.available_tools = []\n",
    "```\n",
    "\n",
    "🔍 **说明**：构造函数初始化了与 MCP Server 的连接会话、OpenAI 接口、系统提示信息（设定 AI 行为），并预留工具列表用于后续调用。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "@staticmethod\n",
    "def convert_custom_object(obj):\n",
    "    if hasattr(obj, \"__dict__\"):\n",
    "        return obj.__dict__\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [MCPClient.convert_custom_object(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: MCPClient.convert_custom_object(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "```\n",
    "\n",
    "🔍 **说明**：辅助函数，用于将复杂对象转换成 JSON 兼容的字典格式，以便 OpenAI API 或日志输出中使用。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 连接 MCP Server\n",
    "\n",
    "```python\n",
    "async def connect_to_server(self, server_script_path: str):\n",
    "    is_python = server_script_path.endswith('.py')\n",
    "    is_js = server_script_path.endswith('.js')\n",
    "    command = \"python\" if is_python else \"node\"\n",
    "    if not (is_python or is_js):\n",
    "        command = 'npx'\n",
    "\n",
    "    server_params = StdioServerParameters(\n",
    "        command=command,\n",
    "        args=[server_script_path],\n",
    "        env=None\n",
    "    )\n",
    "\n",
    "    stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n",
    "    self.stdio, self.write = stdio_transport\n",
    "    self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n",
    "    \n",
    "    await self.session.initialize()\n",
    "    response = await self.session.list_tools()\n",
    "    tools = response.tools\n",
    "    print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "```\n",
    "\n",
    "🔍 **说明**：\n",
    "\n",
    "* 当传入 `.py` 文件时使用 `python`；\n",
    "* `.js` 文件使用 `node`；\n",
    "* 其他情况（如 TypeScript 脚本、CLI 工具等），默认用 `npx` 启动，例如 `npx bing-cn-mcp`；\n",
    "* 适配多种脚本启动方式，提升工具扩展性。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 查询处理流程\n",
    "\n",
    "```python\n",
    "async def process_query(self, query: str) -> str:\n",
    "    self.messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query\n",
    "    })\n",
    "\n",
    "    if not self.available_tools:\n",
    "        response = await self.session.list_tools()\n",
    "        self.available_tools = [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.inputSchema\n",
    "            }\n",
    "        } for tool in response.tools]\n",
    "```\n",
    "\n",
    "🔍 **说明**：将用户输入添加到历史消息中，并在首次调用时获取 MCP Server 提供的所有工具，将其格式转换为 OpenAI 所需的 tools 参数格式。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    current_response = self.client.chat.completions.create(\n",
    "        model=self.model,\n",
    "        messages=self.messages,\n",
    "        tools=self.available_tools,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    if current_response.choices[0].message.content and current_response.choices[0].message.tool_calls:\n",
    "        print(\"\\n  AI:\", current_response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "🔍 **说明**：调用 OpenAI Chat API，生成回复内容，如果 AI 同时给出自然语言回复和工具调用建议，则输出文本部分。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    while current_response.choices[0].message.tool_calls:\n",
    "        for tool_call in current_response.choices[0].message.tool_calls:\n",
    "            tool_name = tool_call.function.name\n",
    "            try:\n",
    "                tool_args = json.loads(tool_call.function.arguments)\n",
    "            except json.JSONDecodeError:\n",
    "                tool_args = {}\n",
    "            print(f\"\\n  调用工具 {tool_name}\")\n",
    "            print(f\"  参数: {tool_args}\")\n",
    "\n",
    "            result = await self.session.call_tool(tool_name, tool_args)\n",
    "            print(f\"\\n工具结果: {result}\")\n",
    "\n",
    "            self.messages.append(current_response.choices[0].message)\n",
    "            self.messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": result.content\n",
    "            })\n",
    "\n",
    "        current_response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            tools=self.available_tools,\n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "    self.messages.append(current_response.choices[0].message)\n",
    "    return current_response.choices[0].message.content or \"\"\n",
    "```\n",
    "\n",
    "🔍 **说明**：循环处理所有工具调用请求，调用本地工具服务获取结果，并将调用结果作为工具回复加入历史消息后继续对话，直到没有工具调用为止。\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 主循环与清理\n",
    "\n",
    "```python\n",
    "async def chat_loop(self):\n",
    "    print(\"\\nMCP Client Started!\")\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nCommend: \").strip()\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "            response = await self.process_query(query)\n",
    "            print(\"\\n AI: \" + response)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError occurs: {e}\")\n",
    "            traceback.print_exc()\n",
    "```\n",
    "\n",
    "🔍 **说明**：进入交互式聊天循环，支持持续输入查询与退出命令，自动处理异常输出以便调试。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "async def cleanup(self):\n",
    "    await self.exit_stack.aclose()\n",
    "```\n",
    "\n",
    "🔍 **说明**：释放客户端所占用的资源，确保退出时所有连接与上下文被正确关闭。\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 入口函数\n",
    "\n",
    "```python\n",
    "async def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python client.py <path_to_server_script>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    client = MCPClient()\n",
    "    try:\n",
    "        await client.connect_to_server(sys.argv[1])\n",
    "        await client.chat_loop()\n",
    "    finally:\n",
    "        await client.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    asyncio.run(main())\n",
    "```\n",
    "\n",
    "🔍 **说明**：程序入口，接收命令行参数指定的工具脚本路径，初始化客户端、连接服务器并启动聊天循环，退出时自动清理资源。\n",
    "\n",
    "---\n",
    "\n",
    "### ▶️ 启动示例\n",
    "\n",
    "```bash\n",
    "# 启动 Python 工具\n",
    "python client.py websearch.py\n",
    "\n",
    "# 启动 JavaScript 工具\n",
    "python client.py tools/weather.js\n",
    "\n",
    "# 启动 TypeScript 工具（用 npx 自动判断）\n",
    "python client.py tools/weather.ts  # 实际调用：npx tools/weather.ts\n",
    "```\n",
    "\n",
    "🔍 **说明**：根据服务脚本格式选择对应运行方式，注意需要事先配置 `.env` 文件并安装依赖模块（如 openai、mcp、dotenv 等）。\n",
    "\n",
    "客户端将：\n",
    "\n",
    "* 连接到指定服务器\n",
    "* 列出可用工具\n",
    "* 启动交互式聊天会话，可：\n",
    "\n",
    "  * 输入查询\n",
    "  * 查看工具调用过程\n",
    "  * 获取大模型的最终响应\n",
    "\n",
    "以网络检索MCP服务为例，我们在启动客户端时输入：\n",
    "\n",
    "```bash\n",
    "# 启动 Python 工具\n",
    "python client.py websearch.py\n",
    "```\n",
    "\n",
    "   <img src=\"images/27.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "如图，客户端会先列出MCP服务中提供的工具，并与大模型连接，进行对话。当大模型判断需要使用工具时会调用MCP服务中的工具，再综合后给出解答。\n",
    "\n",
    "这里我们询问大模型什么是MCP，大模型调用 web_search工具搜索什么是MCP。得到工具调用结果后再给出回答。\n",
    "\n",
    "   <img src=\"images/28.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "\n",
    "我们也可以使用别的服务，例如，使用百度地图的MCP服务，我们在启动客户端时输入：\n",
    "\n",
    "```bash\n",
    "# 启动 Python 工具\n",
    "python client.py map.py\n",
    "```\n",
    "<img src=\"images/29.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "这里，我们可以看到，百度地图的MCP服务提供了很多工具。当我们询问大模型深圳北站附件有哪些美食时，大模型首先使用 map_geocode 工具获取了深圳北站的信息，再 map_search_places 获取了其周围美食的信息。\n",
    "\n",
    "   <img src=\"images/30.png\" alt=\"MCP\" width=\"800\"/>\n",
    "\n",
    "工具调用完毕后，大模型会根据这些结果给出回复。\n",
    "\n",
    "---\n",
    "\n",
    "## 6.3：获取开放的 MCP Server\n",
    "\n",
    "在实际项目中，也可以直接集成社区或平台托管的 MCP 服务，无需自建。\n",
    "\n",
    "### 🔍 MCP 服务资源获取渠道：\n",
    "\n",
    "* [MCP 官方仓库](https://github.com/modelcontextprotocol/servers)\n",
    "* [Awesome MCP Servers 整理](https://github.com/punkpeye/awesome-mcp-servers)\n",
    "* [Cursor Plugin 目录](https://cursor.directory)\n",
    "* [MCP.so 中文资源平台](https://mcp.so/zh)\n",
    "* [ModelScope 平台](https://modelscope.cn/mcp)\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 ModelScope 平台接入示例\n",
    "\n",
    "1. **访问 MCP 广场：**\n",
    "   选择带有 `Hosted` 标签的服务，即表示可直接调用的云端 MCP 工具。\n",
    "\n",
    "   <img src=\"images/15.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "2. **进入服务详情页获取配置：**\n",
    "   点击“连接”按钮，获取对应的 **SSE URL 配置**。\n",
    "\n",
    "   <img src=\"images/19.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "   > 🚨 部分服务可能需要额外设置环境变量，请参考服务介绍页说明。\n",
    "\n",
    "3. **在 Cherry Studio 中配置：**\n",
    "\n",
    "   <img src=\"images/14.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "   * 打开 **设置 > MCP服务器 > 同步服务器**\n",
    "   * 输入 ModelScope API Token（魔搭平台）进行同步\n",
    "   * Cherry Studio 将自动拉取当前账易维护：标准化协议，减少重复开发。\n",
    "\n",
    "    <img src=\"images/16.png\" alt=\"MCP\" width=\"1200\"/>\n",
    "\n",
    "\n",
    "4. **使用托管服务：**\n",
    "\n",
    "以“高德地图 MCP”为例，在 ModelScope 中设置后，Cherry Studio 中同步并勾选对应服务，即可由模型自动调用该工具。\n",
    "\n",
    "<img src=\"images/17.png\" alt=\"MCP\" width=\"1200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59597ef-381a-4835-a27a-deef2491c080",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6.4：案例：企业内部 AI 助理系统\n",
    "\n",
    "### 背景\n",
    "\n",
    "随着企业数字化转型的深入，越来越多的组织开始探索如何借助人工智能提升内部协作效率与员工体验。然而，传统企业内部系统往往彼此孤立，员工需频繁在多个平台之间切换，既耗时又易出错。\n",
    "\n",
    "例如，员工可能希望通过 AI 助理完成以下任务：\n",
    "\n",
    "* 查询公司知识库中的某条制度；\n",
    "* 预定某天某时段的会议室；\n",
    "* 提交 IT 支持请求。\n",
    "\n",
    "在上述场景中，“安全可控的系统接入”是关键挑战。通过引入 **模型上下文协议（MCP）**，企业可以构建统一、标准、安全的 AI 接入架构。每个内部系统可部署独立的 MCP Server，仅暴露经过授权的工具与资源；AI 助理作为 MCP Host，通过 MCP Client 安全访问各系统功能，完成具体操作。\n",
    "\n",
    "例如，当员工询问：“下周一下午三点有空的会议室吗？”时，AI 助理将：\n",
    "\n",
    "1. 通过 MCP Client 接入日历系统对应的 MCP Server；\n",
    "2. 查询会议室预订情况；\n",
    "3. 获取结果后反馈给用户；\n",
    "   全流程在 MCP 安全协议下完成，确保权限隔离与数据安全。\n",
    "\n",
    "---\n",
    "\n",
    "### 问题分析\n",
    "\n",
    "| 问题      | 描述                   |\n",
    "| ------- | -------------------- |\n",
    "| ❌ 接口分散  | 不同系统接口风格各异，LLM难以统一接入 |\n",
    "| ❌ 安全风险高 | 内部系统需严格控制访问权限        |\n",
    "| ❌ 集成成本高 | 每新增一个系统都需额外适配开发      |\n",
    "\n",
    "---\n",
    "\n",
    "### 解决方案：基于 MCP 协议的标准化集成\n",
    "\n",
    "通过 MCP，企业可构建模块化、标准化的 AI 系统接入架构，实现低耦合、高安全、易扩展的能力开放体系。\n",
    "\n",
    "#### （1）系统架构组成\n",
    "\n",
    "| 组件角色           | 功能描述                                     |\n",
    "| -------------- | ---------------------------------------- |\n",
    "| **MCP Host**   | 员工可交互的 AI 助理界面（如桌面助手、企业微信插件），背后连接大语言模型   |\n",
    "| **MCP Client** | 作为桥梁，负责调用 MCP Server 所暴露的工具与资源           |\n",
    "| **MCP Server** | 每个内部系统部署一个 Server，提供独立的功能服务，如会议室查询、知识检索等 |\n",
    "\n",
    "> ✅ 每个 MCP Server 可独立维护、授权和升级，不影响整体系统\n",
    "\n",
    "📌 **架构图示例：** <img src=\"images/20.png\" alt=\"MCP 架构图\" width=\"1200\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### （2）工作流程解析\n",
    "\n",
    "1. **连接初始化**：MCP Host 启动后，自动建立与各 MCP Server 的连接；\n",
    "2. **功能发现**：MCP Client 查询各个 Server 所提供的工具（tools）与资源（resources）；\n",
    "3. **智能决策**：当员工发起请求，LLM 分析任务，选择合适的工具与 Server；\n",
    "4. **安全调用**：MCP Server 执行任务，返回结果；\n",
    "5. **权限控制**：可通过 Server 配置文件限制模型能调用的功能与可访问的数据，实现细粒度授权。\n",
    "\n",
    "---\n",
    "\n",
    "### 核心优势\n",
    "\n",
    "| 优势           | 说明                |\n",
    "| ------------ | ----------------- |\n",
    "| 🧩 **模块化部署** | 每个内部系统独立部署，支持灵活扩展 |\n",
    "| 🔒 **安全性强**  | 支持细粒度访问控制、权限隔离    |\n",
    "| 🚀 **效率提升**  | 提高系统协作效率，响应速度快    |\n",
    "| 🔄 **运维友好**  | 基于统一协议，降低适配与维护成本  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d0173-7362-4d4f-833f-9d20a06ccc95",
   "metadata": {},
   "source": [
    "\n",
    "# 第七模块：MCP与其他接入方式对比\n",
    "\n",
    "## 7.1：MCP vs ChatGPT插件 / 函数调用 / RAG\n",
    "\n",
    "为了更全面地理解模型上下文协议（MCP）的价值和适用场景，将其与目前主流的几种LLM接入外部能力的方式进行对比分析至关重要。以下表格将从多个关键维度对MCP、ChatGPT插件、函数调用和检索增强生成（RAG）进行横向对比，并分析各自的场景适配建议与部署策略。\n",
    "\n",
    "| 特性/方法 | MCP（模型上下文协议） |ChatGPT插件 | 函数调用 | RAG（检索增强生成） |\n",
    "|---|---|---|---|---|\n",
    "| **标准化协议** | 是，开放标准，定义了Host、Client、Server之间的通信规范、数据格式和能力模型 | 否，每个插件通常需要单独开发和部署，遵循OpenAI的插件规范 | 否，函数调用的实现方式取决于具体的LLM平台和API | 否，RAG是一种架构模式，没有统一的协议标准 |\n",
    "| **动态工具发现** | 是，Client可以查询Server获取可用的工具、资源和提示 | 是，ChatGPT可以在运行时发现并使用已安装的插件 | 否，通常需要在提示或模型配置中预先声明可用的函数 | 否，工具或数据源的集成通常在系统设计时确定 |\n",
    "| **上下文管理** | 支持，Client与Server之间维护状态连接，可以跨多个交互保持上下文 | 有限，插件的上下文管理能力取决于插件自身的实现 | 有限，函数调用通常是无状态的，上下文需要在提示中显式传递 | 取决于检索系统的实现，通常在单次查询中增强上下文 |\n",
    "| **安全性** | 强调用户授权和控制，支持本地优先安全，服务器本地运行，需要用户明确批准数据访问和操作 | 需要用户安装和授权插件，安全性取决于插件的开发者和OpenAI的审核 | 安全性取决于函数实现的可靠性和API调用的权限控制 | 安全性取决于检索系统的数据访问控制和用户身份验证 |\n",
    "| **易用性** | 对于开发者而言，提供了标准化的SDK和架构，降低了集成复杂性；对于用户而言，需要Host应用支持MCP | 用户可以在ChatGPT界面方便地搜索、安装和启用插件 | 函数调用通常需要在代码中实现，并与LLM API进行集成 | RAG的实现涉及到数据索引、检索和提示工程，有一定的复杂性 |\n",
    "| **生态系统成熟度** | 相对较新，但社区生态正在迅速成型，Anthropic和社区提供了越来越多的MCP服务器和客户端 | 已经有相当数量的插件可用，覆盖了多种应用场景 | 各大LLM平台都支持函数调用，生态较为成熟 | RAG是一种广泛采用的技术，拥有丰富的工具和实践经验 |\n",
    "| **场景适配建议** | 适用于需要高度互操作性、动态工具发现、复杂工作流程和细粒度安全控制的场景，例如AI代码助手、企业级智能代理等 | 适用于在ChatGPT界面内扩展功能，满足用户特定需求的场景，例如网页浏览、信息查询、特定应用交互等 | 适用于需要让LLM执行特定、预定义的操作的场景，例如调用API获取实时数据、执行计算等 | 适用于需要利用外部知识库增强LLM的知识覆盖和回答质量的场景，例如问答系统、文档检索等 |\n",
    "| **部署策略分析** | 可以本地部署（STDIO）或远程部署（HTTP/SSE），灵活性高，可以根据安全和性能需求选择合适的部署方式 | 插件通常由开发者托管，用户在ChatGPT平台安装和使用 | 函数调用直接与LLM API集成，部署与LLM平台的服务部署一致 | RAG系统通常需要在用户的基础设施或云平台上部署检索服务和知识库 |\n",
    "\n",
    "## 7.2：课程总结与答疑互动\n",
    "\n",
    "在本教材中，我们系统地介绍了模型上下文协议（MCP）的各个方面。从其诞生的背景和要解决的问题入手，我们深入探讨了MCP的基本定义、核心目标以及其在快速发展的人工智能社区中的趋势。我们详细解析了MCP的三层架构，包括Host、Client和Server的角色与职责划分，以及它们之间基于JSON-RPC 2.0的通信协议和数据格式，并阐述了握手流程与会话初始化的机制。\n",
    "\n",
    "我们还深入研究了MCP的三大核心能力模型：资源、工具和提示。通过具体的例子，我们理解了如何使用URI注册和传递资源，如何通过工具调用执行外部功能并回传结果，以及如何在任务定制化场景下利用提示模板提高对话的一致性和标准化。\n",
    "\n",
    "此外，本教材还探讨了MCP与其他主流AI接入方式（如ChatGPT插件、函数调用和RAG）的异同，并分析了它们各自的适用场景和部署策略。通过对比分析，我们可以更清晰地认识到MCP在标准化、动态发现、上下文管理和安全性等方面的独特优势。\n",
    "\n",
    "希望通过本教材的学习，您能够对模型上下文协议（MCP）有一个全面而深入的理解，并掌握其核心概念、架构原理和应用场景。我们相信，随着智能体生态的不断发展，MCP将会在其中扮演越来越重要的角色，为构建更加智能、更加互联的AI应用提供强大的支持。\n",
    "\n",
    "最后，如果您在学习过程中有任何疑问，或者希望更深入地探讨某个特定的主题，欢迎随时提出。我们鼓励您积极参与到MCP的社区中，与其他开发者和研究者交流学习，共同探索MCP在未来的无限潜力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd355e3-3d58-481c-83f8-cd99b40ad35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
